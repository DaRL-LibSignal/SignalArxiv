<!DOCTYPE html>
<html lang="en">

<head>
    <title>LibSignal-SignalArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                Posssible Related Arxiv Papers
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-27T00:00:00Z">2024-03-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Connections between Reachability and Time Optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juho Bae, Ji Hoon Bai, Byung-Yoon Lee, Jun-Yong Lee, Chang-Hun Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the concept of an equivalence relation between the set of
optimal control problems. By leveraging this concept, we show that the boundary
of the reachability set can be constructed by the solutions of time optimal
problems. Alongside, a more generalized equivalence theorem is presented
together. The findings facilitate the use of solution structures from a certain
class of optimal control problems to address problems in corresponding
equivalent classes. As a byproduct, we state and prove the construction methods
of the reachability sets of three-dimensional curves with prescribed curvature
bound. The findings are twofold: Firstly, we prove that any boundary point of
the reachability set, with the terminal direction taken into account, can be
accessed via curves of H, CSC, CCC, or their respective subsegments, where H
denotes a helicoidal arc, C a circular arc with maximum curvature, and S a
straight segment. Secondly, we show that any boundary point of the reachability
set, without considering the terminal direction, can be accessed by curves of
CC, CS, or their respective subsegments. These findings extend the developments
presented in literature regarding planar curves, or Dubins car dynamics, into
spatial curves in $\mathbb{R}^3$. For higher dimensions, we confirm that the
problem of identifying the reachability set of curvature bounded paths subsumes
the well-known Markov-Dubins problem. These advancements in understanding the
reachability of curvature bounded paths in $\mathbb{R}^3$ hold significant
practical implications, particularly in the contexts of mission planning
problems and time optimal guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Risk-aware Branch MPC for Automated Driving that is Robust
  to Uncertain Vehicle Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luyao Zhang, George Pantazis, Shaohang Han, Sergio Grammatico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the critical challenges in automated driving is ensuring safety of
automated vehicles despite the unknown behavior of the other vehicles. Although
motion prediction modules are able to generate a probability distribution
associated with various behavior modes, their probabilistic estimates are often
inaccurate, thus leading to a possibly unsafe trajectory. To overcome this
challenge, we propose a risk-aware motion planning framework that appropriately
accounts for the ambiguity in the estimated probability distribution. We
formulate the risk-aware motion planning problem as a min-max optimization
problem and develop an efficient iterative method by incorporating a
regularization term in the probability update step. Via extensive numerical
studies, we validate the convergence of our method and demonstrate its
advantages compared to the state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPC-CBF with <span class="highlight-title">Adaptive</span> Safety Margins for Safety-critical Teleoperation
  over Imperfect Network Connections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Periotto, Mina Ferizbegovic, Fernando S. Barbosa, Roberto C. Sundin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper focuses on the design of a control strategy for safety-critical
remote teleoperation. The main goal is to make the controlled system track the
desired velocity specified by an operator while avoiding obstacles despite
communication delays. Control Barrier Functions (CBFs) are used to define the
safety constraints that the system has to respect to avoid obstacles, while
Model Predictive Control (MPC) provides the framework for adjusting the desired
input, taking the constraints into account. The resulting input is sent to the
remote system, where appropriate low-level velocity controllers translate it
into system-specific commands. The main novelty of the paper is a method to
make the CBFs robust against the uncertainties caused by the network delays
affecting the system's state and do so in a less conservative manner. The
results show how the proposed method successfully solves the safety-critical
teleoperation problem, making the controlled systems avoid obstacles with
different types of network delay. The controller has also been tested in
simulation and on a real manipulator, demonstrating its general applicability
when reliable low-level velocity controllers are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 2024 European Control Conference
  (ECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Addressing Data Annotation Challenges in Multiple Sensors: A Solution
  for Scania Collected <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajinkya Khoche, Aron Asefaw, Alejandro Gonzalez, Bogdan Timus, Sina Sharif Mansouri, Patric Jensfelt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data annotation in autonomous vehicles is a critical step in the development
of Deep Neural Network (DNN) based models or the performance evaluation of the
perception system. This often takes the form of adding 3D bounding boxes on
time-sequential and registered series of point-sets captured from active
sensors like Light Detection and Ranging (LiDAR) and Radio Detection and
Ranging (RADAR). When annotating multiple active sensors, there is a need to
motion compensate and translate the points to a consistent coordinate frame and
timestamp respectively. However, highly dynamic objects pose a unique
challenge, as they can appear at different timestamps in each sensor's data.
Without knowing the speed of the objects, their position appears to be
different in different sensor outputs. Thus, even after motion compensation,
highly dynamic objects are not matched from multiple sensors in the same frame,
and human annotators struggle to add unique bounding boxes that capture all
objects. This article focuses on addressing this challenge, primarily within
the context of Scania collected datasets. The proposed solution takes a track
of an annotated object as input and uses the Moving Horizon Estimation (MHE) to
robustly estimate its speed. The estimated speed profile is utilized to correct
the position of the annotated box and add boxes to object clusters missed by
the original annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to European Control Conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Control Synthesis of Markov Decision Processes for Efficiency
  with Surveillance Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18632v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18632v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Chen, Xuanyuan Yin, Shaoyuan Li, Xiang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of optimal control synthesis for Markov Decision
Processes (MDPs), addressing both qualitative and quantitative objectives.
Specifically, we require the system to fulfill a qualitative surveillance task
in the sense that a specific region of interest can be visited infinitely often
with probability one. Furthermore, to quantify the performance of the system,
we consider the concept of efficiency, which is defined as the ratio between
rewards and costs. This measure is more general than the standard long-run
average reward metric as it aims to maximize the reward obtained per unit cost.
Our objective is to synthesize a control policy that ensures the surveillance
task while maximizes the efficiency. We provide an effective approach to
synthesize a stationary control policy achieving $\epsilon$-optimality by
integrating state classifications of MDPs and perturbation analysis in a novel
manner. Our results generalize existing works on efficiency-optimal control
synthesis for MDP by incorporating qualitative surveillance tasks. A robot
motion planning case study is provided to illustrate the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Virtual Reality to the Emerging Discipline of Perception
  Engineering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven M. LaValle, Evan G. Center, Timo Ojala, Matti Pouke, Nicoletta Prencipe, Basak Sakcak, Markku Suomalainen, Kalle G. Timperi, Vadim K. Weinstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper makes the case that a powerful new discipline, which we term
perception engineering, is steadily emerging. It follows from a progression of
ideas that involve creating illusions, from historical paintings and film, to
video games and virtual reality in modern times. Rather than creating physical
artifacts such as bridges, airplanes, or computers, perception engineers create
illusory perceptual experiences. The scope is defined over any agent that
interacts with the physical world, including both biological organisms (humans,
animals) and engineered systems (robots, autonomous systems). The key idea is
that an agent, called a producer, alters the environment with the intent to
alter the perceptual experience of another agent, called a receiver. Most
importantly, the paper introduces a precise mathematical formulation of this
process, based on the von Neumann-Morgenstern notion of information, to help
scope and define the discipline. It is then applied to the cases of engineered
and biological agents with discussion of its implications on existing fields
such as virtual reality, robotics, and even social media. Finally, open
challenges and opportunities for involvement are identified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bootstrapping Guarantees: Stability and Performance Analysis for Dynamic
  Encrypted Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Schlor, Frank Allgöwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encrypted dynamic controllers that operate for an unlimited time have been a
challenging subject of research. The fundamental difficulty is the accumulation
of errors and scaling factors in the internal state during operation.
Bootstrapping, a technique commonly employed in fully homomorphic
cryptosystems, can be used to avoid overflows in the controller state but can
potentially introduce significant numerical errors. In this paper, we analyze
dynamic encrypted control with explicit consideration of bootstrapping. By
recognizing the bootstrapping errors occurring in the controller's state as an
uncertainty in the robust control framework, we can provide stability and
performance guarantees for the whole encrypted control system. Further, the
conservatism of the stability and performance test is reduced by using a lifted
version of the control system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Formal Verification with Constrained Polynomial Logical Zonotope 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Hafez, Frank J. Jiang, Karl H. Johansson, Amr Alanwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose using constrained polynomial logical zonotopes for
formal verification of logical systems. We perform reachability analysis to
compute the set of states that could be reached. To do this, we utilize a
recently introduced set representation called polynomial logical zonotopes for
performing computationally efficient and exact reachability analysis on logical
systems. Notably, polynomial logical zonotopes address the "curse of
dimensionality" when analyzing the reachability of logical systems since the
set representation can represent 2^n binary vectors using n generators. After
finishing the reachability analysis, the formal verification involves verifying
whether the intersection of the calculated reachable set and the unsafe set is
empty or not. However, polynomial logical zonotopes are not closed under
intersections. To address this, we formulate constrained polynomial logical
zonotopes, which maintain the computational efficiency and exactness of
polynomial logical zonotopes for reachability analysis while supporting exact
intersections. Furthermore, we present an extensive empirical study
illustrating and verifying the benefits of using constrained polynomial logical
zonotopes for the formal verification of logical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamic Programming Approach for Road Traffic Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Laurini, Irene Saccani, Stefano Ardizzoni, Luca Consolini, Marco Locatelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a road network represented by a directed graph. We assume to
collect many measurements of traffic flows on all the network arcs, or on a
subset of them. We assume that the users are divided into different groups.
Each group follows a different path. The flows of all user groups are modeled
as a set of independent Poisson processes. Our focus is estimating the paths
followed by each user group, and the means of the associated Poisson processes.
We present a possible solution based on a Dynamic Programming algorithm. The
method relies on the knowledge of high order cumulants. We discuss the
theoretical properties of the introduced method. Finally, we present some
numerical tests on well-known benchmark networks, using synthetic data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stability Properties of the Impulsive Goodwin's Oscillator in 1-cycle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton V. Proskurnikov, Alexander Medvedev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Impulsive Goodwin's Oscillator (IGO) is a mathematical model of a hybrid
closed-loop system. It arises by closing a special kind of continuous linear
positive time-invariant system with impulsive feedback, which employs both
amplitude and frequency pulse modulation. The structure of IGO precludes the
existence of equilibria, and all its solutions are oscillatory. With its origin
in mathematical biology, the IGO also presents a control paradigm useful in a
wide range of applications, in particular dosing of chemicals and medicines.
Since the pulse modulation feedback mechanism introduces significant
nonlinearity and non-smoothness in the closedloop dynamics, conventional
controller design methods fail to apply. However, the hybrid dynamics of IGO
reduce to a nonlinear, time-invariant discrete-time system, exhibiting a
one-to-one correspondence between periodic solutions of the original IGO and
those of the discrete-time system. The paper proposes a design approach that
leverages the linearization of the equivalent discrete-time dynamics in the
vicinity of a fixed point. A simple and efficient local stability condition of
the 1-cycle in terms of the characteristics of the amplitude and frequency
modulation functions is obtained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust Reinforcement-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Feedback Linearizable Discretizations of Second Order Mechanical Systems
  using Retraction Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyas N. B., David Martin Diego, Ravi Banavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mechanical systems, in nature, are often described by a set of
continuous-time, nonlinear, second-order differential equations (SODEs). This
has motivated designs of various control laws implemented on digital
controllers, consequently requiring numerical discretization schemes. Feedback
linearizability of such sampled systems depends on the discretization scheme or
map choice. In this article, we utilize retraction maps and their lifts to
construct feedback linearizable discretizations for SODEs, which can be applied
to various mechanical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion
  Planning Algorithm for Hybrid Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Wang, Ricardo G. Sanfelice
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a bidirectional rapidly-exploring random trees (RRT)
algorithm to solve the motion planning problem for hybrid systems. The proposed
algorithm, called HyRRT-Connect, propagates in both forward and backward
directions in hybrid time until an overlap between the forward and backward
propagation results is detected. Then, HyRRT-Connect constructs a motion plan
through the reversal and concatenation of functions defined on hybrid time
domains, ensuring the motion plan thoroughly satisfies the given hybrid
dynamics. To address the potential discontinuity along the flow caused by
tolerating some distance between the forward and backward partial motion plans,
we reconstruct the backward partial motion plan by a forward-in-hybrid-time
simulation from the final state of the forward partial motion plan. By applying
the reversed input of the backward partial motion plan, the reconstruction
process effectively eliminates the discontinuity and ensures that as the
tolerance distance decreases to zero, the distance between the endpoint of the
reconstructed motion plan and the final state set approaches zero. The proposed
algorithm is applied to an actuated bouncing ball example and a walking robot
example so as to highlight its generality and computational improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 8th IFAC International Conference on Analysis and
  Design of Hybrid Systems (ADHS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Adaptive</span> Economic Model Predictive Control for linear systems with
  performance guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Degner, Raffaele Soloperto, Melanie N. Zeilinger, John Lygeros, Johannes Köhler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a model predictive control (MPC) formulation to directly optimize
economic criteria for linear constrained systems subject to disturbances and
uncertain model parameters. The proposed formulation combines a certainty
equivalent economic MPC with a simple least-squares parameter adaptation. For
the resulting adaptive economic MPC scheme, we derive strong asymptotic and
transient performance guarantees. We provide a numerical example involving
building temperature control and demonstrate performance benefits of online
parameter adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, submitted to IEEE CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multivariable control of modular multilevel converters with convergence
  and safety guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18371v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18371v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Daniel Reyes Dreke, Ygor Pereira Marca, Maurice Roes, Mircea Lazar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Well-designed current control is a key factor in ensuring the efficient and
safe operation of modular multilevel converters (MMCs). Even though this
control problem involves multiple control objectives, conventional current
control schemes are comprised of independently designed decoupled controllers,
e.g., proportional-integral (PI) or proportional-resonant (PR). Due to the
bilinearity of the MMC dynamics, tuning PI and PR controllers so that good
performance and constraint satisfaction are guaranteed is quite challenging.
This challenge becomes more relevant in an AC/AC MMC configuration due to the
complexity of tracking the single-phase sinusoidal components of the MMC
output. In this paper, we propose a method to design a multivariable
controller, i.e., a static feedback gain, to regulate the MMC currents. We use
a physics-informed transformation to model the MMC dynamics linearly and
synthesise the proposed controller. We use this linear model to formulate a
linear matrix inequality that computes a feedback gain that guarantees safe and
effective operation, including (i) limited tracking error, (ii) stability, and
(iii) meeting all constraints. To test the efficacy of our method, we examine
its performance in a direct AC/AC MMC simulated in Simulink/PLECS and in a
scaled-down AC/AC MMC prototype to investigate the ultra-fast charging of
electric vehicles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Open Journal of the Industrial Electronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Dual Gradient Tracking for Distributed Resource
  Allocation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huo, Xiaomeng Chen, Lingying Huang, Karl Henrik Johansson, Ling Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates privacy issues in distributed resource allocation
over directed networks, where each agent holds a private cost function and
optimizes its decision subject to a global coupling constraint through local
interaction with other agents. Conventional methods for resource allocation
over directed networks require all agents to transmit their original data to
neighbors, which poses the risk of disclosing sensitive and private
information. To address this issue, we propose an algorithm called
differentially private dual gradient tracking (DP-DGT) for distributed resource
allocation, which obfuscates the exchanged messages using independent Laplacian
noise. Our algorithm ensures that the agents' decisions converge to a
neighborhood of the optimal solution almost surely. Furthermore, without the
assumption of bounded gradients, we prove that the cumulative differential
privacy loss under the proposed algorithm is finite even when the number of
iterations goes to infinity. To the best of our knowledge, we are the first to
simultaneously achieve these two goals in distributed resource allocation
problems over directed networks. Finally, numerical simulations on economic
dispatch problems within the IEEE 14-bus system illustrate the effectiveness of
our proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Distributed Nonconvex Stochastic Optimization
  with Quantized Communications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialong Chen, Jimin Wang, Ji-Feng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new distributed nonconvex stochastic optimization
algorithm that can achieve privacy protection, communication efficiency and
convergence simultaneously. Specifically, each node adds time-varying privacy
noises to its local state to avoid information leakage, and then quantizes its
noise-perturbed state before transmitting to improve communication efficiency.
By employing the subsampling method controlled through the sample-size
parameter, the proposed algorithm reduces the impact of privacy noises, and
enhances the differential privacy level. When the global cost function
satisfies the Polyak-Lojasiewicz condition, the mean and high-probability
convergence rate and the oracle complexity of the proposed algorithm are given.
Importantly, the proposed algorithm achieves both the mean convergence and a
finite cumulative differential privacy budget over infinite iterations as the
sample-size goes to infinity. A numerical example of the distributed training
on the "MNIST" dataset is given to show the effectiveness of the algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with
  Multi-Band Reconfigurability and Antenna-VSWR Resilience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Guo, Yuchen Cao, Kenle Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first-ever highly linear and load-insensitive
three-way load-modulation power amplifier (PA) based on reconfigurable hybrid
asymmetrical load modulated balanced amplifier (H-ALMBA). Through proper
amplitude and phase controls, the carrier, control amplifier (CA), and two
peaking balanced amplifiers (BA1 and BA2) can form a linear high-order load
modulation over wide bandwidth. Moreover, it is theoretically unveiled that the
load modulation behavior of H-ALMBA can be insensitive to load mismatch by
leveraging bias reconfiguration and the intrinsic load-insensitivity of
balanced topology. Specifically, the PA's linearity and efficiency profiles can
be maintained against arbitrary load mismatch through $Z_\mathrm{L}$-dependent
reconfiguration of CA supply voltage ($V_\mathrm{DD,CA}$) and turning-on
sequence of BA1 and BA2. Based on the proposed theory, an RF-input linear
H-ALMBA is developed with GaN transistors and wideband quadrature hybrids. Over
the design bandwidth from $1.7$-$2.9$ GHz, an efficiency of $56.8\%$$-$$72.9\%$
at peak power and $49.8\%$$-$$61.2\%$ at $10$-dB PBO are measured together with
linear AMAM and AMPM responses. In modulated evaluation with 4G LTE signal, an
EVM of $3.1\%$, ACPR of $-39$ dB, and average efficiency of up to $52\%$ are
measured. Moreover, the reconfigurable H-ALMBA experimentally maintains an
excellent average efficiency and linearity against arbitrary load mismatch at
$2:1$ VSWR, and this mismatch-resilient operation can be achieved at any
in-band frequencies. The overall measured performance favorably outperforms the
state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Execution-time-certified QP Algorithm for $\ell_1$ penalty-based
  Soft-constrained MPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wu, Richard D. Braatz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing an execution time certificate and handling possible infeasibility
in closed-loop are two pressing requirements of Model Predictive Control (MPC).
To simultaneously meet these two requirements, this paper uses $\ell_1$
penalty-based soft-constrained MPC formulation and innovatively transforms the
resulting non-smooth QP into a box-constrained QP, which is solved by our
previously proposed direct and execution-time certified algorithm with only
dimension-dependent (data-independent) and exact number of iterations [1]. This
approach not only overcomes the limitation of our previously proposed algorithm
[1], only applicable to input-constrained MPC, but also enjoys exact recovery
feature (exactly recover the same solution when the original problem is
feasible) of $\ell_1$ penalty-based soft-constrained MPC formulation without
suffering numerical difficulty of the resulting non-smoothness. Other various
real-time QP applications, not limited to MPC, will also benefit from our QP
algorithm with execution-time certificate and global feasibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault-tolerant properties of scale-free linear protocols for
  synchronization of homogeneous <span class="highlight-title">multi-agent</span> systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton A. Stoorvogel, Ali Saberi, Zhenwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Originally, protocols were designed for multi-agent systems (MAS) using
information about the network. However, in many cases there is no or only
limited information available about the network. Recently, there has been a
focus on scale-free synchronization of multi-agent systems (MAS). In this case,
the protocol is designed without any prior information about the network. As
long as the network contains a directed spanning tree, the scale-free protocol
guarantees that the network achieves synchronization.
  If there is no directed spanning tree for the network then synchronization
cannot be achieved. But what happens when these scale-free protocols are
applied to such a network where the directed spanning tree no longer exists?
The latter might arise if, for instance, a fault occurs in one of more crucial
links. This paper establishes that the network decomposes into a number of
basic bicomponents which achieves synchronization among all nodes in this basic
bicomponent. On the other hand, nodes which are not part of any basic
bicomponent converge to a weighted average of the synchronized trajectories of
the basic bicomponents. The weights are independent of the initial conditions
and are independent of the designed protocol.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The article was submitted to IEEE Transactions on Automatic Control
  for review at March 27th, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An
  Auction-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pan-Yang Su, Chinmay Maheshwari, Victoria Tuck, Shankar Sastry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of advanced air mobility (AAM) is expected to become a
multibillion-dollar industry in the near future. Market-based mechanisms are
touted to be an integral part of AAM operations, which comprise heterogeneous
operators with private valuations. In this work, we study the problem of
designing a mechanism to coordinate the movement of electric vertical take-off
and landing (eVTOL) aircraft, operated by multiple operators each having
heterogeneous valuations associated with their fleet, between vertiports, while
enforcing the arrival, departure, and parking constraints at vertiports.
Particularly, we propose an incentive-compatible and individually rational
vertiport reservation mechanism that maximizes a social welfare metric, which
encapsulates the objective of maximizing the overall valuations of all
operators while minimizing the congestion at vertiports. Additionally, we
improve the computational tractability of designing the reservation mechanism
by proposing a mixed binary linear programming approach that is based on
constructing network flow graph corresponding to the underlying problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incentive Designs for Learning Agents to Stabilize Coupled Exogenous
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18164v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18164v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jair Certório, Nuno C. Martins, Richard J. La, Murat Arcak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a large population of learning agents noncooperatively selecting
strategies from a common set, influencing the dynamics of an exogenous system
(ES) we seek to stabilize at a desired equilibrium. Our approach is to design a
dynamic payoff mechanism capable of shaping the population's strategy profile,
thus affecting the ES's state, by offering incentives for specific strategies
within budget limits. Employing system-theoretic passivity concepts, we
establish conditions under which a payoff mechanism can be systematically
constructed to ensure the global asymptotic stabilization of the ES's
equilibrium. In comparison to previous approaches originally studied in the
context of the so-called epidemic population games, the method proposed here
allows for more realistic epidemic models and other types of ES, such as
predator-prey dynamics. Stabilization is established with the support of a
Lyapunov function, which provides useful bounds on the transients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of Three Influencer Archetypes for the Control of Opinion Spread
  in Time-Varying Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael DeBuse, Sean Warnick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we consider the impact of information spread in time-varying
social networks, where agents request to follow other agents with aligned
opinions while dropping ties to neighbors whose posts are too dissimilar to
their own views. Opinion control and rhetorical influence has a very long
history, employing various methods including education, persuasion, propaganda,
marketing, and manipulation through mis-, dis-, and mal-information. The
automation of opinion controllers, however, has only recently become easily
deployable at a wide scale, with the advent of large language models (LLMs) and
generative AI that can translate the quantified commands from opinion
controllers into actual content with the appropriate nuance. Automated agents
in social networks can be deployed for various purposes, such as breaking up
echo chambers, bridging valuable new connections between agents, or shaping the
opinions of a target population -- and all of these raise important ethical
concerns that deserve serious attention and thoughtful discussion and debate.
This paper attempts to contribute to this discussion by considering three
archetypal influencing styles observed by human drivers in these settings,
comparing and contrasting the impact of these different control methods on the
opinions of agents in the network. We will demonstrate the efficacy of current
generative AI for generating nuanced content consistent with the command signal
from automatic opinion controllers like these, and we will report on frameworks
for approaching the relevant ethical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission to IEEE 2024 Conference on Decision and Control. 8 pages,
  7 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconstrained learning of networked nonlinear systems via free
  parametrization of stable interconnected operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13967v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13967v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Massai, Danilo Saccani, Luca Furieri, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper characterizes a new parametrization of nonlinear networked
incrementally $L_2$-bounded operators in discrete time. The distinctive novelty
is that our parametrization is \emph{free} -- that is, a sparse large-scale
operator with bounded incremental $L_2$ gain is obtained for any choice of the
real values of our parameters. This property allows one to freely search over
optimal parameters via unconstrained gradient descent, enabling direct
applications in large-scale optimal control and system identification. Further,
we can embed prior knowledge about the interconnection topology and stability
properties of the system directly into the large-scale distributed operator we
design. Our approach is extremely general in that it can seamlessly encapsulate
and interconnect state-of-the-art Neural Network (NN) parametrizations of
stable dynamical systems. To demonstrate the effectiveness of this approach, we
provide a simulation example showcasing the identification of a networked
nonlinear system. The results underscore the superiority of our free
parametrizations over standard NN-based identification methods where a prior
over the system topology and local stability properties are not enforced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version of the paper to appear at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Control Allocation: A Learning Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed Riaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern aircraft are designed with redundant control effectors to cater for
fault tolerance and maneuverability requirements. This leads to aircraft being
over-actuated and requires control allocation schemes to distribute the control
commands among control effectors. Traditionally, optimization-based control
allocation schemes are used; however, for nonlinear allocation problems, these
methods require large computational resources. In this work, an artificial
neural network (ANN) based nonlinear control allocation scheme is proposed. The
proposed scheme is composed of learning the inverse of the control
effectiveness map through ANN, and then implementing it as an allocator instead
of solving an online optimization problem. Stability conditions are presented
for closed-loop systems incorporating the allocator, and computational
challenges are explored with piece-wise linear effectiveness functions and
ANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is
compared with a standard quadratic programming-based method for control
allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Conference on Decision and Control (CDC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Redesigning Large-Scale Multimodal Transit Networks with Shared
  Autonomous Mobility Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.16075v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.16075v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max T. M. Ng, Hani S. Mahmassani, Ömer Verbas, Taner Cokyasar, Roman Engelhardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses a large-scale multimodal transit network design problem,
with Shared Autonomous Mobility Services (SAMS) as both transit feeders and an
origin-to-destination mode. The framework captures spatial demand and modal
characteristics, considers intermodal transfers and express services,
determines transit infrastructure investment and path flows, and generates
transit routes. A system-optimal multimodal transit network is designed with
minimum total door-to-door generalized costs of users and operators, satisfying
transit origin-destination demand within a pre-set infrastructure budget.
Firstly, the geography, demand, and modes in each zone are characterized with
continuous approximation. The decisions of network link investment and
multimodal path flows in zonal connection optimization are formulated as a
minimum-cost multi-commodity network flow (MCNF) problem and solved efficiently
with a mixed-integer linear programming (MILP) solver. Subsequently, the route
generation problem is solved by expanding the MCNF formulation to minimize
intramodal transfers. The model is illustrated through a set of experiments
with the Chicago network comprised of 50 zones and seven modes, under three
scenarios. The computational results present savings in traveler journey time
and operator cost demonstrating the potential benefits of collaboration between
multimodal transit systems and SAMS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 18 figures, accepted for publication in Transportation
  Research Part C: Emerging Technologies, and presentation in the 25th
  International Symposium on Transportation and Traffic Theory (ISTTT25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aggregate Model of District Heating Network for Integrated Energy
  Dispatch: A Physically Informed Data-Driven Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10483v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10483v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Lu, Zihang Gao, Yong Sun, Suhan Zhang, Baoju Li, Chengliang Hao, Yijun Xu, Wei Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The district heating network (DHN) is essential in enhancing the operational
flexibility of integrated energy systems (IES). Yet, it is hard to obtain an
accurate and concise DHN model for the operation owing to complicated network
features and imperfect measurements. Considering this, this paper proposes a
physical-ly informed data-driven aggregate model (AGM) for the DHN, providing a
concise description of the source-load relationship of DHN without exposing
network details. First, we derive the analytical relationship between the state
variables of the source and load nodes of the DHN, offering a physical
fundament for the AGM. Second, we propose a physics-informed estimator for the
AGM that is robust to low-quality measurements, in which the physical
constraints associated with the parameter normalization and sparsity are
embedded to improve the accuracy and robustness. Finally, we propose a
physics-enhanced algorithm to solve the nonlinear estimator with non-closed
constraints efficiently. Simulation results verify the effectiveness of the
proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural-artificial hybrid swarm: Cyborg-insect group navigation in
  unknown obstructed soft terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Phuoc Thanh Tran Ngoc, Huu Duoc Nguyen, Duc Long Le, Quang Huy Ha, Kazuki Kai, Yu Xiang See To, Yaosheng Deng, Jie Song, Naoki Wakamiya, Hirotaka Sato, Masaki Ogura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating multi-robot systems in complex terrains has always been a
challenging task. This is due to the inherent limitations of traditional robots
in collision avoidance, adaptation to unknown environments, and sustained
energy efficiency. In order to overcome these limitations, this research
proposes a solution by integrating living insects with miniature electronic
controllers to enable robotic-like programmable control, and proposing a novel
control algorithm for swarming. Although these creatures, called cyborg
insects, have the ability to instinctively avoid collisions with neighbors and
obstacles while adapting to complex terrains, there is a lack of literature on
the control of multi-cyborg systems. This research gap is due to the difficulty
in coordinating the movements of a cyborg system under the presence of insects'
inherent individual variability in their reactions to control input. In
response to this issue, we propose a novel swarm navigation algorithm
addressing these challenges. The effectiveness of the algorithm is demonstrated
through an experimental validation in which a cyborg swarm was successfully
navigated through an unknown sandy field with obstacles and hills. This
research contributes to the domain of swarm robotics and showcases the
potential of integrating biological organisms with robotics and control theory
to create more intelligent autonomous systems with real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation
  in cluttered environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manan Tayal, Shishir Kolathaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In fields such as mining, search and rescue, and archaeological exploration,
ensuring real-time, collision-free navigation of robots in confined, cluttered
environments is imperative. Despite the value of established path planning
algorithms, they often face challenges in convergence rates and handling
dynamic infeasibilities. Alternative techniques like collision cones struggle
to accurately represent complex obstacle geometries. This paper introduces a
novel category of control barrier functions, known as Polygonal Cone Control
Barrier Function (PolyC2BF), which addresses overestimation and computational
complexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming
(QP) problem, proves effective in facilitating collision-free movement of
multiple robots in complex environments. The efficacy of this approach is
further demonstrated through PyBullet simulations on quadruped (unicycle
model), and crazyflie 2.1 (quadrotor model) in cluttered environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 Pages, 6 Figures. Accepted at European Control Conference (ECC)
  2024. arXiv admin note: text overlap with arXiv:2303.15871</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Commutation Design: Applied to Switched Reluctance Motors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max van Meer, Gert Witvoet, Tom Oomen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Switched Reluctance Motors (SRMs) are cost-effective electric actuators that
utilize magnetic reluctance to generate torque, with torque ripple arising from
unaccounted manufacturing defects in the rotor tooth geometry. This paper aims
to design a versatile, resource-efficient commutation function for accurate
control of a range of SRMs, mitigating torque ripple despite manufacturing
variations across SRMs and individual rotor teeth. The developed commutation
function optimally distributes current between coils by leveraging the variance
in the torque-current-angle model and is designed with few parameters for easy
integration on affordable hardware. Monte Carlo simulations and experimental
results show a tracking error reduction of up to 31% and 11%, respectively. The
developed approach is beneficial for applications using a single driver for
multiple systems and those constrained by memory or modeling effort, providing
an economical solution for improved tracking performance and reduced acoustic
noise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures. Final version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Performance, Calibration Time and Efficiency in Brain-Machine
  Interfaces through Transfer Learning and Wearable EEG Technology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07798v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07798v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaying Wang, Lan Mei, Victor Kartsch, Andrea Cossettini, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain-machine interfaces (BMIs) have emerged as a transformative force in
assistive technologies, empowering individuals with motor impairments by
enabling device control and facilitating functional recovery. However, the
persistent challenge of inter-session variability poses a significant hurdle,
requiring time-consuming calibration at every new use. Compounding this issue,
the low comfort level of current devices further restricts their usage. To
address these challenges, we propose a comprehensive solution that combines a
tiny CNN-based Transfer Learning (TL) approach with a comfortable, wearable EEG
headband. The novel wearable EEG device features soft dry electrodes placed on
the headband and is capable of on-board processing. We acquire multiple
sessions of motor-movement EEG data and achieve up to 96% inter-session
accuracy using TL, greatly reducing the calibration time and improving
usability. By executing the inference on the edge every 100ms, the system is
estimated to achieve 30h of battery life. The comfortable BMI setup with tiny
CNN and TL paves the way to future on-device continual learning, essential for
tackling inter-session variability and improving usability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nigel -- Mechatronic Design and Robust Sim2Real Control of an
  Over-Actuated Autonomous Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.11542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.11542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Vilas Samak, Tanmay Vilas Samak, Javad Mohammadpour Velni, Venkat Narayan Krovi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation to reality (sim2real) transfer from a dynamics and controls
perspective usually involves re-tuning or adapting the designed algorithms to
suit real-world operating conditions, which often violates the performance
guarantees established originally. This work presents a generalizable framework
for achieving reliable sim2real transfer of autonomy-oriented control systems
using multi-model multi-objective robust optimal control synthesis, which lends
well to uncertainty handling and disturbance rejection with theoretical
guarantees. Particularly, this work is centered around a novel
actuation-redundant scaled autonomous vehicle called Nigel, with independent
all-wheel drive and independent all-wheel steering architecture, whose enhanced
configuration space bodes well for robust control applications. To this end, we
present the mechatronic design, dynamics modeling, parameter identification,
and robust stabilizing as well as tracking control of Nigel using the proposed
framework, with exhaustive experimentation and benchmarking in simulation as
well as real-world settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensuring Disturbance Rejection Performance by Synthesizing
  Grid-Following and Grid-Forming Inverters in Power Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16488v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16488v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyilong Ma, Huanhai Xin, Zhiyi Li, Linbin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To satisfy dynamic requirements of power systems, it is imperative for
grid-tied inverters to ensure good disturbance rejection performance (DRP)
under variable grid conditions. This letter discovers and theoretically proves
that for general networks, synthesizing grid-following (GFL) inverters and
grid-forming (GFM) inverters can always more effectively ensure the DRP of
multiple inverters, as compared to homogeneous inverter-based systems that
solely utilize either GFL or GFM inverters. The synthesis of GFL inverters and
GFM inverters can concurrently increase the smallest eigenvalue and decrease
the largest eigenvalue of the network grounded Laplacian matrix. This can be
equivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM
inverters, thereby ensuring the DRP of inverters both in weak and strong grids.
The results reveal the necessity of synthesizing diverse inverter control
schemes from the network-based perspective. Sensitivity function-based tests
and real-time simulations validate our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaT: Constraints as Terminations for Legged Locomotion Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Chane-Sane, Pierre-Alexandre Leziart, Thomas Flayols, Olivier Stasse, Philippe Souères, Nicolas Mansard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has demonstrated impressive results in
solving complex robotic tasks such as quadruped locomotion. Yet, current
solvers fail to produce efficient policies respecting hard constraints. In this
work, we advocate for integrating constraints into robot learning and present
Constraints as Terminations (CaT), a novel constrained RL algorithm. Departing
from classical constrained RL formulations, we reformulate constraints through
stochastic terminations during policy learning: any violation of a constraint
triggers a probability of terminating potential future rewards the RL agent
could attain. We propose an algorithmic approach to this formulation, by
minimally modifying widely used off-the-shelf RL algorithms in robot learning
(such as Proximal Policy Optimization). Our approach leads to excellent
constraint adherence without introducing undue complexity and computational
overhead, thus mitigating barriers to broader adoption. Through empirical
evaluation on the real quadruped robot Solo crossing challenging obstacles, we
demonstrate that CaT provides a compelling solution for incorporating
constraints into RL frameworks. Videos and code are available at
https://constraints-as-terminations.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://constraints-as-terminations.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Usage-Specific Survival Modeling Based on Operational Data and Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Mattias Krysander, Erik Frisk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate predictions of when a component will fail are crucial when planning
maintenance, and by modeling the distribution of these failure times, survival
models have shown to be particularly useful in this context. The presented
methodology is based on conventional neural network-based survival models that
are trained using data that is continuously gathered and stored at specific
times, called snapshots. An important property of this type of training data is
that it can contain more than one snapshot from a specific individual which
results in that standard maximum likelihood training can not be directly
applied since the data is not independent. However, the papers show that if the
data is in a specific format where all snapshot times are the same for all
individuals, called homogeneously sampled, maximum likelihood training can be
applied and produce desirable results. In many cases, the data is not
homogeneously sampled and in this case, it is proposed to resample the data to
make it homogeneously sampled. How densely the dataset is sampled turns out to
be an important parameter; it should be chosen large enough to produce good
results, but this also increases the size of the dataset which makes training
slow. To reduce the number of samples needed during training, the paper also
proposes a technique to, instead of resampling the dataset once before the
training starts, randomly resample the dataset at the start of each epoch
during the training. The proposed methodology is evaluated on both a simulated
dataset and an experimental dataset of starter battery failures. The results
show that if the data is homogeneously sampled the methodology works as
intended and produces accurate survival models. The results also show that
randomly resampling the dataset on each epoch is an effective way to reduce the
size of the training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear model reduction for operator learning <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Eivazi, Stefan Wittek, Andreas Rausch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Operator learning provides methods to approximate mappings between
infinite-dimensional function spaces. Deep operator networks (DeepONets) are a
notable architecture in this field. Recently, an extension of DeepONet based on
model reduction and neural networks, proper orthogonal decomposition
(POD)-DeepONet, has been able to outperform other architectures in terms of
accuracy for several benchmark tests. We extend this idea towards nonlinear
model order reduction by proposing an efficient framework that combines neural
networks with kernel principal component analysis (KPCA) for operator learning.
Our results demonstrate the superior performance of KPCA-DeepONet over
POD-DeepONet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a Tiny Paper at ICLR 2024 (Notable)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality <span class="highlight-title">Prediction</span> Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Traffic Flow <span class="highlight-title">Prediction</span> using Cellular Automata-based
  Model and CNN-LSTM architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaohui Yang, Kshitij Jerath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have attempted to use deep learning to predict future states of
traffic flow, but have met with mixed results. These approaches face two key
challenges. First, training deep learning neural networks requires large
amounts of training data which are not yet easily available for traffic flow
systems. Second, even when data is available, the neural networks require
access to historical data that covers most possible traffic flow dynamics to
successfully predict future traffic states. Specifically, these deep learning
approaches do not fully leverage domain-knowledge about traffic flow dynamics,
despite a significant existing knowledge-base. In this work, we propose to
solve both issues using a Convolutional Neural Network (CNNs) with Long Short
Term Memory (LSTM) deep learning architecture to successfully predict traffic
flow, while leveraging a cellular automata-based statistical mechanics model of
traffic flow to generate training and test data. Another major contribution of
this paper is the insight that training data for a large traffic system can
actually be sampled from the simulations of a much smaller traffic system. This
is achieved through observing that the normalized energy distribution of the
statistical mechanics model is scale invariant, which significantly eases the
burden of data generation for large scale traffic systems. The resulting
simulations indicate good agreement between the predicted and the true traffic
flow dynamics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conditional Wasserstein Distances with Applications in Bayesian OT Flow
  Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jannis Chemseddine, Paul Hagemann, Christian Wald, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In inverse problems, many conditional generative models approximate the
posterior measure by minimizing a distance between the joint measure and its
learned approximation. While this approach also controls the distance between
the posterior measures in the case of the Kullback--Leibler divergence, this is
in general not hold true for the Wasserstein distance. In this paper, we
introduce a conditional Wasserstein distance via a set of restricted couplings
that equals the expected Wasserstein distance of the posteriors. Interestingly,
the dual formulation of the conditional Wasserstein-1 flow resembles losses in
the conditional Wasserstein GAN literature in a quite natural way. We derive
theoretical properties of the conditional Wasserstein distance, characterize
the corresponding geodesics and velocity fields as well as the flow ODEs.
Subsequently, we propose to approximate the velocity fields by relaxing the
conditional Wasserstein distance. Based on this, we propose an extension of OT
Flow Matching for solving Bayesian inverse problems and demonstrate its
numerical advantages on an inverse problem and class-conditional image
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper supersedes arXiv:2310.13433</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fpga-Based Neural Thrust Controller for UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharif Azem, David Scheunert, Mengguang Li, Jonas Gehrunger, Kai Cui, Christian Hochberger, Heinz Koepp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of unmanned aerial vehicles (UAVs) has improved a variety of
fields by providing a versatile, cost-effective and accessible platform for
implementing state-of-the-art algorithms. To accomplish a broader range of
tasks, there is a growing need for enhanced on-board computing to cope with
increasing complexity and dynamic environmental conditions. Recent advances
have seen the application of Deep Neural Networks (DNNs), particularly in
combination with Reinforcement Learning (RL), to improve the adaptability and
performance of UAVs, especially in unknown environments. However, the
computational requirements of DNNs pose a challenge to the limited computing
resources available on many UAVs. This work explores the use of Field
Programmable Gate Arrays (FPGAs) as a viable solution to this challenge,
offering flexibility, high performance, energy and time efficiency. We propose
a novel hardware board equipped with an Artix-7 FPGA for a popular open-source
micro-UAV platform. We successfully validate its functionality by implementing
an RL-based low-level controller using real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InceptionTime vs. Wavelet -- A comparison for time series classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Klenkert, Daniel Schaeffer, Julian Stauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks were used to classify infrasound data. Two different
approaches were compared. One based on the direct classification of time series
data, using a custom implementation of the InceptionTime network. For the other
approach, we generated 2D images of the wavelet transformation of the signals,
which were subsequently classified using a ResNet implementation. Choosing
appropriate hyperparameter settings, both achieve a classification accuracy of
above 90 %, with the direct approach reaching 95.2 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representatividad Muestral en la Incertidumbre Simétrica Multivariada
  para la Selección de Atributos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gustavo Sosa-Cabrera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we analyze the behavior of the multivariate symmetric
uncertainty (MSU) measure through the use of statistical simulation techniques
under various mixes of informative and non-informative randomly generated
features. Experiments show how the number of attributes, their cardinalities,
and the sample size affect the MSU. In this thesis, through observation of
results, it is proposed an heuristic condition that preserves good quality in
the MSU under different combinations of these three factors, providing a new
useful criterion to help drive the process of dimension reduction.
  --
  En el presente trabajo hemos analizado el comportamiento de una versi\'on
multivariada de la incertidumbre sim\'etrica a trav\'es de t\'ecnicas de
simulaci\'on estad\'isticas sobre varias combinaciones de atributos
informativos y no-informativos generados de forma aleatoria. Los experimentos
muestran como el n\'umero de atributos, sus cardinalidades y el tama\~no
muestral afectan al MSU como medida. En esta tesis, mediante la observaci\'on
de resultados hemos propuesto una condici\'on que preserva una buena calidad en
el MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual
provee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\'on
de dimensionalidad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>52 pages, in Spanish. Advisors: Miguel Garc\'ia-Torres, Santiago
  G\'omez-Guerrero, Christian E. Schaerer Serra</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NL-ITI: Optimizing Probing and Intervention for Improvement of ITI
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakub Hoscilowicz, Adam Wiacek, Jan Chojnacki, Adam Cieslak, Leszek Michon, Vitalii Urbanevych, Artur Janicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLM) are prone to returning false information. It
constitutes one of major challenges in the AI field. In our work, we explore
paradigm introduced by Inference-Time-Intervention (ITI). In first stage, it
identifies attention heads, which contain the highest amount of desired type of
knowledge (e.g., truthful). Afterwards, during inference, LLM activations are
shifted for chosen subset of attention heads. We further improved the ITI
framework by introducing a nonlinear probing and multi-token intervention -
Non-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice
benchmarks, including TruthfulQA, on which we report around 14% MC1 metric
improvement with respect to the baseline ITI results. NL-ITI achieves also
encouraging results on other testsets - on Business Ethics subdomain of MMLU,
around 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI
performs better while being less invasive in the behavior of LLM at the same
time (as measured by Kullback-Leibler divergence).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/Samsung/NL-ITI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact Checking Beyond Training Set 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Payam Karisani, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the veracity of everyday claims is time consuming and in some
cases requires domain expertise. We empirically demonstrate that the commonly
used fact checking pipeline, known as the retriever-reader, suffers from
performance deterioration when it is trained on the labeled data from one
domain and used in another domain. Afterwards, we delve into each component of
the pipeline and propose novel algorithms to address this problem. We propose
an adversarial algorithm to make the retriever component robust against
distribution shift. Our core idea is to initially train a bi-encoder on the
labeled source data, and then, to adversarially train two separate document and
claim encoders using unlabeled target data. We then focus on the reader
component and propose to train it such that it is insensitive towards the order
of claims and evidence documents. Our empirical evaluations support the
hypothesis that such a reader shows a higher robustness against distribution
shift. To our knowledge, there is no publicly available multi-topic fact
checking dataset. Thus, we propose a simple automatic method to re-purpose two
well-known fact checking datasets. We then construct eight fact checking
scenarios from these datasets, and compare our model to a set of strong
baseline models, including recent domain adaptation models that use GPT4 for
generating synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Network-Based Piecewise Survival Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olov Holmer, Erik Frisk, Mattias Krysander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a family of neural network-based survival models is presented.
The models are specified based on piecewise definitions of the hazard function
and the density function on a partitioning of the time; both constant and
linear piecewise definitions are presented, resulting in a family of four
models. The models can be seen as an extension of the commonly used
discrete-time and piecewise exponential models and thereby add flexibility to
this set of standard models. Using a simulated dataset the models are shown to
perform well compared to the highly expressive, state-of-the-art energy-based
model, while only requiring a fraction of the computation time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers-based architectures for stroke segmentation: A <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yalda Zafari-Ghadim, Essam A. Rashed, Mohamed Mabrok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stroke remains a significant global health concern, necessitating precise and
efficient diagnostic tools for timely intervention and improved patient
outcomes. The emergence of deep learning methodologies has transformed the
landscape of medical image analysis. Recently, Transformers, initially designed
for natural language processing, have exhibited remarkable capabilities in
various computer vision applications, including medical image analysis. This
comprehensive review aims to provide an in-depth exploration of the
cutting-edge Transformer-based architectures applied in the context of stroke
segmentation. It commences with an exploration of stroke pathology, imaging
modalities, and the challenges associated with accurate diagnosis and
segmentation. Subsequently, the review delves into the fundamental ideas of
Transformers, offering detailed insights into their architectural intricacies
and the underlying mechanisms that empower them to effectively capture complex
spatial information within medical images. The existing literature is
systematically categorized and analyzed, discussing various approaches that
leverage Transformers for stroke segmentation. A critical assessment is
provided, highlighting the strengths and limitations of these methods,
including considerations of performance and computational efficiency.
Additionally, this review explores potential avenues for future research and
development
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fusion approaches for emotion recognition from speech using acoustic and
  text-based features <span class="chip">SP 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Pepino, Pablo Riera, Luciana Ferrer, Agustin Gravano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study different approaches for classifying emotions from
speech using acoustic and text-based features. We propose to obtain
contextualized word embeddings with BERT to represent the information contained
in speech transcriptions and show that this results in better performance than
using Glove embeddings. We also propose and compare different strategies to
combine the audio and text modalities, evaluating them on IEMOCAP and
MSP-PODCAST datasets. We find that fusing acoustic and text-based systems is
beneficial on both datasets, though only subtle differences are observed across
the evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect
that the criteria used to define the cross-validation folds have on results. In
particular, the standard way of creating folds for this dataset results in a
highly optimistic estimation of performance for the text-based system,
suggesting that some previous works may overestimate the advantage of
incorporating transcriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages. Accepted in ICASSP 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ First Experiences with the Identification of People at Risk for Diabetes
  in Argentina using Machine Learning Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enzo Rucci, Gonzalo Tittarelli, Franco Ronchetti, Jorge F. Elgart, Laura Lanzarini, Juan José Gagliardino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for
medicine due to the absence of pathogenic symptoms and the lack of known
associated risk factors. Even though some proposals for machine learning models
enable the identification of people at risk, the nature of the condition makes
it so that a model suitable for one population may not necessarily be suitable
for another. In this article, the development and assessment of predictive
models to identify people at risk for T2D and PD specifically in Argentina are
discussed. First, the database was thoroughly preprocessed and three specific
datasets were generated considering a compromise between the number of records
and the amount of available variables. After applying 5 different
classification models, the results obtained show that a very good performance
was observed for two datasets with some of these models. In particular, RF, DT,
and ANN demonstrated great classification power, with good values for the
metrics under consideration. Given the lack of this type of tool in Argentina,
this work represents the first step towards the development of more
sophisticated models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Computer Science - CACIC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Lipschitz Estimation for CNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusuf Sulehman, Tingting Mu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the Lipschitz constant of deep neural networks is of growing
interest as it is useful for informing on generalisability and adversarial
robustness. Convolutional neural networks (CNNs) in particular, underpin much
of the recent success in computer vision related applications. However,
although existing methods for estimating the Lipschitz constant can be tight,
they have limited scalability when applied to CNNs. To tackle this, we propose
a novel method to accelerate Lipschitz constant estimation for CNNs. The core
idea is to divide a large convolutional block via a joint layer and width-wise
partition, into a collection of smaller blocks. We prove an upper-bound on the
Lipschitz constant of the larger block in terms of the Lipschitz constants of
the smaller blocks. Through varying the partition factor, the resulting method
can be adjusted to prioritise either accuracy or scalability and permits
parallelisation. We demonstrate an enhanced scalability and comparable accuracy
to existing baselines through a range of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Peridynamic Neural Operators: Discover Biotissue
  Constitutive Law and Microstructure From Digital Image Correlation
  Measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siavash Jafarzadeh, Stewart Silling, Lu Zhang, Colton Ross, Chung-Hao Lee, S. M. Rakibur Rahman, Shuodao Wang, Yue Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human tissues are highly organized structures with specific collagen fiber
arrangements varying from point to point. The effects of such heterogeneity
play an important role for tissue function, and hence it is of critical to
discover and understand the distribution of such fiber orientations from
experimental measurements, such as the digital image correlation data. To this
end, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)
approach, for data-driven constitutive modeling of heterogeneous anisotropic
materials. The goal is to learn both a nonlocal constitutive law together with
the material microstructure, in the form of a heterogeneous fiber orientation
field, from loading field-displacement field measurements. To this end, we
propose a two-phase learning approach. Firstly, we learn a homogeneous
constitutive law in the form of a neural network-based kernel function and a
nonlocal bond force, to capture complex homogeneous material responses from
data. Then, in the second phase we reinitialize the learnt bond force and the
kernel function, and training them together with a fiber orientation field for
each material point. Owing to the state-based peridynamic skeleton, our
HeteroPNO-learned material models are objective and have the balance of linear
and angular momentum guaranteed. Moreover, the effects from heterogeneity and
nonlinear constitutive relationship are captured by the kernel function and the
bond force respectively, enabling physical interpretability. As a result, our
HeteroPNO architecture can learn a constitutive model for a biological tissue
with anisotropic heterogeneous response undergoing large deformation regime.
Moreover, the framework is capable to provide displacement and stress field
predictions for new and unseen loading instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency
  Attacks in Computer Vision <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Müller, Erwin Quiring
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Resource efficiency plays an important role for machine learning nowadays.
The energy and decision latency are two critical aspects to ensure a
sustainable and practical application. Unfortunately, the energy consumption
and decision latency are not robust against adversaries. Researchers have
recently demonstrated that attackers can compute and submit so-called sponge
examples at inference time to increase the energy consumption and decision
latency of neural networks. In computer vision, the proposed strategy crafts
inputs with less activation sparsity which could otherwise be used to
accelerate the computation. In this paper, we analyze the mechanism how these
energy-latency attacks reduce activation sparsity. In particular, we find that
input uniformity is a key enabler. A uniform image, that is, an image with
mostly flat, uniformly colored surfaces, triggers more activations due to a
specific interplay of convolution, batch normalization, and ReLU activation.
Based on these insights, we propose two new simple, yet effective strategies
for crafting sponge examples: sampling images from a probability distribution
and identifying dense, yet inconspicuous inputs in natural datasets. We
empirically examine our findings in a comprehensive evaluation with multiple
image classification models and show that our attack achieves the same sparsity
effect as prior sponge-example methods, but at a fraction of computation
effort. We also show that our sponge examples transfer between different neural
networks. Finally, we discuss applications of our findings for the good by
improving efficiency by increasing sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the DLSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One flow to correct them all: improving <span class="highlight-title">simulation</span>s in high-energy
  physics with a single normalising flow and a switch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caio Cesar Daumann, Mauro Donega, Johannes Erdmann, Massimiliano Galli, Jan Lukas Späh, Davide Valsecchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulated events are key ingredients in almost all high-energy physics
analyses. However, imperfections in the simulation can lead to sizeable
differences between the observed data and simulated events. The effects of such
mismodelling on relevant observables must be corrected either effectively via
scale factors, with weights or by modifying the distributions of the
observables and their correlations. We introduce a correction method that
transforms one multidimensional distribution (simulation) into another one
(data) using a simple architecture based on a single normalising flow with a
boolean condition. We demonstrate the effectiveness of the method on a
physics-inspired toy dataset with non-trivial mismodelling of several
observables and their correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Optimizing Hyperparameters for Quantum Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabrina Herbst, Vincenzo De Maio, Ivona Brandic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing capabilities of Machine Learning (ML) models go hand in hand
with an immense amount of data and computational power required for training.
Therefore, training is usually outsourced into HPC facilities, where we have
started to experience limits in scaling conventional HPC hardware, as theorized
by Moore's law. Despite heavy parallelization and optimization efforts, current
state-of-the-art ML models require weeks for training, which is associated with
an enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum
Machine Learning (QML), can offer significant theoretical speed-ups and
enhanced expressive power. However, training QML models requires tuning various
hyperparameters, which is a nontrivial task and suboptimal choices can highly
affect the trainability and performance of the models. In this study, we
identify the most impactful hyperparameters and collect data about the
performance of QML models. We compare different configurations and provide
researchers with performance data and concrete suggestions for hyperparameter
selection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SteinGen: Generating Fidelitous and Diverse Graph Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gesine Reinert, Wenkai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating graphs that preserve characteristic structures while promoting
sample diversity can be challenging, especially when the number of graph
observations is small. Here, we tackle the problem of graph generation from
only one observed graph. The classical approach of graph generation from
parametric models relies on the estimation of parameters, which can be
inconsistent or expensive to compute due to intractable normalisation
constants. Generative modelling based on machine learning techniques to
generate high-quality graph samples avoids parameter estimation but usually
requires abundant training samples. Our proposed generating procedure,
SteinGen, which is phrased in the setting of graphs as realisations of
exponential random graph models, combines ideas from Stein's method and MCMC by
employing Markovian dynamics which are based on a Stein operator for the target
model. SteinGen uses the Glauber dynamics associated with an estimated Stein
operator to generate a sample, and re-estimates the Stein operator from the
sample after every sampling step. We show that on a class of exponential random
graph models this novel "estimation and re-estimation" generation strategy
yields high distributional similarity (high fidelity) to the original data,
combined with high sample diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-Robust Keyword Spotting through Self-supervised Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Mørk, Holger Severin Bovbjerg, Gergely Kiss, Zheng-Hua Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice assistants are now widely available, and to activate them a keyword
spotting (KWS) algorithm is used. Modern KWS systems are mainly trained using
supervised learning methods and require a large amount of labelled data to
achieve a good performance. Leveraging unlabelled data through self-supervised
learning (SSL) has been shown to increase the accuracy in clean conditions.
This paper explores how SSL pretraining such as Data2Vec can be used to enhance
the robustness of KWS models in noisy conditions, which is under-explored.
  Models of three different sizes are pretrained using different pretraining
approaches and then fine-tuned for KWS. These models are then tested and
compared to models trained using two baseline supervised learning methods, one
being standard training using clean data and the other one being multi-style
training (MTR). The results show that pretraining and fine-tuning on clean data
is superior to supervised learning on clean data across all testing conditions,
and superior to supervised MTR for testing conditions of SNR above 5 dB. This
indicates that pretraining alone can increase the model's robustness. Finally,
it is found that using noisy data for pretraining models, especially with the
Data2Vec-denoising approach, significantly enhances the robustness of KWS
models in noisy conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-aware semantic relevance predicting Chinese sentence reading 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, several influential computational models and metrics have
been proposed to predict how humans comprehend and process sentence. One
particularly promising approach is contextual semantic similarity. Inspired by
the attention algorithm in Transformer and human memory mechanisms, this study
proposes an ``attention-aware'' approach for computing contextual semantic
relevance. This new approach takes into account the different contributions of
contextual parts and the expectation effect, allowing it to incorporate
contextual information fully. The attention-aware approach also facilitates the
simulation of existing reading models and evaluate them. The resulting
``attention-aware'' metrics of semantic relevance can more accurately predict
fixation durations in Chinese reading tasks recorded in an eye-tracking corpus
than those calculated by existing approaches. The study's findings further
provide strong support for the presence of semantic preview benefits in Chinese
naturalistic reading. Furthermore, the attention-aware metrics of semantic
relevance, being memory-based, possess high interpretability from both
linguistic and cognitive standpoints, making them a valuable computational tool
for modeling eye-movements in reading and further gaining insight into the
process of language comprehension. Our approach underscores the potential of
these metrics to advance our comprehension of how humans understand and process
language, ultimately leading to a better understanding of language
comprehension and processing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> skscope: Fast Sparsity-Constrained Optimization in Python 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezhi Wang, Jin Zhu, Peng Chen, Huiyang Peng, Xiaoke Zhang, Anran Wang, <span class="highlight-author">Yu Zheng</span>, Junxian Zhu, Xueqin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying iterative solvers on sparsity-constrained optimization (SCO)
requires tedious mathematical deduction and careful programming/debugging that
hinders these solvers' broad impact. In the paper, the library skscope is
introduced to overcome such an obstacle. With skscope, users can solve the SCO
by just programming the objective function. The convenience of skscope is
demonstrated through two examples in the paper, where sparse linear regression
and trend filtering are addressed with just four lines of code. More
importantly, skscope's efficient implementation allows state-of-the-art solvers
to quickly attain the sparse solution regardless of the high dimensionality of
parameter space. Numerical experiments reveal the available solvers in skscope
can achieve up to 80x speedup on the competing relaxation solutions obtained
via the benchmarked convex solver. skscope is published on the Python Package
Index (PyPI) and Conda, and its source code is available at:
https://github.com/abess-team/skscope.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Robust Reinforcement-Learning: Principles and Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taku Yamagata, Raul Santos-Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has shown remarkable success in solving
relatively complex tasks, yet the deployment of RL systems in real-world
scenarios poses significant challenges related to safety and robustness. This
paper aims to identify and further understand those challenges thorough the
exploration of the main dimensions of the safe and robust RL landscape,
encompassing algorithmic, ethical, and practical considerations. We conduct a
comprehensive review of methodologies and open problems that summarizes the
efforts in recent years to address the inherent risks associated with RL
applications.
  After discussing and proposing definitions for both safe and robust RL, the
paper categorizes existing research works into different algorithmic approaches
that enhance the safety and robustness of RL agents. We examine techniques such
as uncertainty estimation, optimisation methodologies, exploration-exploitation
trade-offs, and adversarial training. Environmental factors, including
sim-to-real transfer and domain adaptation, are also scrutinized to understand
how RL systems can adapt to diverse and dynamic surroundings. Moreover, human
involvement is an integral ingredient of the analysis, acknowledging the broad
set of roles that humans can take in this context.
  Importantly, to aid practitioners in navigating the complexities of safe and
robust RL implementation, this paper introduces a practical checklist derived
from the synthesized literature. The checklist encompasses critical aspects of
algorithm design, training environment considerations, and ethical guidelines.
It will serve as a resource for developers and policymakers alike to ensure the
responsible deployment of RL systems in many application domains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhihao Duan, Yuning Huang, Fengqing Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies reveal a significant theoretical link between variational
autoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to
estimate the theoretical upper bound of the information rate-distortion
function of images. Such estimated theoretical bounds substantially exceed the
performance of existing neural image codecs (NICs). To narrow this gap, we
propose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The
proposed BG-VAE leverages the theoretical bound to guide the NIC model towards
enhanced performance. We implement the BG-VAE using Hierarchical VAEs and
demonstrate its effectiveness through extensive experiments. Along with
advanced neural network blocks, we provide a versatile, variable-rate NIC that
outperforms existing methods when considering both rate-distortion performance
and computational complexity. The code is available at BG-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 IEEE International Conference on Multimedia and Expo (ICME2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Plays a Pivotal Role in the Object-Attribute Compositional
  Generalization of CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Abbasi, Mohammad Samiei, Mohammad Hossein Rohban, Mahdieh Soleymani Baghshah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models, such as CLIP, have shown promising
Out-of-Distribution (OoD) generalization under various types of distribution
shifts. Recent studies attempted to investigate the leading cause of this
capability. In this work, we follow the same path, but focus on a specific type
of OoD data - images with novel compositions of attribute-object pairs - and
study whether such models can successfully classify those images into
composition classes. We carefully designed an authentic image test dataset
called ImageNet-AO, consisting of attributes for objects that are unlikely
encountered in the CLIP training sets. We found that CLIPs trained with large
datasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude
improvement in effective compositional OoD generalization compared to both
supervised models and CLIPs trained with smaller datasets, such as CC-12M and
YFCC-15M. Our results provide evidence that the scale and diversity of training
data and language supervision play a key role in unlocking the compositional
generalization abilities of vision-language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Oral accepted at OODCV 2023(http://www.ood-cv.org)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Algorithms for Regularized Nonnegative Scale-invariant
  Low-rank Approximation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy E. Cohen, Valentin Leplat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularized nonnegative low-rank approximations such as sparse Nonnegative
Matrix Factorization or sparse Nonnegative Tucker Decomposition are an
important branch of dimensionality reduction models with enhanced
interpretability. However, from a practical perspective, the choice of
regularizers and regularization coefficients, as well as the design of
efficient algorithms, is challenging because of the multifactor nature of these
models and the lack of theory to back these choices. This paper aims at
improving upon these issues. By studying a more general model called the
Homogeneous Regularized Scale-Invariant, we prove that the scale-invariance
inherent to low-rank approximation models causes an implicit regularization
with both unexpected beneficial and detrimental effects. This observation
allows to better understand the effect of regularization functions in low-rank
approximation models, to guide the choice of the regularization
hyperparameters, and to design balancing strategies to enhance the convergence
speed of dedicated optimization algorithms. Some of these results were already
known but restricted to specific instances of regularized low-rank
approximations. We also derive a generic Majorization Minimization algorithm
that handles many regularized nonnegative low-rank approximations, with
convergence guarantees. We showcase our contributions on sparse Nonnegative
Matrix Factorization, ridge-regularized Canonical Polyadic decomposition and
sparse Nonnegative Tucker Decomposition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection
  of Pathological Pulmonary CT scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aissam Djahnine, Alexandre Popoff, Emilien Jupin-Delevaux, Vincent Cottin, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised pathology detection can be implemented by training a model on
healthy data only and measuring the deviation from the training set upon
inference, for example with CNN-based feature extraction and one-class
classifiers, or reconstruction-score-based methods such as AEs, GANs and
Diffusion models. Normalizing Flows (NF) have the ability to directly learn the
probability distribution of training examples through an invertible
architecture. We leverage this property in a novel 3D NF-based model named
CT-3DFlow, specifically tailored for patient-level pulmonary pathology
detection in chest CT data. Our model is trained unsupervised on healthy 3D
pulmonary CT patches, and detects deviations from its log-likelihood
distribution as anomalies. We aggregate patches-level likelihood values from a
patient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.
Out-of-distribution detection performance is evaluated using expert annotations
on a separate chest CT test dataset, outperforming other state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Maximum Consensus over Noisy Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Naveen K. D. Venkategowda, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a distributed algorithm, termed noise-robust distributed maximum
consensus (RD-MC), for estimating the maximum value within a multi-agent
network in the presence of noisy communication links. Our approach entails
redefining the maximum consensus problem as a distributed optimization problem,
allowing a solution using the alternating direction method of multipliers.
Unlike existing algorithms that rely on multiple sets of noise-corrupted
estimates, RD-MC employs a single set, enhancing both robustness and
efficiency. To further mitigate the effects of link noise and improve
robustness, we apply moving averaging to the local estimates. Through extensive
simulations, we demonstrate that RD-MC is significantly more robust to
communication link noise compared to existing maximum-consensus algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 7 figures, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct mineral content <span class="highlight-title">prediction</span> from drill core images via transfer
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romana Boiger, Sergey V. Churakov, Ignacio Ballester Llagaria, Georg Kosakowski, Raphael Wüst, Nikolaos I. Prasianakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep subsurface exploration is important for mining, oil and gas industries,
as well as in the assessment of geological units for the disposal of chemical
or nuclear waste, or the viability of geothermal energy systems. Typically,
detailed examinations of subsurface formations or units are performed on
cuttings or core materials extracted during drilling campaigns, as well as on
geophysical borehole data, which provide detailed information about the
petrophysical properties of the rocks. Depending on the volume of rock samples
and the analytical program, the laboratory analysis and diagnostics can be very
time-consuming. This study investigates the potential of utilizing machine
learning, specifically convolutional neural networks (CNN), to assess the
lithology and mineral content solely from analysis of drill core images, aiming
to support and expedite the subsurface geological exploration. The paper
outlines a comprehensive methodology, encompassing data preprocessing, machine
learning methods, and transfer learning techniques. The outcome reveals a
remarkable 96.7% accuracy in the classification of drill core segments into
distinct formation classes. Furthermore, a CNN model was trained for the
evaluation of mineral content using a learning data set from multidimensional
log analysis data (silicate, total clay, carbonate). When benchmarked against
laboratory XRD measurements on samples from the cores, both the advanced
multidimensional log analysis model and the neural network approach developed
here provide equally good performance. This work demonstrates that deep
learning and particularly transfer learning can support extracting
petrophysical properties, including mineral content and formation
classification, from drill core images, thus offering a road map for enhancing
model performance and data set quality in image-based analysis of drill cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning in PINNs: Phase transition, total diffusion, and generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18494v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18494v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sokratis J. Anagnostopoulos, Juan Diego Toscano, Nikolaos Stergiopulos, George Em Karniadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the learning dynamics of fully-connected neural networks
through the lens of gradient signal-to-noise ratio (SNR), examining the
behavior of first-order optimizers like Adam in non-convex objectives. By
interpreting the drift/diffusion phases in the information bottleneck theory,
focusing on gradient homogeneity, we identify a third phase termed ``total
diffusion", characterized by equilibrium in the learning rates and homogeneous
gradients. This phase is marked by an abrupt SNR increase, uniform residuals
across the sample space and the most rapid training convergence. We propose a
residual-based re-weighting scheme to accelerate this diffusion in quadratic
loss functions, enhancing generalization. We also explore the information
compression phenomenon, pinpointing a significant saturation-induced
compression of activations at the total diffusion phase, with deeper layers
experiencing negligible information loss. Supported by experimental data on
physics-informed neural networks (PINNs), which underscore the importance of
gradient homogeneity due to their PDE-based sample inter-dependence, our
findings suggest that recognizing phase transitions could refine ML
optimization strategies for improved generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Young-Jae Park, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are five types of trajectory prediction tasks: deterministic,
stochastic, domain adaptation, momentary observation, and few-shot. These
associated tasks are defined by various factors, such as the length of input
paths, data split and pre-processing methods. Interestingly, even though they
commonly take sequential coordinates of observations as input and infer future
paths in the same coordinates as output, designing specialized architectures
for each task is still necessary. For the other task, generality issues can
lead to sub-optimal performances. In this paper, we propose SingularTrajectory,
a diffusion-based universal trajectory prediction framework to reduce the
performance gap across the five tasks. The core of SingularTrajectory is to
unify a variety of human dynamics representations on the associated tasks. To
do this, we first build a Singular space to project all types of motion
patterns from each task into one embedding space. We next propose an adaptive
anchor working in the Singular space. Unlike traditional fixed anchor methods
that sometimes yield unacceptable paths, our adaptive anchor enables correct
anchors, which are put into a wrong location, based on a traversability map.
Finally, we adopt a diffusion-based predictor to further enhance the prototype
paths using a cascaded denoising process. Our unified framework ensures the
generality across various benchmark settings such as input modality, and
trajectory lengths. Extensive experiments on five public benchmarks demonstrate
that SingularTrajectory substantially outperforms existing models, highlighting
its effectiveness in estimating general dynamics of human movements. Code is
publicly available at https://github.com/inhwanbae/SingularTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Language Beat Numerical Regression? Language-Based Multimodal
  Trajectory <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inhwan Bae, Junoh Lee, Hae-Gon Jeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have demonstrated impressive ability in context understanding
and generative performance. Inspired by the recent success of language
foundation models, in this paper, we propose LMTraj (Language-based Multimodal
Trajectory predictor), which recasts the trajectory prediction task into a sort
of question-answering problem. Departing from traditional numerical regression
models, which treat the trajectory coordinate sequence as continuous signals,
we consider them as discrete signals like text prompts. Specially, we first
transform an input space for the trajectory coordinate into the natural
language space. Here, the entire time-series trajectories of pedestrians are
converted into a text prompt, and scene images are described as text
information through image captioning. The transformed numerical and image data
are then wrapped into the question-answering template for use in a language
model. Next, to guide the language model in understanding and reasoning
high-level knowledge, such as scene context and social relationships between
pedestrians, we introduce an auxiliary multi-task question and answering. We
then train a numerical tokenizer with the prompt data. We encourage the
tokenizer to separate the integer and decimal parts well, and leverage it to
capture correlations between the consecutive numbers in the language model.
Lastly, we train the language model using the numerical tokenizer and all of
the question-answer prompts. Here, we propose a beam-search-based most-likely
prediction and a temperature-based multimodal prediction to implement both
deterministic and stochastic inferences. Applying our LMTraj, we show that the
language-based model can be a powerful pedestrian trajectory predictor, and
outperforms existing numerical-based predictor methods. Code is publicly
available at https://github.com/inhwanbae/LMTrajectory .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRESCO: Federated Reinforcement Energy System for Cooperative
  Optimization <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Mauricio Cuadrado, Roberto Alejandro Gutierrez, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in renewable energy is creating new dynamics in the energy grid that
promise to create a cleaner and more participative energy grid, where
technology plays a crucial part in making the required flexibility to achieve
the vision of the next-generation grid. This work presents FRESCO, a framework
that aims to ease the implementation of energy markets using a hierarchical
control architecture of reinforcement learning agents trained using federated
learning. The core concept we are proving is that having greedy agents subject
to changing conditions from a higher level agent creates a cooperative setup
that will allow for fulfilling all the individual objectives. This paper
presents a general overview of the framework, the current progress, and some
insights we obtained from the recent results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tiny Paper at ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Policy Learning for Smart Grids: FL TRPO Approach <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiang Li, Nicolas Mauricio Cuadrado, Samuel Horváth, Martin Takáč
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The smart grid domain requires bolstering the capabilities of existing energy
management systems; Federated Learning (FL) aligns with this goal as it
demonstrates a remarkable ability to train models on heterogeneous datasets
while maintaining data privacy, making it suitable for smart grid applications,
which often involve disparate data distributions and interdependencies among
features that hinder the suitability of linear models. This paper introduces a
framework that combines FL with a Trust Region Policy Optimization (FL TRPO)
aiming to reduce energy-associated emissions and costs. Our approach reveals
latent interconnections and employs personalized encoding methods to capture
unique insights, understanding the relationships between features and optimal
strategies, allowing our model to generalize to previously unseen data.
Experimental results validate the robustness of our approach, affirming its
proficiency in effectively learning policy models for smart grid challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 Workshop: Tackling Climate Change with Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Vegetation Modeling with Pre-Trained Weather Transformers <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Florian Gallusser, Simon Hentschel, Andreas Hotho, Anna Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate vegetation models can produce further insights into the complex
interaction between vegetation activity and ecosystem processes. Previous
research has established that long-term trends and short-term variability of
temperature and precipitation affect vegetation activity. Motivated by the
recent success of Transformer-based Deep Learning models for medium-range
weather forecasting, we adapt the publicly available pre-trained FourCastNet to
model vegetation activity while accounting for the short-term dynamics of
climate variability. We investigate how the learned global representation of
the atmosphere's state can be transferred to model the normalized difference
vegetation index (NDVI). Our model globally estimates vegetation activity at a
resolution of \SI{0.25}{\degree} while relying only on meteorological data. We
demonstrate that leveraging pre-trained weather models improves the NDVI
estimates compared to learning an NDVI model from scratch. Additionally, we
compare our results to other recent data-driven NDVI modeling approaches from
machine learning and ecology literature. We further provide experimental
evidence on how much data and training time is necessary to turn FourCastNet
into an effective vegetation model. Code and models will be made available upon
publication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tackling Climate Change with Machine Learning Workshop @ ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Active Learning in Conditional Trust Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zan-Kai Chong, Hiroyuki Ohsaki, Bryan Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate collaborative active learning, a paradigm in
which multiple collaborators explore a new domain by leveraging their combined
machine learning capabilities without disclosing their existing data and
models. Instead, the collaborators share prediction results from the new domain
and newly acquired labels. This collaboration offers several advantages: (a) it
addresses privacy and security concerns by eliminating the need for direct
model and data disclosure; (b) it enables the use of different data sources and
insights without direct data exchange; and (c) it promotes cost-effectiveness
and resource efficiency through shared labeling costs. To realize these
benefits, we introduce a collaborative active learning framework designed to
fulfill the aforementioned objectives. We validate the effectiveness of the
proposed framework through simulations. The results demonstrate that
collaboration leads to higher AUC scores compared to independent efforts,
highlighting the framework's ability to overcome the limitations of individual
models. These findings support the use of collaborative approaches in active
learning, emphasizing their potential to enhance outcomes through collective
expertise and shared resources. Our work provides a foundation for further
research on collaborative active learning and its practical applications in
various domains where data privacy, cost efficiency, and model performance are
critical considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 9 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SemRoDe: Macro Adversarial Training to Learn Representations That are
  Robust to Word-Level Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Formento, Wenjie Feng, Chuan Sheng Foo, Luu Anh Tuan, See-Kiong Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) are indispensable tools for natural language processing
tasks, but their vulnerability to adversarial attacks remains a concern. While
current research has explored adversarial training techniques, their
improvements to defend against word-level attacks have been limited. In this
work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a
Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing
inspiration from recent studies in the image domain, we investigate and later
confirm that in a discrete data setting such as language, adversarial samples
generated via word substitutions do indeed belong to an adversarial domain
exhibiting a high Wasserstein distance from the base domain. Our method learns
a robust representation that bridges these two domains. We hypothesize that if
samples were not projected into an adversarial domain, but instead to a domain
with minimal shift, it would improve attack robustness. We align the domains by
incorporating a new distance-based objective. With this, our model is able to
learn more generalized representations by aligning the model's high-level
output features and therefore better handling unseen adversarial samples. This
method can be generalized across word embeddings, even when they share minimal
overlap at both vocabulary and word-substitution levels. To evaluate the
effectiveness of our approach, we conduct experiments on BERT and RoBERTa
models on three datasets. The results demonstrate promising state-of-the-art
robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in NAACL 2024 (Main Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Topos of Transformer Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Jacopo Villani, Peter McBurney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformer neural network has significantly out-shined all other neural
network architectures as the engine behind large language models. We provide a
theoretical analysis of the expressivity of the transformer architecture
through the lens of topos theory. From this viewpoint, we show that many common
neural network architectures, such as the convolutional, recurrent and graph
convolutional networks, can be embedded in a pretopos of piecewise-linear
functions, but that the transformer necessarily lives in its topos completion.
In particular, this suggests that the two network families instantiate
different fragments of logic: the former are first order, whereas transformers
are higher-order reasoners. Furthermore, we draw parallels with architecture
search and gradient descent, integrating our analysis in the framework of
cybernetic agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Spectrogram Analysis in a Multiple Classifier Fusion Framework for
  Power Grid Classification Using Electric Network Frequency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Tzolopoulos, Christos Korgialas, Constantine Kotropoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Electric Network Frequency (ENF) serves as a unique signature inherent to
power distribution systems. Here, a novel approach for power grid
classification is developed, leveraging ENF. Spectrograms are generated from
audio and power recordings across different grids, revealing distinctive ENF
patterns that aid in grid classification through a fusion of classifiers. Four
traditional machine learning classifiers plus a Convolutional Neural Network
(CNN), optimized using Neural Architecture Search, are developed for One-vs-All
classification. This process generates numerous predictions per sample, which
are then compiled and used to train a shallow multi-label neural network
specifically designed to model the fusion process, ultimately leading to the
conclusive class prediction for each sample. Experimental findings reveal that
both validation and testing accuracy outperform those of current
state-of-the-art classifiers, underlining the effectiveness and robustness of
the proposed methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13th International Conference on Pattern Recognition Applications and
  Methods (ICPRAM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18393v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18393v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Shi, Lei Cao, Yunshan Ye, Yu Zhao, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph learning is widely recognized as a crucial technique in multi-view
clustering. Existing graph learning methods typically involve constructing an
adaptive neighbor graph based on probabilistic neighbors and then learning a
consensus graph to for clustering, however, they are confronted with two
limitations. Firstly, they often rely on Euclidean distance to measure
similarity when constructing the adaptive neighbor graph, which proves
inadequate in capturing the intrinsic structure among data points in many
real-world scenarios. Secondly, most of these methods focus solely on consensus
graph, ignoring view-specific graph information. In response to the
aforementioned drawbacks, we in this paper propose a novel tensor-based graph
learning framework that simultaneously considers consistency and specificity
for multi-view clustering. Specifically, we calculate the similarity distance
on the Stiefel manifold to preserve the intrinsic structure among data points.
By making an assumption that the learned neighbor graph of each view comprises
both a consistent graph and a view-specific graph, we formulate a new
tensor-based target graph learning paradigm. Owing to the benefits of tensor
singular value decomposition (t-SVD) in uncovering high-order correlations,
this model is capable of achieving a complete understanding of the target
graph. Furthermore, we develop an iterative algorithm to solve the proposed
objective optimization problem. Experiments conducted on real-world datasets
have demonstrated the superior performance of the proposed method over some
state-of-the-art multi-view clustering methods. The source code has been
released on https://github.com/lshi91/CSTGL-Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stragglers-Aware Low-Latency Synchronous Federated Learning via
  Layer-Wise Model Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natalie Lang, Alejandro Cohen, Nir Shlezinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synchronous federated learning (FL) is a popular paradigm for collaborative
edge learning. It typically involves a set of heterogeneous devices locally
training neural network (NN) models in parallel with periodic centralized
aggregations. As some of the devices may have limited computational resources
and varying availability, FL latency is highly sensitive to stragglers.
Conventional approaches discard incomplete intra-model updates done by
stragglers, alter the amount of local workload and architecture, or resort to
asynchronous settings; which all affect the trained model performance under
tight training latency constraints. In this work, we propose straggler-aware
layer-wise federated learning (SALF) that leverages the optimization procedure
of NNs via backpropagation to update the global model in a layer-wise fashion.
SALF allows stragglers to synchronously convey partial gradients, having each
layer of the global model be updated independently with a different
contributing set of users. We provide a theoretical analysis, establishing
convergence guarantees for the global model under mild assumptions on the
distribution of the participating devices, revealing that SALF converges at the
same asymptotic rate as FL with no timing limitations. This insight is matched
with empirical observations, demonstrating the performance gains of SALF
compared to alternative mechanisms mitigating the device heterogeneity gap in
FL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ship in Sight: Diffusion Models for Ship-Image Super Resolution <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luigi Sigillo, Riccardo Fosco Gramaccioni, Alessandro Nicolosi, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, remarkable advancements have been achieved in the field of
image generation, primarily driven by the escalating demand for high-quality
outcomes across various image generation subtasks, such as inpainting,
denoising, and super resolution. A major effort is devoted to exploring the
application of super-resolution techniques to enhance the quality of
low-resolution images. In this context, our method explores in depth the
problem of ship image super resolution, which is crucial for coastal and port
surveillance. We investigate the opportunity given by the growing interest in
text-to-image diffusion models, taking advantage of the prior knowledge that
such foundation models have already learned. In particular, we present a
diffusion-model-based architecture that leverages text conditioning during
training while being class-aware, to best preserve the crucial details of the
ships during the generation of the super-resoluted image. Since the specificity
of this task and the scarcity availability of off-the-shelf data, we also
introduce a large labeled ship dataset scraped from online ship images, mostly
from ShipSpotting\footnote{\url{www.shipspotting.com}} website. Our method
achieves more robust results than other deep learning models previously
employed for super resolution, as proven by the multiple experiments performed.
Moreover, we investigate how this model can benefit downstream tasks, such as
classification and object detection, thus emphasizing practical implementation
in a real-world scenario. Experimental results show flexibility, reliability,
and impressive performance of the proposed framework over state-of-the-art
methods for different tasks. The code is available at:
https://github.com/LuigiSigillo/ShipinSight .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 International Joint Conference on Neural Networks
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Multiple Kernel Learning approaches for multi-omics data
  integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitja Briscik, Gabriele Tazza, Marie-Agnes Dillies, László Vidács, Sébastien Dejean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in high-throughput technologies have originated an ever-increasing
availability of omics datasets. The integration of multiple heterogeneous data
sources is currently an issue for biology and bioinformatics. Multiple kernel
learning (MKL) has shown to be a flexible and valid approach to consider the
diverse nature of multi-omics inputs, despite being an underused tool in
genomic data mining.We provide novel MKL approaches based on different kernel
fusion strategies.To learn from the meta-kernel of input kernels, we
adaptedunsupervised integration algorithms for supervised tasks with support
vector machines.We also tested deep learning architectures for kernel fusion
and classification.The results show that MKL-based models can compete with more
complex, state-of-the-art, supervised multi-omics integrative approaches.
Multiple kernel learning offers a natural framework for predictive models in
multi-omics genomic data. Our results offer a direction for bio-data mining
research and further development of methods for heterogeneous data integration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Artificial Neural Twin -- Process Optimization and Continual
  Learning in Distributed Process Chains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Emmert, Ronald Mendez, Houman Mirzaalian Dastjerdi, Christopher Syben, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial process optimization and control is crucial to increase economic
and ecologic efficiency. However, data sovereignty, differing goals, or the
required expert knowledge for implementation impede holistic implementation.
Further, the increasing use of data-driven AI-methods in process models and
industrial sensory often requires regular fine-tuning to accommodate
distribution drifts. We propose the Artificial Neural Twin, which combines
concepts from model predictive control, deep learning, and sensor networks to
address these issues. Our approach introduces differentiable data fusion to
estimate the state of distributed process steps and their dependence on input
data. By treating the interconnected process steps as a quasi neural-network,
we can backpropagate loss gradients for process optimization or model
fine-tuning to process parameters or AI models respectively. The concept is
demonstrated on a virtual machine park simulated in Unity, consisting of bulk
material processes in plastic recycling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Macroscale fracture surface segmentation via semi-supervised learning
  considering the structural similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Rosenberger, Johannes Tlatlik, Sebastian Münstermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To this date the safety assessment of materials, used for example in the
nuclear power sector, commonly relies on a fracture mechanical analysis
utilizing macroscopic concepts, where a global load quantity K or J is compared
to the materials fracture toughness curve. Part of the experimental effort
involved in these concepts is dedicated to the quantitative analysis of
fracture surfaces. Within the scope of this study a methodology for the
semi-supervised training of deep learning models for fracture surface
segmentation on a macroscopic level was established. Therefore, three distinct
and unique datasets were created to analyze the influence of structural
similarity on the segmentation capability. The structural similarity differs
due to the assessed materials and specimen, as well as imaging-induced variance
due to fluctuations in image acquisition in different laboratories. The
datasets correspond to typical isolated laboratory conditions, complex
real-world circumstances, and a curated subset of the two. We implemented a
weak-to-strong consistency regularization for semi-supervised learning. On the
heterogeneous dataset we were able to train robust and well-generalizing models
that learned feature representations from images across different domains
without observing a significant drop in prediction quality. Furthermore, our
approach reduced the number of labeled images required for training by a factor
of 6. To demonstrate the success of our method and the benefit of our approach
for the fracture mechanics assessment, we utilized the models for initial crack
size measurements with the area average method. For the laboratory setting, the
deep learning assisted measurements proved to have the same quality as manual
measurements. For models trained on the heterogeneous dataset, very good
measurement accuracies with mean deviations smaller than 1 % could be
achieved...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>During review title changed to: Deep learning based initial crack
  size measurements utilizing macroscale fracture surface segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Dataset</span> for Pharmacovigilance in German, French, and Japanese:
  Annotating Adverse Drug Reactions across Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18336v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18336v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lisa Raithel, Hui-Syuan Yeh, Shuntaro Yada, Cyril Grouin, Thomas Lavergne, Aurélie Névéol, Patrick Paroubek, Philippe Thomas, Tomohiro Nishiyama, Sebastian Möller, Eiji Aramaki, Yuji Matsumoto, Roland Roller, Pierre Zweigenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User-generated data sources have gained significance in uncovering Adverse
Drug Reactions (ADRs), with an increasing number of discussions occurring in
the digital world. However, the existing clinical corpora predominantly revolve
around scientific articles in English. This work presents a multilingual corpus
of texts concerning ADRs gathered from diverse sources, including patient fora,
social media, and clinical reports in German, French, and Japanese. Our corpus
contains annotations covering 12 entity types, four attribute types, and 13
relation types. It contributes to the development of real-world multilingual
language models for healthcare. We provide statistics to highlight certain
challenges associated with the corpus and conduct preliminary experiments
resulting in strong baselines for extracting entities and relations between
these entities, both within and across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tracking-Assisted Object Detection with Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Kang Yen, Igor Morawski, Shusil Dangi, Kai He, Chung-Yi Lin, Jia-Fong Yeh, Hung-Ting Su, Winston Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based object detection has recently garnered attention in the computer
vision community due to the exceptional properties of event cameras, such as
high dynamic range and no motion blur. However, feature asynchronism and
sparsity cause invisible objects due to no relative motion to the camera,
posing a significant challenge in the task. Prior works have studied various
memory mechanisms to preserve as many features as possible at the current time,
guided by temporal clues. While these implicit-learned memories retain some
short-term information, they still struggle to preserve long-term features
effectively. In this paper, we consider those invisible objects as
pseudo-occluded objects and aim to reveal their features. Firstly, we introduce
visibility attribute of objects and contribute an auto-labeling algorithm to
append additional visibility labels on an existing event camera dataset.
Secondly, we exploit tracking strategies for pseudo-occluded objects to
maintain their permanence and retain their bounding boxes, even when features
have not been available for a very long time. These strategies can be treated
as an explicit-learned memory guided by the tracking objective to record the
displacements of objects across frames. Lastly, we propose a spatio-temporal
feature aggregation module to enrich the latent features and a consistency loss
to increase the robustness of the overall pipeline. We conduct comprehensive
experiments to verify our method's effectiveness where still objects are
retained but real occluded objects are discarded. The results demonstrate that
(1) the additional visibility labels can assist in supervised training, and (2)
our method outperforms state-of-the-art approaches with a significant
improvement of 7.9% absolute mAP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy-Preserving Distributed Nonnegative Matrix Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Lari, Reza Arablouei, Stefan Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonnegative matrix factorization (NMF) is an effective data representation
tool with numerous applications in signal processing and machine learning.
However, deploying NMF in a decentralized manner over ad-hoc networks
introduces privacy concerns due to the conventional approach of sharing raw
data among network agents. To address this, we propose a privacy-preserving
algorithm for fully-distributed NMF that decomposes a distributed large data
matrix into left and right matrix factors while safeguarding each agent's local
data privacy. It facilitates collaborative estimation of the left matrix factor
among agents and enables them to estimate their respective right factors
without exposing raw data. To ensure data privacy, we secure information
exchanges between neighboring agents utilizing the Paillier cryptosystem, a
probabilistic asymmetric algorithm for public-key cryptography that allows
computations on encrypted data without decryption. Simulation results conducted
on synthetic and real-world datasets demonstrate the effectiveness of the
proposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc
networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure, submitted to EUSIPCO 2024 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum Algorithms: A New Frontier in Financial Crime Prevention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abraham Itzhak Weinberg, Alessio Faccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial crimes fast proliferation and sophistication require novel
approaches that provide robust and effective solutions. This paper explores the
potential of quantum algorithms in combating financial crimes. It highlights
the advantages of quantum computing by examining traditional and Machine
Learning (ML) techniques alongside quantum approaches. The study showcases
advanced methodologies such as Quantum Machine Learning (QML) and Quantum
Artificial Intelligence (QAI) as powerful solutions for detecting and
preventing financial crimes, including money laundering, financial crime
detection, cryptocurrency attacks, and market manipulation. These quantum
approaches leverage the inherent computational capabilities of quantum
computers to overcome limitations faced by classical methods. Furthermore, the
paper illustrates how quantum computing can support enhanced financial risk
management analysis. Financial institutions can improve their ability to
identify and mitigate risks, leading to more robust risk management strategies
by exploiting the quantum advantage. This research underscores the
transformative impact of quantum algorithms on financial risk management. By
embracing quantum technologies, organisations can enhance their capabilities to
combat evolving threats and ensure the integrity and stability of financial
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implementation of the Principal Component Analysis onto High-Performance
  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and
  Comparisons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        E. Martel, R. Lazcano, J. Lopez, D. Madroñal, R. Salvador, S. Lopez, E. Juarez, R. Guerra, C. Sanz, R. Sarmiento
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dimensionality reduction represents a critical preprocessing step in order to
increase the efficiency and the performance of many hyperspectral imaging
algorithms. However, dimensionality reduction algorithms, such as the Principal
Component Analysis (PCA), suffer from their computationally demanding nature,
becoming advisable for their implementation onto high-performance computer
architectures for applications under strict latency constraints. This work
presents the implementation of the PCA algorithm onto two different
high-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and
a Kalray manycore, uncovering a highly valuable set of tips and tricks in order
to take full advantage of the inherent parallelism of these high-performance
computing platforms, and hence, reducing the time that is required to process a
given hyperspectral image. Moreover, the achieved results obtained with
different hyperspectral images have been compared with the ones that were
obtained with a field programmable gate array (FPGA)-based implementation of
the PCA algorithm that has been recently published, providing, for the first
time in the literature, a comprehensive analysis in order to highlight the pros
and cons of each option.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Modal Contrastive Learning for Online Clinical Time-Series
  Applications <span class="chip">ICLR2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Baldenweg, Manuel Burger, Gunnar Rätsch, Rita Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)
contain a diverse set of data modalities. While prior works have successfully
leveraged multiple modalities in supervised settings, we apply advanced
self-supervised multi-modal contrastive learning techniques to ICU data,
specifically focusing on clinical notes and time-series for clinically relevant
online prediction tasks. We introduce a loss function Multi-Modal Neighborhood
Contrastive Loss (MM-NCL), a soft neighborhood function, and showcase the
excellent linear probe and zero-shot performance of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a Workshop Paper at TS4H@ICLR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using
  SDO/HMI Data and an Attention-Aided Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhui Xu, Jason T. L. Wang, Haimin Wang, Haodi Jiang, Qin Li, Yasser Abduallah, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution has been an important subject in image processing and
recognition. Here, we present an attention-aided convolutional neural network
(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to
enhance the quality of line-of-sight (LOS) magnetograms of solar active regions
(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and
Heliospheric Observatory (SOHO). The ground-truth labels used for training
SolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic
Imager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist
of strong magnetic fields in which magnetic energy can suddenly be released to
produce extreme space weather events, such as solar flares, coronal mass
ejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which
is stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI
magnetograms allow for better understanding and forecasting of violent events
of space weather. Experimental results show that SolarCNN improves the quality
of SOHO/MDI magnetograms in terms of the structural similarity index measure
(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise
ratio (PSNR).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Clustering Change Sign Detection by Fusing Mixture Complexity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Urano, Ryo Yuki, Kenji Yamanishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an early detection method for cluster structural changes.
Cluster structure refers to discrete structural characteristics, such as the
number of clusters, when data are represented using finite mixture models, such
as Gaussian mixture models. We focused on scenarios in which the cluster
structure gradually changed over time. For finite mixture models, the concept
of mixture complexity (MC) measures the continuous cluster size by considering
the cluster proportion bias and overlap between clusters. In this paper, we
propose MC fusion as an extension of MC to handle situations in which multiple
mixture numbers are possible in a finite mixture model. By incorporating the
fusion of multiple models, our approach accurately captured the cluster
structure during transitional periods of gradual change. Moreover, we introduce
a method for detecting changes in the cluster structure by examining the
transition of MC fusion. We demonstrate the effectiveness of our method through
empirical analysis using both artificial and real-world datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branch-Tuning: Balancing Stability and Plasticity for Continual
  Self-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18266v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18266v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhuo Liu, Fei Zhu, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as an effective paradigm for
deriving general representations from vast amounts of unlabeled data. However,
as real-world applications continually integrate new content, the high
computational and resource demands of SSL necessitate continual learning rather
than complete retraining. This poses a challenge in striking a balance between
stability and plasticity when adapting to new information. In this paper, we
employ Centered Kernel Alignment for quantitatively analyzing model stability
and plasticity, revealing the critical roles of batch normalization layers for
stability and convolutional layers for plasticity. Motivated by this, we
propose Branch-tuning, an efficient and straightforward method that achieves a
balance between stability and plasticity in continual SSL. Branch-tuning
consists of branch expansion and compression, and can be easily applied to
various SSL methods without the need of modifying the original methods,
retaining old data or models. We validate our method through incremental
experiments on various benchmark datasets, demonstrating its effectiveness and
practical value in real-world scenarios. We hope our work offers new insights
for future continual self-supervised learning research. The code will be made
publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Image Transformers for Prostate Cancer Detection from
  Ultrasound Data <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar, Amoon Jamzad, Mahdi Gilany, Minh Nguyen Nhat To, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in
ultrasound images typically employ convolutional networks (CNNs) to detect
cancer in small regions of interest (ROI) along a needle trace region. However,
this approach suffers from weak labelling, since the ground-truth
histopathology labels do not describe the properties of individual ROIs.
Recently, multi-scale approaches have sought to mitigate this issue by
combining the context awareness of transformers with a CNN feature extractor to
detect cancer from multiple ROIs using multiple-instance learning (MIL). In
this work, we present a detailed study of several image transformer
architectures for both ROI-scale and multi-scale classification, and a
comparison of the performance of CNNs and transformers for ultrasound-based
prostate cancer classification. We also design a novel multi-objective learning
strategy that combines both ROI and core predictions to further mitigate label
noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer
classification, then use the strongest model to tune a multi-scale classifier
with MIL. We train our MIL models using our novel multi-objective learning
strategy and compare our results to existing baselines. RESULTS: We find that
for both ROI-scale and multi-scale PCa detection, image transformer backbones
lag behind their CNN counterparts. This deficit in performance is even more
noticeable for larger models. When using multi-objective learning, we can
improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a
specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for
modelling sparse datasets of prostate ultrasounds, producing more robust
features than transformers in PCa detection. Multi-scale methods remain the
best architecture for this task, with multi-objective learning presenting an
effective way to improve performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>early draft, 7 pages; Accepted to SPIE Medical Imaging 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fourier or Wavelet bases as counterpart self-attention in spikformer for
  efficient visual classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Wang, Duzhen Zhang, Tilelin Zhang, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy-efficient spikformer has been proposed by integrating the biologically
plausible spiking neural network (SNN) and artificial Transformer, whereby the
Spiking Self-Attention (SSA) is used to achieve both higher accuracy and lower
computational cost. However, it seems that self-attention is not always
necessary, especially in sparse spike-form calculation manners. In this paper,
we innovatively replace vanilla SSA (using dynamic bases calculating from Query
and Key) with spike-form Fourier Transform, Wavelet Transform, and their
combinations (using fixed triangular or wavelets bases), based on a key
hypothesis that both of them use a set of basis functions for information
transformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is
proposed and verified in visual classification tasks, including both static
image and event-based video datasets. The FWformer can achieve comparable or
even higher accuracies ($0.4\%$-$1.5\%$), higher running speed ($9\%$-$51\%$
for training and $19\%$-$70\%$ for inference), reduced theoretical energy
consumption ($20\%$-$25\%$), and reduced GPU memory usage ($4\%$-$26\%$),
compared to the standard spikformer. Our result indicates the continuous
refinement of new Transformers, that are inspired either by biological
discovery (spike-form), or information theory (Fourier or Wavelet Transform),
is promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2308.02557</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Deployment of Pre-trained Language-Conditioned
  Imitation Learning Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Wu, Bruce D. Lee, Kostas Daniilidis, Bernadette Bucher, Nikolai Matni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale robotic policies trained on data from diverse tasks and robotic
platforms hold great promise for enabling general-purpose robots; however,
reliable generalization to new environment conditions remains a major
challenge. Toward addressing this challenge, we propose a novel approach for
uncertainty-aware deployment of pre-trained language-conditioned imitation
learning agents. Specifically, we use temperature scaling to calibrate these
models and exploit the calibrated model to make uncertainty-aware decisions by
aggregating the local information of candidate actions. We implement our
approach in simulation using three such pre-trained models, and showcase its
potential to significantly enhance task completion rates. The accompanying code
is accessible at the link:
https://github.com/BobWu1998/uncertainty_quant_all.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with <span class="highlight-title">Reinforcement Learning</span> and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimax Optimal Fair Classification with Bounded Demographic Disparity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianli Zeng, Guang Cheng, Edgar Dobriban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the disparate impact of statistical machine learning methods is
crucial for ensuring fairness. While extensive research aims to reduce
disparity, the effect of using a \emph{finite dataset} -- as opposed to the
entire population -- remains unclear. This paper explores the statistical
foundations of fair binary classification with two protected groups, focusing
on controlling demographic disparity, defined as the difference in acceptance
rates between the groups. Although fairness may come at the cost of accuracy
even with infinite data, we show that using a finite sample incurs additional
costs due to the need to estimate group-specific acceptance thresholds. We
study the minimax optimal classification error while constraining demographic
disparity to a user-specified threshold. To quantify the impact of fairness
constraints, we introduce a novel measure called \emph{fairness-aware excess
risk} and derive a minimax lower bound on this measure that all classifiers
must satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding
method with an offset that we show attains the minimax lower bound. Our lower
bound proofs involve several innovations. Experiments support that
FairBayes-DDP+ controls disparity at the user-specified level, while being
faster and having a more favorable fairness-accuracy tradeoff than several
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13483v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13483v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Patel, Sangeetha Abdu Jyothi, Nina Narodytska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplified Diffusion Schrödinger Bridge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicong Tang, Tiankai Hang, Shuyang Gu, Dong Chen, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical simplification of the Diffusion
Schr\"odinger Bridge (DSB) that facilitates its unification with Score-based
Generative Models (SGMs), addressing the limitations of DSB in complex data
generation and enabling faster convergence and enhanced performance. By
employing SGMs as an initial solution for DSB, our approach capitalizes on the
strengths of both frameworks, ensuring a more efficient training process and
improving the performance of SGM. We also propose a reparameterization
technique that, despite theoretical approximations, practically improves the
network's fitting capabilities. Our extensive experimental evaluations confirm
the effectiveness of the simplified DSB, demonstrating its significant
improvements. We believe the contributions of this work pave the way for
advanced generative modeling. The code is available at
https://github.com/checkcrab/SDSB.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preventing Arbitrarily High Confidence on Far-Away Data in
  Point-Estimated Discriminative Neural Networks <span class="chip">AISTATS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Rashid, Serena Hacker, Guojun Zhang, Agustinus Kristiadi, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discriminatively trained, deterministic neural networks are the de facto
choice for classification problems. However, even though they achieve
state-of-the-art results on in-domain test sets, they tend to be overconfident
on out-of-distribution (OOD) data. For instance, ReLU networks - a popular
class of neural network architectures - have been shown to almost always yield
high confidence predictions when the test data are far away from the training
set, even when they are trained with OOD data. We overcome this problem by
adding a term to the output of the neural network that corresponds to the logit
of an extra class, that we design to dominate the logits of the original
classes as we move away from the training data.This technique provably prevents
arbitrarily high confidence on far-away test data while maintaining a simple
discriminative point-estimate training. Evaluation on various benchmarks
demonstrates strong performance against competitive baselines on both far-away
and realistic OOD data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nesting Particle Filters for Experimental Design in Dynamical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahel Iqbal, Adrien Corenflos, Simo Särkkä, Hany Abdulsamad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel approach to Bayesian experimental design
for non-exchangeable data that formulates it as risk-sensitive policy
optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential
Monte Carlo technique to infer optimal designs, and embed it into a particle
Markov chain Monte Carlo framework to perform gradient-based policy
amortization. Our approach is distinct from other amortized experimental design
techniques, as it does not rely on contrastive estimators. Numerical validation
on a set of dynamical systems showcases the efficacy of our method in
comparison to other state-of-the-art strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Review</span> of Community Detection in Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu Yang, Yutong Liu, Yonggang Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of complex networks has significantly advanced our understanding of
community structures which serves as a crucial feature of real-world graphs.
Detecting communities in graphs is a challenging problem with applications in
sociology, biology, and computer science. Despite the efforts of an
interdisciplinary community of scientists, a satisfactory solution to this
problem has not yet been achieved. This review article delves into the topic of
community detection in graphs, which serves as a thorough exposition of various
community detection methods from perspectives of modularity-based method,
spectral clustering, probabilistic modelling, and deep learning. Along with the
methods, a new community detection method designed by us is also presented.
Additionally, the performance of these methods on the datasets with and without
ground truth is compared. In conclusion, this comprehensive review provides a
deep understanding of community detection in graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering Data Mesh with Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyuan Li, Salman Toor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of data architecture has seen the rise of data lakes, aiming to
solve the bottlenecks of data management and promote intelligent
decision-making. However, this centralized architecture is limited by the
proliferation of data sources and the growing demand for timely analysis and
processing. A new data paradigm, Data Mesh, is proposed to overcome these
challenges. Data Mesh treats domains as a first-class concern by distributing
the data ownership from the central team to each data domain, while keeping the
federated governance to monitor domains and their data products. Many
multi-million dollar organizations like Paypal, Netflix, and Zalando have
already transformed their data analysis pipelines based on this new
architecture. In this decentralized architecture where data is locally
preserved by each domain team, traditional centralized machine learning is
incapable of conducting effective analysis across multiple domains, especially
for security-sensitive organizations. To this end, we introduce a pioneering
approach that incorporates Federated Learning into Data Mesh. To the best of
our knowledge, this is the first open-source applied work that represents a
critical advancement toward the integration of federated learning methods into
the Data Mesh paradigm, underscoring the promising prospects for
privacy-preserving and decentralized data analysis strategies within Data Mesh
architecture.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Adaptive</span> Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guided Distant Supervision for Multilingual Relation Extraction Data:
  Adapting to a New Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alistair Plum, Tharindu Ranasinghe, Christoph Purschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction is essential for extracting and understanding
biographical information in the context of digital humanities and related
subjects. There is a growing interest in the community to build datasets
capable of training machine learning models to extract relationships. However,
annotating such datasets can be expensive and time-consuming, in addition to
being limited to English. This paper applies guided distant supervision to
create a large biographical relationship extraction dataset for German. Our
dataset, composed of more than 80,000 instances for nine relationship types, is
the largest biographical German relationship extraction dataset. We also create
a manually annotated dataset with 2000 instances to evaluate the models and
release it together with the dataset compiled using guided distant supervision.
We train several state-of-the-art machine learning models on the automatically
created dataset and release them as well. Furthermore, we experiment with
multilingual and cross-lingual experiments that could benefit many low-resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to LREC-COLING 2024 (The 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LCANets++: Robust Audio Classification using Multi-layer Neural Networks
  with Lateral Competition <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayanton V. Dibbo, Juston S. Moore, Garrett T. Kenyon, Michael A. Teti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio classification aims at recognizing audio signals, including speech
commands or sound events. However, current audio classifiers are susceptible to
perturbations and adversarial attacks. In addition, real-world audio
classification tasks often suffer from limited labeled data. To help bridge
these gaps, previous work developed neuro-inspired convolutional neural
networks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)
in the first layer (i.e., LCANets) for computer vision. LCANets learn in a
combination of supervised and unsupervised learning, reducing dependency on
labeled samples. Motivated by the fact that auditory cortex is also sparse, we
extend LCANets to audio recognition tasks and introduce LCANets++, which are
CNNs that perform sparse coding in multiple layers via LCA. We demonstrate that
LCANets++ are more robust than standard CNNs and LCANets against perturbations,
e.g., background noise, as well as black-box and white-box attacks, e.g.,
evasion and fast gradient sign (FGSM) attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing Workshops (ICASSPW)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online <span class="highlight-title">Prediction</span> of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Learning in Bilateral Trade via Global Budget Balance <span class="chip">STOC 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martino Bernasconi, Matteo Castiglioni, Andrea Celli, Federico Fusco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral trade models the problem of intermediating between two rational
agents -- a seller and a buyer -- both characterized by a private valuation for
an item they want to trade. We study the online learning version of the
problem, in which at each time step a new seller and buyer arrive and the
learner has to set prices for them without any knowledge about their
(adversarially generated) valuations.
  In this setting, known impossibility results rule out the existence of
no-regret algorithms when budget balanced has to be enforced at each time step.
In this paper, we introduce the notion of \emph{global budget balance}, which
only requires the learner to fulfill budget balance over the entire time
horizon. Under this natural relaxation, we provide the first no-regret
algorithms for adversarial bilateral trade under various feedback models.
First, we show that in the full-feedback model, the learner can guarantee
$\tilde O(\sqrt{T})$ regret against the best fixed prices in hindsight, and
that this bound is optimal up to poly-logarithmic terms. Second, we provide a
learning algorithm guaranteeing a $\tilde O(T^{3/4})$ regret upper bound with
one-bit feedback, which we complement with a $\Omega(T^{5/7})$ lower bound that
holds even in the two-bit feedback model. Finally, we introduce and analyze an
alternative benchmark that is provably stronger than the best fixed prices in
hindsight and is inspired by the literature on bandits with knapsacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at STOC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Shot Detection of Machine-Generated Text using Style Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Kailin Koch, Aleem Khan, Barry Chen, Marcus Bishop, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of instruction-tuned language models that convincingly mimic human
writing poses a significant risk of abuse. However, such abuse may be
counteracted with the ability to detect whether a piece of text was composed by
a language model rather than a human author. Some previous approaches to this
problem have relied on supervised methods by training on corpora of confirmed
human- and machine- written documents. Unfortunately, model under-specification
poses an unavoidable challenge for neural network-based detectors, making them
brittle in the face of data shifts, such as the release of newer language
models producing still more fluent text than the models used to train the
detectors. Other approaches require access to the models that may have
generated a document in question, which is often impractical. In light of these
challenges, we pursue a fundamentally different approach not relying on samples
from language models of concern at training time. Instead, we propose to
leverage representations of writing style estimated from human-authored text.
Indeed, we find that features effective at distinguishing among human authors
are also effective at distinguishing human from machine authors, including
state-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.
Furthermore, given a handful of examples composed by each of several specific
language models of interest, our approach affords the ability to predict which
model generated a given document. The code and data to reproduce our
experiments are available at
https://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Optimized Orthogonal Basis Piecewise Polynomial
  Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannes Waclawek, Stefan Huber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,
like trajectory planning, to approximate position profiles given in the form of
a set of points. While the approximation target along with domain-specific
requirements, like Ck -continuity, can be formulated as a system of equations
and a result can be computed directly, such closed-form solutions posses
limited flexibility with respect to polynomial degrees, polynomial bases or
adding further domain-specific requirements. Sufficiently complex optimization
goals soon call for the use of numerical methods, like gradient descent. Since
gradient descent lies at the heart of training Artificial Neural Networks
(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set
of gradient-based optimizers potentially suitable for a wide range of
optimization problems beyond the training task for ANNs. Our approach is to
utilize the versatility of PP models and combine it with the potential of
modern ML optimizers for the use in function approximation in 1D trajectory
planning in the context of electronic cam design. We utilize available
optimizers of the ML framework TensorFlow directly, outside of the scope of
ANNs, to optimize model parameters of our PP model. In this paper, we show how
an orthogonal polynomial basis contributes to improving approximation and
continuity optimization performance. Utilizing Chebyshev polynomials of the
first kind, we develop a novel regularization approach enabling clearly
improved convergence behavior. We show that, using this regularization
approach, Chebyshev basis performs better than power basis for all relevant
optimizers in the combined approximation and continuity optimization setting
and demonstrate usability of the presented approach within the electronic cam
domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to LION18</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Limit Order Book Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09267v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09267v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Briola, Silvia Bartolucci, Tomaso Aste
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We exploit cutting-edge deep learning methodologies to explore the
predictability of high-frequency Limit Order Book mid-price changes for a
heterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we
release `LOBFrame', an open-source code base to efficiently process large-scale
Limit Order Book data and quantitatively assess state-of-the-art deep learning
models' forecasting capabilities. Our results are twofold. We demonstrate that
the stocks' microstructural characteristics influence the efficacy of deep
learning methods and that their high forecasting power does not necessarily
correspond to actionable trading signals. We argue that traditional machine
learning metrics fail to adequately assess the quality of forecasts in the
Limit Order Book context. As an alternative, we propose an innovative
operational framework that evaluates predictions' practicality by focusing on
the probability of accurately forecasting complete transactions. This work
offers academics and practitioners an avenue to make informed and robust
decisions on the application of deep learning techniques, their scope and
limitations, effectively exploiting emergent statistical properties of the
Limit Order Book.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 14 figures, 12 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel
  Segmentation via Two-Phase Training Angiography-to-Venography Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Galati, Daniele Falcetta, Rosa Cortese, Barbara Casolla, Ferran Prados, Ninon Burgos, Maria A. Zuluaga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a semi-supervised domain adaptation framework for brain vessel
segmentation from different image modalities. Existing state-of-the-art methods
focus on a single modality, despite the wide range of available cerebrovascular
imaging techniques. This can lead to significant distribution shifts that
negatively impact the generalization across modalities. By relying on annotated
angiographies and a limited number of annotated venographies, our framework
accomplishes image-to-image translation and semantic segmentation, leveraging a
disentangled and semantically rich latent space to represent heterogeneous data
and perform image-level adaptation from source to target domains. Moreover, we
reduce the typical complexity of cycle-based architectures and minimize the use
of adversarial training, which allows us to build an efficient and intuitive
model with stable training. We evaluate our method on magnetic resonance
angiographies and venographies. While achieving state-of-the-art performance in
the source domain, our method attains a Dice score coefficient in the target
domain that is only 8.9% lower, highlighting its promising potential for robust
cerebrovascular image segmentation across different modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 34th British Machine Vision Conference (BMVC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Amos Storkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline pretraining with a static dataset followed by online fine-tuning
(offline-to-online, or OtO) is a paradigm well matched to a real-world RL
deployment process. In this scenario, we aim to find the best-performing policy
within a limited budget of online interactions. Previous work in the OtO
setting has focused on correcting for bias introduced by the policy-constraint
mechanisms of offline RL algorithms. Such constraints keep the learned policy
close to the behavior policy that collected the dataset, but we show this can
unnecessarily limit policy performance if the behavior policy is far from
optimal. Instead, we forgo constraints and frame OtO RL as an exploration
problem that aims to maximize the benefit of online data-collection. We first
study the major online RL exploration methods based on intrinsic rewards and
UCB in the OtO setting, showing that intrinsic rewards add training instability
through reward-function modification, and UCB methods are myopic and it is
unclear which learned-component's ensemble to use for action selection. We then
introduce an algorithm for planning to go out-of-distribution (PTGOOD) that
avoids these issues. PTGOOD uses a non-myopic planning procedure that targets
exploration in relatively high-reward regions of the state-action space
unlikely to be visited by the behavior policy. By leveraging concepts from the
Conditional Entropy Bottleneck, PTGOOD encourages data collected online to
provide new information relevant to improving the final deployment policy
without altering rewards. We show empirically in several continuous control
tasks that PTGOOD significantly improves agent returns during online
fine-tuning and avoids the suboptimal policy convergence that many of our
baselines exhibit in several environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 17 figures, preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emerging Trends in Federated Learning: From Model Fusion to Federated X
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.12920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.12920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new learning paradigm that decouples data collection
and model training via multi-party computation and model aggregation. As a
flexible learning setting, federated learning has the potential to integrate
with other learning frameworks. We conduct a focused survey of federated
learning in conjunction with other learning algorithms. Specifically, we
explore various learning algorithms to improve the vanilla federated averaging
algorithm and review model fusion methods such as adaptive aggregation,
regularization, clustered methods, and Bayesian methods. Following the emerging
trends, we also discuss federated learning in the intersection with other
learning paradigms, termed federated X learning, where X includes multitask
learning, meta-learning, transfer learning, unsupervised learning, and
reinforcement learning. In addition to reviewing state-of-the-art studies, this
paper also identifies key challenges and applications in this field, while also
highlighting promising future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the International Journal of Machine Learning and
  Cybernetics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Chen, Chao Tang, Amir Aghabiglou, Chung San Chu, Yves Wiaux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new approach for non-Cartesian magnetic resonance image
reconstruction. While unrolled architectures provide robustness via
data-consistency layers, embedding measurement operators in Deep Neural Network
(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)
approaches, where the denoising DNNs are blind to the measurement setting, are
not affected by this limitation and have also proven effective, but their
highly iterative nature also affects scalability. To address this scalability
challenge, we leverage the "Residual-to-Residual DNN series for high-Dynamic
range imaging (R2D2)" approach recently introduced in astronomical imaging.
R2D2's reconstruction is formed as a series of residual images, iteratively
estimated as outputs of DNNs taking the previous iteration's image estimate and
associated data residual as inputs. The method can be interpreted as a learned
version of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,
considering radial k-space sampling acquisition sequences. Our preliminary
results suggest that R2D2 achieves: (i) suboptimal performance compared to its
unrolled incarnation R2D2-Net, which is however non-scalable due to the
necessary embedding of NUFFT-based data-consistency layers; (ii) superior
reconstruction quality to a scalable version of R2D2-Net embedding an FFT-based
approximation for data consistency; (iii) superior reconstruction quality to
PnP, while only requiring few iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE EUSIPCO 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Distributed Gradient Descent with Arbitrary Number of
  Byzantine Attackers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.13352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.13352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puning Zhao, Zhiguo Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust distributed learning with Byzantine failures has attracted extensive
research interests in recent years. However, most of existing methods suffer
from curse of dimensionality, which is increasingly serious with the growing
complexity of modern machine learning models. In this paper, we design a new
method that is suitable for high dimensional problems, under arbitrary number
of Byzantine attackers. The core of our design is a direct high dimensional
semi-verified mean estimation method. Our idea is to identify a subspace first.
The components of mean value perpendicular to this subspace can be estimated
via gradient vectors uploaded from worker machines, while the components within
this subspace are estimated using auxiliary dataset. We then use our new method
as the aggregator of distributed learning problems. Our theoretical analysis
shows that the new method has minimax optimal statistical rates. In particular,
the dependence on dimensionality is significantly improved compared with
previous works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centered Masking for Language-Image Pre-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingliang Liang, Martha Larson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,
straightforward, and effective technique for masking image patches during
pre-training of a vision-language model. GLIP builds on Fast Language-Image
Pre-Training (FLIP), which randomly masks image patches while training a CLIP
model. GLIP replaces random masking with centered masking, that uses a Gaussian
distribution and is inspired by the importance of image patches at the center
of the image. GLIP retains the same computational savings as FLIP, while
improving performance across a range of downstream datasets and tasks, as
demonstrated by our experimental results. We show the benefits of GLIP to be
easy to obtain, requiring no delicate tuning of the Gaussian, and also
applicable to data sets containing images without an obvious center focus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Bayes risk of semi-supervised learning with uncertain
  labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Leger, Romain Couillet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article considers a semi-supervised classification setting on a Gaussian
mixture model, where the data is not labeled strictly as usual, but instead
with uncertain labels. Our main aim is to compute the Bayes risk for this
model. We compare the behavior of the Bayes risk and the best known algorithm
for this model. This comparison eventually gives new insights over the
algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physics-embedded Deep Learning Framework for Cloth <span class="highlight-title">Simulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A derivation is incomplete, and updations are being processed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial <span class="highlight-title">Reinforcement Learning</span> <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample Efficient <span class="highlight-title">Reinforcement Learning</span> with Partial Dynamics Knowledge <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12558v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12558v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meshal Alharbi, Mardavij Roozbehani, Munther Dahleh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of sample complexity of online reinforcement learning is often
studied in the literature without taking into account any partial knowledge
about the system dynamics that could potentially accelerate the learning
process. In this paper, we study the sample complexity of online Q-learning
methods when some prior knowledge about the dynamics is available or can be
learned efficiently. We focus on systems that evolve according to an additive
disturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$
represents the underlying system dynamics, and $W_h$ are unknown disturbances
independent of states and actions. In the setting of finite episodic Markov
decision processes with $S$ states, $A$ actions, and episode length $H$, we
present an optimistic Q-learning algorithm that achieves
$\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{T})$ regret under perfect knowledge of
$f$, where $T$ is the total number of interactions with the system. This is in
contrast to the typical $\tilde{\mathcal{O}}(\text{Poly}(H)\sqrt{SAT})$ regret
for existing Q-learning methods. Further, if only a noisy estimate $\hat{f}$ of
$f$ is available, our method can learn an approximately optimal policy in a
number of samples that is independent of the cardinalities of state and action
spaces. The sub-optimality gap depends on the approximation error $\hat{f}-f$,
as well as the Lipschitz constant of the corresponding optimal value function.
Our approach does not require modeling of the transition probabilities and
enjoys the same memory complexity as model-free methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the 38th Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Model Makes Clustering A Better Initialization For Cold-Start
  Active Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Yuan, Chuan Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning selects the most informative samples from the unlabelled
dataset to annotate in the context of a limited annotation budget. While
numerous methods have been proposed for subsequent sample selection based on an
initialized model, scant attention has been paid to the indispensable phase of
active learning: selecting samples for model cold-start initialization. Most of
the previous studies resort to random sampling or naive clustering. However,
random sampling is prone to fluctuation, and naive clustering suffers from
convergence speed, particularly when dealing with high-dimensional data such as
imaging data. In this work, we propose to integrate foundation models with
clustering methods to select samples for cold-start active learning
initialization. Foundation models refer to those trained on massive datasets by
the self-supervised paradigm and capable of generating informative and
compacted embeddings for various downstream tasks. Leveraging these embeddings
to replace raw features such as pixel values, clustering quickly converges and
identifies better initial samples. For a comprehensive comparison, we included
a classic ImageNet-supervised model to acquire embeddings. Experiments on two
clinical tasks of image classification and segmentation demonstrated that
foundation model-based clustering efficiently pinpointed informative initial
samples, leading to models showcasing enhanced performance than the baseline
methods. We envisage that this study provides an effective paradigm for future
cold-start active learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expectations Versus Reality: Evaluating Intrusion Detection Systems in
  Practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Hesford, Daniel Cheng, Alan Wan, Larry Huynh, Seungho Kim, Hyoungshick Kim, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper provides empirical comparisons between recent IDSs to provide an
objective comparison between them to help users choose the most appropriate
solution based on their requirements. Our results show that no one solution is
the best, but is dependent on external variables such as the types of attacks,
complexity, and network environment in the dataset. For example, BoT_IoT and
Stratosphere IoT datasets both capture IoT-related attacks, but the deep neural
network performed the best when tested using the BoT_IoT dataset while HELAD
performed the best when tested using the Stratosphere IoT dataset. So although
we found that a deep neural network solution had the highest average F1 scores
on tested datasets, it is not always the best-performing one. We further
discuss difficulties in using IDS from literature and project repositories,
which complicated drawing definitive conclusions regarding IDS selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CBQ: Cross-Block Quantization for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Ding, Xiaoyu Liu, Zhijun Tu, Yun Zhang, Wei Li, Jie Hu, Hanting Chen, Yehui Tang, Zhiwei Xiong, Baoqun Yin, Yunhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) has played a key role in compressing large
language models (LLMs) with ultra-low costs. However, existing PTQ methods only
focus on handling the outliers within one layer or one block, which ignores the
dependency of blocks and leads to severe performance degradation in low-bit
settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ
method for LLMs. CBQ employs a cross-block dependency using a homologous
reconstruction scheme, establishing long-range dependencies across multiple
blocks to minimize error accumulation. Furthermore, CBQ incorporates a
coarse-to-fine preprocessing (CFP) strategy for suppressing weight and
activation outliers, coupled with an adaptive LoRA-Rounding technique for
precise weight quantization. These innovations enable CBQ to not only handle
extreme outliers effectively but also improve overall quantization accuracy.
Extensive experiments show that CBQ achieves superior low-bit quantization
(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across
various LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model
within only 4.3 hours on a single GPU, achieving a commendable tradeoff between
performance and quantization efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simple Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.16025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.16025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengpeng Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose Simple Policy Optimization (SPO)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. Extensive experimental results in Atari 2600
environments indicate that, compared to the mainstream variants of PPO, SPO
achieves better sample efficiency, extremely low KL divergence, and higher
policy entropy, and is robust to the increase in network depth or complexity.
More importantly, SPO maintains the simplicity of an unconstrained first-order
algorithm. Code is available at
https://github.com/MyRepositories-hub/Simple-Policy-Optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering and Mitigating Visual Biases through Keyword Explanation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11104v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11104v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Younghyun Kim, Sangwoo Mo, Minkyu Kim, Kyungmin Lee, Jaeho Lee, Jinwoo Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing biases in computer vision models is crucial for real-world AI
deployments. However, mitigating visual biases is challenging due to their
unexplainable nature, often identified indirectly through visualization or
sample statistics, which necessitates additional human supervision for
interpretation. To tackle this issue, we propose the Bias-to-Text (B2T)
framework, which interprets visual biases as keywords. Specifically, we extract
common keywords from the captions of mispredicted images to identify potential
biases in the model. We then validate these keywords by measuring their
similarity to the mispredicted images using a vision-language scoring model.
The keyword explanation form of visual bias offers several advantages, such as
a clear group naming for bias discovery and a natural extension for debiasing
using these group names. Our experiments demonstrate that B2T can identify
known biases, such as gender bias in CelebA, background bias in Waterbirds, and
distribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in
larger datasets, such as Dollar Street and ImageNet. For example, we discovered
a contextual bias between "bee" and "flower" in ImageNet. We also highlight
various applications of B2T keywords, including debiased training, CLIP
prompting, and model comparison.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAFE: Towards Compact, <span class="highlight-title">Adaptive</span>, and Fast Embedding for Large-scale
  Recommendation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailin Zhang, Zirui Liu, Boxuan Chen, Yikai Zhao, Tong Zhao, Tong Yang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the growing memory demands of embedding tables in Deep Learning
Recommendation Models (DLRMs) pose great challenges for model training and
deployment. Existing embedding compression solutions cannot simultaneously meet
three key design requirements: memory efficiency, low latency, and adaptability
to dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,
and Fast Embedding compression framework that addresses the above requirements.
The design philosophy of CAFE is to dynamically allocate more memory resources
to important features (called hot features), and allocate less memory to
unimportant ones. In CAFE, we propose a fast and lightweight sketch data
structure, named HotSketch, to capture feature importance and report hot
features in real time. For each reported hot feature, we assign it a unique
embedding. For the non-hot features, we allow multiple features to share one
embedding by using hash embedding technique. Guided by our design philosophy,
we further propose a multi-level hash embedding framework to optimize the
embedding tables of non-hot features. We theoretically analyze the accuracy of
HotSketch, and analyze the model convergence against deviation. Extensive
experiments show that CAFE significantly outperforms existing embedding
compression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo
Kaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The
source codes of CAFE are available at GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Gemini: Mining the Potential of Multi-modality Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce Mini-Gemini, a simple and effective framework
enhancing multi-modality Vision Language Models (VLMs). Despite the
advancements in VLMs facilitating basic visual dialog and reasoning, a
performance gap persists compared to advanced models like GPT-4 and Gemini. We
try to narrow the gap by mining the potential of VLMs for better performance
and any-to-any workflow from three aspects, i.e., high-resolution visual
tokens, high-quality data, and VLM-guided generation. To enhance visual tokens,
we propose to utilize an additional visual encoder for high-resolution
refinement without increasing the visual token count. We further construct a
high-quality dataset that promotes precise image comprehension and
reasoning-based generation, expanding the operational scope of current VLMs. In
general, Mini-Gemini further mines the potential of VLMs and empowers current
frameworks with image understanding, reasoning, and generation simultaneously.
Mini-Gemini supports a series of dense and MoE Large Language Models (LLMs)
from 2B to 34B. It is demonstrated to achieve leading performance in several
zero-shot benchmarks and even surpasses the developed private models. Code and
models are available at https://github.com/dvlab-research/MiniGemini.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and models are available at
  https://github.com/dvlab-research/MiniGemini</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patni, Aradhye Agarwal, Chetan Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of parallax cues, a learning-based single image depth
estimation (SIDE) model relies heavily on shading and contextual cues in the
image. While this simplicity is attractive, it is necessary to train such
models on large and varied datasets, which are difficult to capture. It has
been shown that using embeddings from pre-trained foundational models, such as
CLIP, improves zero shot transfer in several applications. Taking inspiration
from this, in our paper we explore the use of global image priors generated
from a pre-trained ViT model to provide more detailed contextual information.
We argue that the embedding vector from a ViT model, pre-trained on a large
dataset, captures greater relevant information for SIDE than the usual route of
generating pseudo image captions, followed by CLIP based text embeddings. Based
on this idea, we propose a new SIDE model using a diffusion backbone which is
conditioned on ViT embeddings. Our proposed design establishes a new
state-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of
0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on
KITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to
0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model
trained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)
over NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,
18%, 45%, 9%) by ZoeDepth. The code is available at
https://github.com/Aradhye2002/EcoDepth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form factuality in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often generate content that contains factual
errors when responding to fact-seeking prompts on open-ended topics. To
benchmark a model's long-form factuality in open domains, we first use GPT-4 to
generate LongFact, a prompt set comprising thousands of questions spanning 38
topics. We then propose that LLM agents can be used as automated evaluators for
long-form factuality through a method which we call Search-Augmented Factuality
Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into
a set of individual facts and to evaluate the accuracy of each fact using a
multi-step reasoning process comprising sending search queries to Google Search
and determining whether a fact is supported by the search results. Furthermore,
we propose extending F1 score as an aggregated metric for long-form factuality.
To do so, we balance the percentage of supported facts in a response
(precision) with the percentage of provided facts relative to a hyperparameter
representing a user's preferred response length (recall).
  Empirically, we demonstrate that LLM agents can achieve superhuman rating
performance - on a set of ~16k individual facts, SAFE agrees with crowdsourced
human annotators 72% of the time, and on a random subset of 100 disagreement
cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times
cheaper than human annotators. We also benchmark thirteen language models on
LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding
that larger language models generally achieve better long-form factuality.
LongFact, SAFE, and all experimental code are available at
https://github.com/google-deepmind/long-form-factuality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng Yan, Xinchao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the challenge of efficiently reconstructing a 3D asset from a
single image with growing demands for automated 3D content creation pipelines.
Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural
Radiance Fields (NeRF). Despite their significant success, these approaches
encounter practical limitations due to lengthy optimization and considerable
memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D
reconstruction model from single-view images, emphasizing two main insights:
(1) 3D representation: leveraging a large number of 3D Gaussians for an
efficient 3D Gaussian splatting process; (2) Backbone design: introducing a
Mamba-based sequential network that facilitates context-dependent reasoning and
linear scalability with the sequence (token) length, accommodating a
substantial number of Gaussians. Gamba incorporates significant advancements in
data preprocessing, regularization design, and training methodologies. We
assessed Gamba against existing optimization-based and feed-forward 3D
generation approaches using the real-world scanned OmniObject3D dataset. Here,
Gamba demonstrates competitive generation capabilities, both qualitatively and
quantitatively, while achieving remarkable speed, approximately 0.6 second on a
single NVIDIA A100 GPU.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImageNet-D: Benchmarking Neural Network Robustness on Diffusion
  Synthetic Object 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenshuang Zhang, Fei Pan, Junmo Kim, In So Kweon, Chengzhi Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish rigorous benchmarks for visual perception robustness. Synthetic
images such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific
type of evaluation over synthetic corruptions, backgrounds, and textures, yet
those robustness benchmarks are restricted in specified variations and have low
synthetic quality. In this work, we introduce generative model as a data source
for synthesizing hard images that benchmark deep models' robustness. Leveraging
diffusion models, we are able to generate images with more diversified
backgrounds, textures, and materials than any prior work, where we term this
benchmark as ImageNet-D. Experimental results show that ImageNet-D results in a
significant accuracy drop to a range of vision models, from the standard ResNet
visual classifier to the latest foundation models like CLIP and MiniGPT-4,
significantly reducing their accuracy by up to 60\%. Our work suggests that
diffusion models can be an effective source to test vision models. The code and
dataset are available at https://github.com/chenshuang-zhang/imagenet_d.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Superior Parallel Big Data Clustering through Competitive Stochastic
  Sample Size Optimization in Big-means 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rustam Mussabayev, Ravil Mussabayev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel K-means clustering algorithm, an advancement on
the conventional Big-means methodology. The proposed method efficiently
integrates parallel processing, stochastic sampling, and competitive
optimization to create a scalable variant designed for big data applications.
It addresses scalability and computation time challenges typically faced with
traditional techniques. The algorithm adjusts sample sizes dynamically for each
worker during execution, optimizing performance. Data from these sample sizes
are continually analyzed, facilitating the identification of the most efficient
configuration. By incorporating a competitive element among workers using
different sample sizes, efficiency within the Big-means algorithm is further
stimulated. In essence, the algorithm balances computational time and
clustering quality by employing a stochastic, competitive sampling strategy in
a parallel computing setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18762v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18762v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weidong Xie, Lun Luo, Nanfei Ye, Yi Ren, Shaoyi Du, Minhang Wang, Jintao Xu, Rui Ai, Weihao Gu, Xieyuanli Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is an important task for robots and autonomous cars to
localize themselves and close loops in pre-built maps. While single-modal
sensor-based methods have shown satisfactory performance, cross-modal place
recognition that retrieving images from a point-cloud database remains a
challenging problem. Current cross-modal methods transform images into 3D
points using depth estimation for modality conversion, which are usually
computationally intensive and need expensive labeled data for depth
supervision. In this work, we introduce a fast and lightweight framework to
encode images and point clouds into place-distinctive descriptors. We propose
an effective Field of View (FoV) transformation module to convert point clouds
into an analogous modality as images. This module eliminates the necessity for
depth estimation and helps subsequent modules achieve real-time performance. We
further design a non-negative factorization-based encoder to extract mutually
consistent semantic features between point clouds and images. This encoder
yields more distinctive global descriptors for retrieval. Experimental results
on the KITTI dataset show that our proposed methods achieve state-of-the-art
performance while running in real time. Additional evaluation on the HAOMO
dataset covering a 17 km trajectory further shows the practical generalization
capabilities. We have released the implementation of our methods as open source
at: https://github.com/haomo-ai/ModaLink.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection of subclinical atherosclerosis by image-based deep learning on
  chest x-ray 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guglielmo Gallone, Francesco Iodice, Alberto Presta, Davide Tore, Ovidio de Filippo, Michele Visciano, Carlo Alberto Barbano, Alessandro Serafini, Paola Gorrini, Alessandro Bruno, Walter Grosso Marra, James Hughes, Mario Iannaccone, Paolo Fonio, Attilio Fiandrotti, Alessandro Depaoli, Marco Grangetto, Gaetano Maria de Ferrari, Fabrizio D'Ascenzo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Heart Journal - Cardiovascular Imaging Added
  also the additional material 44 pages (30 main paper, 14 additional
  material), 14 figures (5 main manuscript, 9 additional material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective Evolutionary Influence Maximization: Balancing Spread,
  Budget, Fairness, and Time <span class="chip">GECCO
  24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Influence Maximization (IM) problem seeks to discover the set of nodes in
a graph that can spread the information propagation at most. This problem is
known to be NP-hard, and it is usually studied by maximizing the influence
(spread) and, optionally, optimizing a second objective, such as minimizing the
seed set size or maximizing the influence fairness. However, in many practical
scenarios multiple aspects of the IM problem must be optimized at the same
time. In this work, we propose a first case study where several IM-specific
objective functions, namely budget, fairness, communities, and time, are
optimized on top of the maximization of influence and minimization of the seed
set size. To this aim, we introduce MOEIM (Many-Objective Evolutionary
Algorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm
(MOEA) based on NSGA-II incorporating graph-aware operators and a smart
initialization. We compare MOEIM in two experimental settings, including a
total of nine graph datasets, two heuristic methods, a related MOEA, and a
state-of-the-art Deep Learning approach. The experiments show that MOEIM
overall outperforms the competitors in most of the tested many-objective
settings. To conclude, we also investigate the correlation between the
objectives, leading to novel insights into the topic. The codebase is available
at https://github.com/eliacunegatti/MOEIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Genetic and Evolutionary Computation Conference (GECCO
  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,
  NY, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Learning Dynamics of Alignment with Human Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shawn Im, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models (LLMs) with human intentions has become a
critical task for safely deploying models in real-world systems. While existing
alignment approaches have seen empirical success, theoretically understanding
how these methods affect model behavior remains an open question. Our work
provides an initial attempt to theoretically analyze the learning dynamics of
human preference alignment. We formally show how the distribution of preference
datasets influences the rate of model updates and provide rigorous guarantees
on the training accuracy. Our theory also reveals an intricate phenomenon where
the optimization is prone to prioritizing certain behaviors with higher
preference distinguishability. We empirically validate our findings on
contemporary LLMs and alignment tasks, reinforcing our theoretical insights and
shedding light on considerations for future alignment approaches. Disclaimer:
This paper contains potentially offensive text; reader discretion is advised.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Manufacturing Quality <span class="highlight-title">Prediction</span> Models through the
  Integration of Explainability Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker, Arnaud Gotlieb, Ricardo Knoblauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a method that utilizes explainability techniques to
amplify the performance of machine learning (ML) models in forecasting the
quality of milling processes, as demonstrated in this paper through a
manufacturing use case. The methodology entails the initial training of ML
models, followed by a fine-tuning phase where irrelevant features identified
through explainability methods are eliminated. This procedural refinement
results in performance enhancements, paving the way for potential reductions in
manufacturing costs and a better understanding of the trained ML models. This
study highlights the usefulness of explainability techniques in both explaining
and optimizing predictive models in the manufacturing realm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Model Checking of Stochastic <span class="highlight-title">Reinforcement Learning</span>
  Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dennis Gross, Helge Spieker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method to verify stochastic reinforcement learning (RL)
policies. This approach is compatible with any RL algorithm as long as the
algorithm and its corresponding environment collectively adhere to the Markov
property. In this setting, the future state of the environment should depend
solely on its current state and the action executed, independent of any
previous states or actions. Our method integrates a verification technique,
referred to as model checking, with RL, leveraging a Markov decision process, a
trained RL policy, and a probabilistic computation tree logic (PCTL) formula to
build a formal model that can be subsequently verified via the model checker
Storm. We demonstrate our method's applicability across multiple benchmarks,
comparing it to baseline methods called deterministic safety estimates and
naive monolithic model checking. Our results show that our method is suited to
verify stochastic RL policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semi-Supervised Learning for Deep Causal Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasin Ibrahim, Hermione Warr, Konstantinos Kamnitsas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing models that can answer questions of the form "How would $x$ change
if $y$ had been $z$?" is fundamental for advancing medical image analysis.
Training causal generative models that address such counterfactual questions,
though, currently requires that all relevant variables have been observed and
that corresponding labels are available in training data. However, clinical
data may not have complete records for all patients and state of the art causal
generative models are unable to take full advantage of this. We thus develop,
for the first time, a semi-supervised deep causal generative model that
exploits the causal relationships between variables to maximise the use of all
available data. We explore this in the setting where each sample is either
fully labelled or fully unlabelled, as well as the more clinically realistic
case of having different labels missing for each sample. We leverage techniques
from causal inference to infer missing values and subsequently generate
realistic counterfactuals, even for samples with incomplete labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mitigating Hallucinations in Large Vision-Language Models with
  Instruction Contrastive Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintong Wang, Jingheng Pan, Liang Ding, Chris Biemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) are increasingly adept at generating
contextually detailed and coherent responses from visual inputs. However, their
application in multimodal decision-making and open-ended generation is hindered
by a notable rate of hallucinations, where generated text inaccurately
represents the visual contents. To address this issue, this paper introduces
the Instruction Contrastive Decoding (ICD) method, a novel approach designed to
reduce hallucinations during LVLM inference. Our method is inspired by our
observation that what we call disturbance instructions significantly exacerbate
hallucinations in multimodal fusion modules. ICD contrasts distributions from
standard and instruction disturbance, thereby increasing alignment uncertainty
and effectively subtracting hallucinated concepts from the original
distribution. Through comprehensive experiments on discriminative benchmarks
(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that
ICD significantly mitigates both object-level and attribute-level
hallucinations. Moreover, our method not only addresses hallucinations but also
significantly enhances the general perception and recognition capabilities of
LVLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable
  Transient-Free 3D reconstruction from Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camille Billouard, Dawa Derksen, Emmanuelle Sarrazin, Bruno Vallet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current stereo-vision pipelines produce high accuracy 3D reconstruction when
using multiple pairs or triplets of satellite images. However, these pipelines
are sensitive to the changes between images that can occur as a result of
multi-date acquisitions. Such variations are mainly due to variable shadows,
reflexions and transient objects (cars, vegetation). To take such changes into
account, Neural Radiance Fields (NeRF) have recently been applied to multi-date
satellite imagery. However, Neural methods are very compute-intensive, taking
dozens of hours to learn, compared with minutes for standard stereo-vision
pipelines. Following the ideas of Instant Neural Graphics Primitives we propose
to use an efficient sampling strategy and multi-resolution hash encoding to
accelerate the learning. Our model, Satellite Neural Graphics Primitives
(SAT-NGP) decreases the learning time to 15 minutes while maintaining the
quality of the 3D reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table; Accepted to International Geoscience and
  Remote Sensing Symposium (IGARSS) 2024; Code available at
  https://github.com/Ellimac0/SAT-NGP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning with Orthonormal Anchors (CLOA) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on addressing the instability issues prevalent in
contrastive learning, specifically examining the InfoNCE loss function and its
derivatives. We reveal a critical observation that these loss functions exhibit
a restrictive behavior, leading to a convergence phenomenon where embeddings
tend to merge into a singular point. This "over-fusion" effect detrimentally
affects classification accuracy in subsequent supervised-learning tasks.
Through theoretical analysis, we demonstrate that embeddings, when equalized or
confined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In
response to this challenge, our research introduces an innovative strategy that
leverages the same or fewer labeled data than typically used in the fine-tuning
phase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to
disentangle embedding clusters, significantly enhancing the distinctiveness of
each embedding while simultaneously ensuring their aggregation into dense,
well-defined clusters. Our method demonstrates remarkable improvements with
just a fraction of the conventional label requirements, as evidenced by our
results on CIFAR10 and CIFAR100 datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Annolid: Annotate, Segment, and Track Anything You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Yang, Thomas A. Cleland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annolid is a deep learning-based software package designed for the
segmentation, labeling, and tracking of research targets within video files,
focusing primarily on animal behavior analysis. Based on state-of-the-art
instance segmentation methods, Annolid now harnesses the Cutie video object
segmentation model to achieve resilient, markerless tracking of multiple
animals from single annotated frames, even in environments in which they may be
partially or entirely concealed by environmental features or by one another.
Our integration of Segment Anything and Grounding-DINO strategies additionally
enables the automatic masking and segmentation of recognizable animals and
objects by text command, removing the need for manual annotation. Annolid's
comprehensive approach to object segmentation flexibly accommodates a broad
spectrum of behavior analysis applications, enabling the classification of
diverse behavioral states such as freezing, digging, pup huddling, and social
interactions in addition to the tracking of animals and their body parts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransFusion: Contrastive Learning with Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18681v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18681v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanran Li, Daniel Pimentel-Alarcón
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel framework, TransFusion, designed to make the
process of contrastive learning more analytical and explainable. TransFusion
consists of attention blocks whose softmax being replaced by ReLU, and its
final block's weighted-sum operation is truncated to leave the adjacency matrix
as the output. The model is trained by minimizing the Jensen-Shannon Divergence
between its output and the target affinity matrix, which indicates whether each
pair of samples belongs to the same or different classes. The main contribution
of TransFusion lies in defining a theoretical limit for answering two
fundamental questions in the field: the maximum level of data augmentation and
the minimum batch size required for effective contrastive learning.
Furthermore, experimental results indicate that TransFusion successfully
extracts features that isolate clusters from complex real-world data, leading
to improved classification accuracy in downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 4 figures,</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming for Relevance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Eini Porat, Danny Eytan, Uri Shalit
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vital signs are crucial in intensive care units (ICUs). They are used to
track the patient's state and to identify clinically significant changes.
Predicting vital sign trajectories is valuable for early detection of adverse
events. However, conventional machine learning metrics like RMSE often fail to
capture the true clinical relevance of such predictions. We introduce novel
vital sign prediction performance metrics that align with clinical contexts,
focusing on deviations from clinical norms, overall trends, and trend
deviations. These metrics are derived from empirical utility curves obtained in
a previous study through interviews with ICU clinicians. We validate the
metrics' usefulness using simulated and real clinical datasets (MIMIC and
eICU). Furthermore, we employ these metrics as loss functions for neural
networks, resulting in models that excel in predicting clinically significant
events. This research paves the way for clinically relevant machine learning
model evaluation and optimization, promising to improve ICU patient care. 10
pages, 9 figures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, AMIA Informatics 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INEXA: Interactive and Explainable Process Model Abstraction Through
  Object-Centric Process Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janik-Vasily Benzin, Gyunam Park, Juergen Mangler, Stefanie Rinderle-Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Process events are recorded by multiple information systems at different
granularity levels. Based on the resulting event logs, process models are
discovered at different granularity levels, as well. Events stored at a
fine-grained granularity level, for example, may hinder the discovered process
model to be displayed due the high number of resulting model elements. The
discovered process model of a real-world manufacturing process, for example,
consists of 1,489 model elements and over 2,000 arcs. Existing process model
abstraction techniques could help reducing the size of the model, but would
disconnect it from the underlying event log. Existing event abstraction
techniques do neither support the analysis of mixed granularity levels, nor
interactive exploration of a suitable granularity level. To enable the
exploration of discovered process models at different granularity levels, we
propose INEXA, an interactive, explainable process model abstraction method
that keeps the link to the event log. As a starting point, INEXA aggregates
large process models to a "displayable" size, e.g., for the manufacturing use
case to a process model with 58 model elements. Then, the process analyst can
explore granularity levels interactively, while applied abstractions are
automatically traced in the event log for explainability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic
  Learning over Low-power Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Fu, Gaolei Li, Jun Wu, Jianhua Li, Xi Lin, Kai Zhou, Yuchen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural
networks and federated learning frameworks to effectively execute intelligent
analysis tasks over amounts of distributed low-power devices but also perform
vulnerability to poisoning attacks. The threat of backdoor attacks on
traditional deep neural networks typically comes from time-invariant data.
However, in FedNL, unknown threats may be hidden in time-varying spike signals.
In this paper, we start to explore a novel vulnerability of FedNL-based systems
with the concept of time division multiplexing, termed Spikewhisper, which
allows attackers to evade detection as much as possible, as multiple malicious
clients can imperceptibly poison with different triggers at different
timeslices. In particular, the stealthiness of Spikewhisper is derived from the
time-domain divisibility of global triggers, in which each malicious client
pastes only one local trigger to a certain timeslice in the neuromorphic
sample, and also the polarity and motion of each local trigger can be
configured by attackers. Extensive experiments based on two different
neuromorphic datasets demonstrate that the attack success rate of Spikewispher
is higher than the temporally centralized attacks. Besides, it is validated
that the effect of Spikewispher is sensitive to the trigger duration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAP: Retrieval-Augmented Planner for <span class="highlight-title">Adaptive</span> Procedure Planning in
  Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Zare, Yulei Niu, Hammad Ayyubi, Shih-fu Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Procedure Planning in instructional videos entails generating a sequence of
action steps based on visual observations of the initial and target states.
Despite the rapid progress in this task, there remain several critical
challenges to be solved: (1) Adaptive procedures: Prior works hold an
unrealistic assumption that the number of action steps is known and fixed,
leading to non-generalizable models in real-world scenarios where the sequence
length varies. (2) Temporal relation: Understanding the step temporal relation
knowledge is essential in producing reasonable and executable plans. (3)
Annotation cost: Annotating instructional videos with step-level labels (i.e.,
timestamp) or sequence-level labels (i.e., action category) is demanding and
labor-intensive, limiting its generalizability to large-scale datasets.In this
work, we propose a new and practical setting, called adaptive procedure
planning in instructional videos, where the procedure length is not fixed or
pre-determined. To address these challenges we introduce Retrieval-Augmented
Planner (RAP) model. Specifically, for adaptive procedures, RAP adaptively
determines the conclusion of actions using an auto-regressive model
architecture. For temporal relation, RAP establishes an external memory module
to explicitly retrieve the most relevant state-action pairs from the training
videos and revises the generated procedures. To tackle high annotation cost,
RAP utilizes a weakly-supervised learning manner to expand the training dataset
to other task-relevant, unannotated videos by generating pseudo labels for
action steps. Experiments on CrossTask and COIN benchmarks show the superiority
of RAP over traditional fixed-length models, establishing it as a strong
baseline solution for adaptive procedure planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote
  Sensing Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Run Shao, Zhaoyang Zhang, Chao Tao, Yunsheng Zhang, Chengli Peng, Haifeng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tokenizer, as one of the fundamental components of large models, has long
been overlooked or even misunderstood in visual tasks. One key factor of the
great comprehension power of the large language model is that natural language
tokenizers utilize meaningful words or subwords as the basic elements of
language. In contrast, mainstream visual tokenizers, represented by patch-based
methods such as Patch Embed, rely on meaningless rectangular patches as basic
elements of vision, which cannot serve as effectively as words or subwords in
language. Starting from the essence of the tokenizer, we defined semantically
independent regions (SIRs) for vision. We designed a simple HOmogeneous visual
tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception
Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,
the OPM splits the image into 4*4 pixel seeds and then utilizes the attention
mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds
within the same SIR. To achieve adaptability, the OVM defines a variable number
of learnable vectors as cross-attention queries, allowing for the adjustment of
token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19
classification dataset, and GID5 segmentation dataset for sparse and dense
tasks. The results demonstrate that the visual tokens obtained by HOOK
correspond to individual objects, which demonstrates homogeneity. HOOK
outperformed Patch Embed by 6\% and 10\% in the two tasks and achieved
state-of-the-art performance compared to the baselines used for comparison.
Compared to Patch Embed, which requires more than one hundred tokens for one
image, HOOK requires only 6 and 8 tokens for sparse and dense tasks,
respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The
code is available at https://github.com/GeoX-Lab/Hook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Graph Neural Networks for Water Distribution Systems <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inaam Ashraf, Janine Strotherm, Luca Hermes, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Water distribution systems (WDS) are an integral part of critical
infrastructure which is pivotal to urban development. As 70% of the world's
population will likely live in urban environments in 2050, efficient simulation
and planning tools for WDS play a crucial role in reaching UN's sustainable
developmental goal (SDG) 6 - "Clean water and sanitation for all". In this
realm, we propose a novel and efficient machine learning emulator, more
precisely, a physics-informed deep learning (DL) model, for hydraulic state
estimation in WDS. Using a recursive approach, our model only needs a few graph
convolutional neural network (GCN) layers and employs an innovative algorithm
based on message passing. Unlike conventional machine learning tasks, the model
uses hydraulic principles to infer two additional hydraulic state features in
the process of reconstructing the available ground truth feature in an
unsupervised manner. To the best of our knowledge, this is the first DL
approach to emulate the popular hydraulic simulator EPANET, utilizing no
additional information. Like most DL models and unlike the hydraulic simulator,
our model demonstrates vastly faster emulation times that do not increase
drastically with the size of the WDS. Moreover, we achieve high accuracy on the
ground truth and very similar results compared to the hydraulic simulator as
demonstrated through experiments on five real-world WDS datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper with the same title published at
  Proceedings of the AAAI Conference on Artificial Intelligence 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop
  <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiang Zhao, Zhuomin Chai, Xun Jiang, Yibo Lin, Runsheng Wang, Ru Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  IR drop on the power delivery network (PDN) is closely related to PDN's
configuration and cell current consumption. As the integrated circuit (IC)
design is growing larger, dynamic IR drop simulation becomes computationally
unaffordable and machine learning based IR drop prediction has been explored as
a promising solution. Although CNN-based methods have been adapted to IR drop
prediction task in several works, the shortcomings of overlooking PDN
configuration is non-negligible. In this paper, we consider not only how to
properly represent cell-PDN relation, but also how to model IR drop following
its physical nature in the feature aggregation procedure. Thus, we propose a
novel graph structure, PDNGraph, to unify the representations of the PDN
structure and the fine-grained cell-PDN relation. We further propose a
dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN
branches to favorably capture the above features during the learning process.
Several key designs are presented to make the dynamic IR drop prediction highly
effective and interpretable. We are the first work to apply graph structure to
deep-learning based dynamic IR drop prediction method. Experiments show that
PDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%
reduction in prediction error and achieves 545x speedup compared to the
commercial tool, which demonstrates the superiority of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Architecture Search for Sentence Classification with BERT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Sarah Schröder, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre training of language models on large text corpora is common practice in
Natural Language Processing. Following, fine tuning of these models is
performed to achieve the best results on a variety of tasks. In this paper we
question the common practice of only adding a single output layer as a
classification head on top of the network. We perform an AutoML search to find
architectures that outperform the current single layer at only a small compute
cost. We validate our classification architecture on a variety of NLP
benchmarks from the GLUE dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18546v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18546v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siang Chen, Wei Tang, Pengwei Xie, Wenming Yang, Guijin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast and robust object grasping in clutter is a crucial component of
robotics. Most current works resort to the whole observed point cloud for 6-Dof
grasp generation, ignoring the guidance information excavated from global
semantics, thus limiting high-quality grasp generation and real-time
performance. In this work, we show that the widely used heatmaps are
underestimated in the efficiency of 6-Dof grasp generation. Therefore, we
propose an effective local grasp generator combined with grasp heatmaps as
guidance, which infers in a global-to-local semantic-to-point way.
Specifically, Gaussian encoding and the grid-based strategy are applied to
predict grasp heatmaps as guidance to aggregate local points into graspable
regions and provide global semantic information. Further, a novel non-uniform
anchor sampling mechanism is designed to improve grasp accuracy and diversity.
Benefiting from the high-efficiency encoding in the image space and focusing on
points in local graspable regions, our framework can perform high-quality grasp
detection in real-time and achieve state-of-the-art results. In addition, real
robot experiments demonstrate the effectiveness of our method with a success
rate of 94% and a clutter completion rate of 100%. Our code is available at
https://github.com/THU-VCLab/HGGD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extensive results on GraspNet-1B dataset</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Path Towards Legal Autonomy: An interoperable and explainable approach
  to extracting, transforming, loading and computing legal information using
  large language models, expert systems and Bayesian networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Axel Constant, Hannes Westermann, Bryan Wilson, Alex Kiefer, Ines Hipolito, Sylvain Pronovost, Steven Swanson, Mahault Albarracin, Maxwell J. D. Ramstead
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal autonomy - the lawful activity of artificial intelligence agents - can
be achieved in one of two ways. It can be achieved either by imposing
constraints on AI actors such as developers, deployers and users, and on AI
resources such as data, or by imposing constraints on the range and scope of
the impact that AI agents can have on the environment. The latter approach
involves encoding extant rules concerning AI driven devices into the software
of AI agents controlling those devices (e.g., encoding rules about limitations
on zones of operations into the agent software of an autonomous drone device).
This is a challenge since the effectivity of such an approach requires a method
of extracting, loading, transforming and computing legal information that would
be both explainable and legally interoperable, and that would enable AI agents
to reason about the law. In this paper, we sketch a proof of principle for such
a method using large language models (LLMs), expert legal systems known as
legal decision paths, and Bayesian networks. We then show how the proposed
method could be applied to extant regulation in matters of autonomous cars,
such as the California Vehicle Code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Behavior-Based Recommendation System for E-commerce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Barzegar Nozari, Mahdi Divsalar, Sepehr Akbarzadeh Abkenar, Mohammadreza Fadavi Amiri, Ali Divsalar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The majority of existing recommender systems rely on user ratings, which are
limited by the lack of user collaboration and the sparsity problem. To address
these issues, this study proposes a behavior-based recommender system that
leverages customers' natural behaviors, such as browsing and clicking, on
e-commerce platforms. The proposed recommendation system involves clustering
active customers, determining neighborhoods, collecting similar users,
calculating product reputation based on similar users, and recommending
high-reputation products. To overcome the complexity of customer behaviors and
traditional clustering methods, an unsupervised clustering approach based on
product categories is developed to enhance the recommendation methodology. This
study makes notable contributions in several aspects. Firstly, a groundbreaking
behavior-based recommendation methodology is developed, incorporating customer
behavior to generate accurate and tailored recommendations leading to improved
customer satisfaction and engagement. Secondly, an original unsupervised
clustering method, focusing on product categories, enables more precise
clustering and facilitates accurate recommendations. Finally, an approach to
determine neighborhoods for active customers within clusters is established,
ensuring grouping of customers with similar behavioral patterns to enhance
recommendation accuracy and relevance. The proposed recommendation methodology
and clustering method contribute to improved recommendation performance,
offering valuable insights for researchers and practitioners in the field of
e-commerce recommendation systems. Additionally, the proposed method
outperforms benchmark methods in experiments conducted using a behavior dataset
from the well-known e-commerce site Alibaba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Line Search Methods for Large Scale Neural Network Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent studies, line search methods have shown significant improvements in
the performance of traditional stochastic gradient descent techniques,
eliminating the need for a specific learning rate schedule. In this paper, we
identify existing issues in state-of-the-art line search methods, propose
enhancements, and rigorously evaluate their effectiveness. We test these
methods on larger datasets and more complex data domains than before.
Specifically, we improve the Armijo line search by integrating the momentum
term from ADAM in its search direction, enabling efficient large-scale
training, a task that was previously prone to failure using Armijo line search
methods. Our optimization approach outperforms both the previous Armijo
implementation and tuned learning rate schedules for Adam. Our evaluation
focuses on Transformers and CNNs in the domains of NLP and image data. Our work
is publicly available as a Python package, which provides a hyperparameter free
Pytorch optimizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Convergence for Transformer Fine-tuning with Line Search Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Kenneweg, Leonardo Galli, Tristan Kenneweg, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown that line search methods greatly increase performance
of traditional stochastic gradient descent methods on a variety of datasets and
architectures [1], [2]. In this work we succeed in extending line search
methods to the novel and highly popular Transformer architecture and dataset
domains in natural language processing. More specifically, we combine the
Armijo line search with the Adam optimizer and extend it by subdividing the
networks architecture into sensible units and perform the line search
separately on these local units. Our optimization method outperforms the
traditional Adam optimizer and achieves significant performance improvements
for small data sets or small training budgets, while performing equal or better
for other tested cases. Our work is publicly available as a python package,
which provides a hyperparameter-free pytorch optimizer that is compatible with
arbitrary network architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Employing Weather Forecast Data as Input to the Estimation of
  Evapotranspiration by Deep Neural Network Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro J. Vaz, Gabriela Schütz, Carlos Guerrero, Pedro J. S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reference Evapotranspiration (ET0) is a key parameter for designing smart
irrigation scheduling, since it is related by a coefficient to the water needs
of a crop. The United Nations Food and Agriculture Organization, proposed a
standard method for ET0 computation (FAO56PM), based on the parameterization of
the Penman-Monteith equation, that is widely adopted in the literature. To
compute ET0 using the FAO56-PM method, four main weather parameters are needed:
temperature, humidity, wind, and solar radiation (SR). One way to make daily
ET0 estimations for future days is to use freely available weather forecast
services (WFSs), where many meteorological parameters are estimated up to the
next 15 days. A problem with this method is that currently, SR is not provided
as a free forecast parameter on most of those online services or, normally,
such forecasts present a financial cost penalty. For this reason, several ET0
estimation models using machine and deep learning were developed and presented
in the literature, that use as input features a reduced set of carefully
selected weather parameters, that are compatible with common freely available
WFSs. However, most studies on this topic have only evaluated model performance
using data from weather stations (WSs), without considering the effect of using
weather forecast data. In this study, the performance of authors' previous
models is evaluated when using weather forecast data from two online WFSs, in
the following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)
estimate SR by ANN model, and then use that estimation for ET0 computation,
using the FAO56-PM method. Employing data collected from two WFSs and a WS
located in Vale do Lobo, Portugal, the latter approach achieved the best
result, with a coefficient of determination (R2) ranging between 0.893 and
0.667, when considering forecasts up to 15 days.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A partial version of the work submitted to ESRE/INTERNATIONAL
  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthesizing EEG Signals from Event-Related Potential Paradigms with
  Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guido Klein, Pierre Guetschel, Gianluigi Silvestri, Michael Tangermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data scarcity in the brain-computer interface field can be alleviated through
the use of generative models, specifically diffusion models. While diffusion
models have previously been successfully applied to electroencephalogram (EEG)
data, existing models lack flexibility w.r.t.~sampling or require alternative
representations of the EEG data. To overcome these limitations, we introduce a
novel approach to conditional diffusion models that utilizes classifier-free
guidance to directly generate subject-, session-, and class-specific EEG data.
In addition to commonly used metrics, domain-specific metrics are employed to
evaluate the specificity of the generated samples. The results indicate that
the proposed model can generate EEG data that resembles real data for each
subject, session, and class.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to 9th Graz BCI conference, 6 pages, 3 figures, first
  figure is split into two subfigures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain
  <span class="highlight-title">Adaptive</span> Segmentation of 3D Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Yuan, Wankang Zeng, Yanfei Su, Weiquan Liu, Ming Cheng, Yulan Guo, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to
annotating new domains. Self-training is a competitive approach for this task,
but its performance is limited by different sensor sampling patterns (i.e.,
variations in point density) and incomplete training strategies. In this work,
we propose a density-guided translator (DGT), which translates point density
between domains, and integrates it into a two-stage self-training pipeline
named DGT-ST. First, in contrast to existing works that simultaneously conduct
data generation and feature/output alignment within unstable adversarial
training, we employ the non-learnable DGT to bridge the domain gap at the input
level. Second, to provide a well-initialized model for self-training, we
propose a category-level adversarial network in stage one that utilizes the
prototype to prevent negative transfer. Finally, by leveraging the designs
above, a domain-mixed self-training method with source-aware consistency loss
is proposed in stage two to narrow the domain gap further. Experiments on two
synthetic-to-real segmentation tasks (SynLiDAR $\rightarrow$ semanticKITTI and
SynLiDAR $\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms
state-of-the-art methods, achieving 9.4$\%$ and 4.3$\%$ mIoU improvements,
respectively. Code is available at \url{https://github.com/yuan-zm/DGT-ST}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Ionova, Jan Kristof Behrens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assembly processes involving humans and robots are challenging scenarios
because the individual activities and access to shared workspace have to be
coordinated. Fixed robot programs leave no room to diverge from a fixed
protocol. Working on such a process can be stressful for the user and lead to
ineffective behavior or failure. We propose a novel approach of online
constraint-based scheduling in a reactive execution control framework
facilitating behavior trees called CoBOS. This allows the robot to adapt to
uncertain events such as delayed activity completions and activity selection
(by the human). The user will experience less stress as the robotic coworkers
adapt their behavior to best complement the human-selected activities to
complete the common task. In addition to the improved working conditions, our
algorithm leads to increased efficiency, even in highly uncertain scenarios. We
evaluate our algorithm using a probabilistic simulation study with 56000
experiments. We outperform all baselines by a margin of 4-10%. Initial real
robot experiments using a Franka Emika Panda robot and human tracking based on
HTC Vive VR gloves look promising.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in
  Resource-Constrained CPS and IoT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Hu, Jinhang Zuo, Alanis Zhao, Bob Iannucci, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FMs) emerge as a promising solution to harness distributed
and diverse environmental data by leveraging prior knowledge to understand the
complicated temporal and spatial correlations within heterogeneous datasets.
Unlike distributed learning frameworks such as federated learning, which often
struggle with multimodal data, FMs can transform diverse inputs into
embeddings. This process facilitates the integration of information from
various modalities and the application of prior learning to new domains.
However, deploying FMs in resource-constrained edge systems poses significant
challenges. To this end, we introduce CoRAST, a novel learning framework that
utilizes FMs for enhanced analysis of distributed, correlated heterogeneous
data. Utilizing a server-based FM, CoRAST can exploit existing environment
information to extract temporal, spatial, and cross-modal correlations among
sensor data. This enables CoRAST to offer context-aware insights for localized
client tasks through FM-powered global representation learning. Our evaluation
on real-world weather dataset demonstrates CoRAST's ability to exploit
correlated heterogeneous data through environmental representation learning to
reduce the forecast errors by up to 50.3% compared to the baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted and to be published in 2024 IEEE International Workshop on
  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ilias Mitsouras, Eleftherios Tsonis, Paraskevi Tzouveli, Athanasios Voulodimos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated remarkable performance in text-to-image
synthesis, producing realistic and high resolution images that faithfully
adhere to the corresponding text-prompts. Despite their great success, they
still fall behind in sketch-to-image synthesis tasks, where in addition to
text-prompts, the spatial layout of the generated images has to closely follow
the outlines of certain reference sketches. Employing an MLP latent edge
predictor to guide the spatial layout of the synthesized image by predicting
edge maps at each denoising step has been recently proposed. Despite yielding
promising results, the pixel-wise operation of the MLP does not take into
account the spatial layout as a whole, and demands numerous denoising
iterations to produce satisfactory images, leading to time inefficiency. To
this end, we introduce U-Sketch, a framework featuring a U-Net type latent edge
predictor, which is capable of efficiently capturing both local and global
features, as well as spatial correlations between pixels. Moreover, we propose
the addition of a sketch simplification network that offers the user the choice
of preprocessing and simplifying input sketches for enhanced outputs. The
experimental results, corroborated by user feedback, demonstrate that our
proposed U-Net latent edge predictor leads to more realistic results, that are
better aligned with the spatial outlines of the reference sketches, while
drastically reducing the number of required denoising steps and, consequently,
the overall execution time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18421v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18421v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance
on a wide variety of biomedical NLP tasks. However, these models have hundreds
of billions of parameters, are computationally expensive to run, require users
to send their input data over the internet, and are trained on unknown data
sources. Can smaller, more targeted models compete? To address this question,
we build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive
model trained exclusively on PubMed abstracts and full articles. When
fine-tuned, BioMedLM can produce strong multiple-choice biomedical
question-answering results competitive with much larger models, such as
achieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical
Genetics exam. BioMedLM can also be fine-tuned to produce useful answers to
patient questions on medical topics. This demonstrates that smaller models can
potentially serve as transparent, privacy-preserving, economical and
environmentally friendly foundations for particular NLP applications, such as
in biomedicine. The model is available on the Hugging Face Hub:
https://huggingface.co/stanford-crfm/BioMedLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is
  Critical for Semi-supervised Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Wu, Junbiao Pang, Baochang Zhang, Qingming Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) is a practical challenge in computer vision.
Pseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of
The Art (SOTA) performances in SSL. These approaches employ a
threshold-to-pseudo-label (T2L) process to generate PLs by truncating the
confidence scores of unlabeled data predicted by the self-training method.
However, self-trained models typically yield biased and high-variance
predictions, especially in the scenarios when a little labeled data are
supplied. To address this issue, we propose a lightweight channel-based
ensemble method to effectively consolidate multiple inferior PLs into the
theoretically guaranteed unbiased and low-variance one. Importantly, our
approach can be readily extended to any SSL framework, such as FixMatch or
FreeMatch. Experimental results demonstrate that our method significantly
outperforms state-of-the-art techniques on CIFAR10/100 in terms of
effectiveness and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering
  Using a VLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkyun Kim, Changin Choi, Wonseok Lee, Wonjong Rhee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stimulated by the sophisticated reasoning capabilities of recent Large
Language Models (LLMs), a variety of strategies for bridging video modality
have been devised. A prominent strategy involves Video Language Models
(VideoLMs), which train a learnable interface with video data to connect
advanced vision encoders with LLMs. Recently, an alternative strategy has
surfaced, employing readily available foundation models, such as VideoLMs and
LLMs, across multiple stages for modality bridging. In this study, we introduce
a simple yet novel strategy where only a single Vision Language Model (VLM) is
utilized. Our starting point is the plain insight that a video comprises a
series of images, or frames, interwoven with temporal information. The essence
of video comprehension lies in adeptly managing the temporal aspects along with
the spatial details of each frame. Initially, we transform a video into a
single composite image by arranging multiple frames in a grid layout. The
resulting single image is termed as an image grid. This format, while
maintaining the appearance of a solitary image, effectively retains temporal
information within the grid structure. Therefore, the image grid approach
enables direct application of a single high-performance VLM without
necessitating any video-data training. Our extensive experimental analysis
across ten zero-shot video question answering benchmarks, including five
open-ended and five multiple-choice benchmarks, reveals that the proposed Image
Grid Vision Language Model (IG-VLM) surpasses the existing methods in nine out
of ten benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code is available at https://github.com/imagegridworth/IG-VLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Relevance Judgments in Legal Case
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengjie Ma, Chong Chen, Qi Chu, Jiaxin Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting relevant judgments for legal case retrieval is a challenging and
time-consuming task. Accurately judging the relevance between two legal cases
requires a considerable effort to read the lengthy text and a high level of
domain expertise to extract Legal Facts and make juridical judgments. With the
advent of advanced large language models, some recent studies have suggested
that it is promising to use LLMs for relevance judgment. Nonetheless, the
method of employing a general large language model for reliable relevance
judgments in legal case retrieval is yet to be thoroughly explored. To fill
this research gap, we devise a novel few-shot workflow tailored to the relevant
judgment of legal cases. The proposed workflow breaks down the annotation
process into a series of stages, imitating the process employed by human
annotators and enabling a flexible integration of expert reasoning to enhance
the accuracy of relevance judgments. By comparing the relevance judgments of
LLMs and human experts, we empirically show that we can obtain reliable
relevance judgments with the proposed workflow. Furthermore, we demonstrate the
capacity to augment existing legal case retrieval models through the synthesis
of data generated by the large language model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colour and Brush Stroke Pattern Recognition in Abstract Art using
  Modified Deep Convolutional Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srinitish Srinivasan, Varenya Pathak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstract Art is an immensely popular, discussed form of art that often has
the ability to depict the emotions of an artist. Many researchers have made
attempts to study abstract art in the form of edge detection, brush stroke and
emotion recognition algorithms using machine and deep learning. This papers
describes the study of a wide distribution of abstract paintings using
Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and
reproduce a distribution enabling researchers and scientists to effectively
explore and study the generated image space. However, the challenge lies in
developing an efficient GAN architecture that overcomes common training
pitfalls. This paper addresses this challenge by introducing a modified-DCGAN
(mDCGAN) specifically designed for high-quality artwork generation. The
approach involves a thorough exploration of the modifications made, delving
into the intricate workings of DCGANs, optimisation techniques, and
regularisation methods aimed at improving stability and realism in art
generation enabling effective study of generated patterns. The proposed mDCGAN
incorporates meticulous adjustments in layer configurations and architectural
choices, offering tailored solutions to the unique demands of art generation
while effectively combating issues like mode collapse and gradient vanishing.
Further this paper explores the generated latent space by performing random
walks to understand vector relationships between brush strokes and colours in
the abstract art space and a statistical analysis of unstable outputs after a
certain period of GAN training and compare its significant difference. These
findings validate the effectiveness of the proposed approach, emphasising its
potential to revolutionise the field of digital art generation and digital art
ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 5 tables, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Wu, Velibor Bojkovic, Bin Gu, Kun Suo, Kai Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient
computing compared with Artificial Neural Networks (ANNs), closely mirroring
biological neural processes. However, this potential comes with inherent
challenges in directly training SNNs through spatio-temporal backpropagation --
stemming from the temporal dynamics of spiking neurons and their discrete
signal processing -- which necessitates alternative ways of training, most
notably through ANN-SNN conversion. In this work, we introduce a lightweight
Forward Temporal Bias Correction (FTBC) technique, aimed at enhancing
conversion accuracy without the computational overhead. We ground our method on
provided theoretical findings that through proper temporal bias calibration the
expected error of ANN-SNN conversion can be reduced to be zero after each time
step. We further propose a heuristic algorithm for finding the temporal bias
only in the forward pass, thus eliminating the computational burden of
backpropagation and we evaluate our method on CIFAR-10/100 and ImageNet
datasets, achieving a notable increase in accuracy on all datasets. Codes are
released at a GitHub repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Multi-modal Models are Good Class-Incremental Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Cao, Haori Lu, Linlan Huang, Xialei Liu, Ming-Ming Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic
forgetting caused by the classifier's bias towards the current task has long
posed a significant challenge. It is mainly caused by the characteristic of
discriminative models. With the growing popularity of the generative
multi-modal models, we would explore replacing discriminative models with
generative ones for CIL. However, transitioning from discriminative to
generative models requires addressing two key challenges. The primary challenge
lies in transferring the generated textual information into the classification
of distinct categories. Additionally, it requires formulating the task of CIL
within a generative framework. To this end, we propose a novel generative
multi-modal model (GMM) framework for class-incremental learning. Our approach
directly generates labels for images using an adapted generative model. After
obtaining the detailed text, we use a text encoder to extract text features and
employ feature matching to determine the most similar label as the
classification prediction. In the conventional CIL settings, we achieve
significantly better results in long-sequence task scenarios. Under the
Few-shot CIL setting, we have improved by at least 14\% accuracy over all the
current state-of-the-art methods with significantly less forgetting. Our code
is available at \url{https://github.com/DoubleClass/GMM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Attributed Text Generation of Large Language Models via
  Preference Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfang Li, Zetian Sun, Baotian Hu, Zhenyu Liu, Xinshuo Hu, Xuebo Liu, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have been widely adopted in natural language
processing, yet they face the challenge of generating unreliable content.
Recent works aim to reduce misinformation and hallucinations by resorting to
attribution as a means to provide evidence (i.e., citations). However, current
attribution methods usually focus on the retrieval stage and automatic
evaluation that neglect mirroring the citation mechanisms in human scholarly
writing to bolster credibility. In this paper, we address these challenges by
modelling the attribution task as preference learning and introducing an
Automatic Preference Optimization (APO) framework. First, we create a curated
collection for post-training with 6,330 examples by collecting and filtering
from existing datasets. Second, considering the high cost of labelling
preference data, we further propose an automatic method to synthesize
attribution preference data resulting in 95,263 pairs. Moreover, inspired by
the human citation process, we further propose a progressive preference
optimization method by leveraging fine-grained information. Extensive
experiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate
that APO achieves state-of-the-art citation F1 with higher answer quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 15 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining
  Useful Life <span class="highlight-title">Prediction</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzai Ye, Li Feng, Jianlan Guo, Yuqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion
batteries is crucial for maintaining the safe and stable operation of
rechargeable battery management systems. However, this task is often
challenging due to the complex temporal dynamics involved. Recently,
attention-based networks, such as Transformers and Informer, have been the
popular architecture in time series forecasting. Despite their effectiveness,
these models with abundant parameters necessitate substantial training time to
unravel temporal patterns. To tackle these challenges, we propose a simple
MLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which
is an architecture based exclusively on multi-layer perceptrons (MLPs),
extracting information by mixing operations along both intra-patch and
inter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer
comprises parallel dual-head mixer layers: the intra-patch mixing MLP,
capturing local temporal patterns in the short-term period, and the inter-patch
mixing MLP, capturing global temporal patterns in the long-term period.
Notably, to address the varying importance of features in RUL prediction, we
introduce a weighted loss function in the MLP-Mixer-based architecture, marking
the first time such an approach has been employed. Our experiments demonstrate
that IIP-Mixer achieves competitive performance in battery RUL prediction,
outperforming other popular time-series frameworks
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, Mehdi Bennis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of supporting Industrial Internet of Things user
equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and
random traffic arrival. A deep reinforcement learning (DRL) based centralized
dynamic scheduler for time-frequency resources is proposed to learn how to
schedule the available communication resources among the IIoT UEs. The proposed
scheduler leverages an RL framework to adapt to the dynamic changes in the
wireless communication system and traffic arrivals. Moreover, a graph-based
reduction scheme is proposed to reduce the state and action space of the RL
framework to allow fast convergence and a better learning strategy. Simulation
results demonstrate the effectiveness of the proposed intelligent scheduler in
guaranteeing the expressed intent of IIoT UEs compared to several traditional
scheduling schemes, such as round-robin, semi-static, and heuristic approaches.
The proposed scheduler also outperforms the contention-free and
contention-based schemes in maximizing the number of successfully computed
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Diverse Agricultural Data for Vision-Based Farming
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikolaj Cieslak, Umabharathi Govindarajan, Alejandro Garcia, Anuradha Chandrashekar, Torsten Hädrich, Aleksander Mendoza-Drosik, Dominik L. Michels, Sören Pirk, Chia-Chun Fu, Wojciech Pałubicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specialized procedural model for generating synthetic
agricultural scenes, focusing on soybean crops, along with various weeds. This
model is capable of simulating distinct growth stages of these plants, diverse
soil conditions, and randomized field arrangements under varying lighting
conditions. The integration of real-world textures and environmental factors
into the procedural generation process enhances the photorealism and
applicability of the synthetic data. Our dataset includes 12,000 images with
semantic labels, offering a comprehensive resource for computer vision tasks in
precision agriculture, such as semantic segmentation for autonomous weed
control. We validate our model's effectiveness by comparing the synthetic data
against real agricultural images, demonstrating its potential to significantly
augment training data for machine learning models in agriculture. This approach
not only provides a cost-effective solution for generating high-quality,
diverse data but also addresses specific needs in agricultural vision tasks
that are not fully covered by general-purpose models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal
  Holes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanmoy Bandyopadhyay, Suman Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection and analysis of the solar coronal holes (CHs) is an important
field of study in the domain of solar physics. Mainly, it is required for the
proper prediction of the geomagnetic storms which directly or indirectly affect
various space and ground-based systems. For the detection of CHs till date, the
solar scientist depends on manual hand-drawn approaches. However, with the
advancement of image processing technologies, some automated image segmentation
methods have been used for the detection of CHs. In-spite of this, fast and
accurate detection of CHs are till a major issues. Here in this work, a novel
quantum computing-based fast fuzzy c-mean technique has been developed for fast
detection of the CHs region. The task has been carried out in two stages, in
first stage the solar image has been segmented using a quantum computing based
fast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted
out from the segmented image based on image morphological operation. In the
work, quantum computing has been used to optimize the cost function of the fast
fuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm
(QAOA) has been used to optimize the quadratic part of the cost function. The
proposed method has been tested for 193 \AA{} SDO/AIA full-disk solar image
datasets and has been compared with the existing techniques. The outcome shows
the comparable performance of the proposed method with the existing one within
a very lesser time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LC-LLM: Explainable Lane-Change Intention and Trajectory <span class="highlight-title">Prediction</span>s
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu, Kehua Chen,  Hao,  Yang, Xuesong Wang, Yinhai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To ensure safe driving in dynamic environments, autonomous vehicles should
possess the capability to accurately predict the lane change intentions of
surrounding vehicles in advance and forecast their future trajectories.
Existing motion prediction approaches have ample room for improvement,
particularly in terms of long-term prediction accuracy and interpretability. In
this paper, we address these challenges by proposing LC-LLM, an explainable
lane change prediction model that leverages the strong reasoning capabilities
and self-explanation abilities of Large Language Models (LLMs). Essentially, we
reformulate the lane change prediction task as a language modeling problem,
processing heterogeneous driving scenario information in natural language as
prompts for input into the LLM and employing a supervised fine-tuning technique
to tailor the LLM specifically for our lane change prediction task. This allows
us to utilize the LLM's powerful common sense reasoning abilities to understand
complex interactive information, thereby improving the accuracy of long-term
predictions. Furthermore, we incorporate explanatory requirements into the
prompts in the inference stage. Therefore, our LC-LLM model not only can
predict lane change intentions and trajectories but also provides explanations
for its predictions, enhancing the interpretability. Extensive experiments on
the large-scale highD dataset demonstrate the superior performance and
interpretability of our LC-LLM in lane change prediction task. To the best of
our knowledge, this is the first attempt to utilize LLMs for predicting lane
change behavior. Our study shows that LLMs can encode comprehensive interaction
information for driving behavior understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ mALBERT: Is a Compact Multilingual BERT Model Still Worth It? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christophe Servan, Sahar Ghannay, Sophie Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Within the current trend of Pretained Language Models (PLM), emerge more and
more criticisms about the ethical andecological impact of such models. In this
article, considering these critical remarks, we propose to focus on
smallermodels, such as compact models like ALBERT, which are more ecologically
virtuous than these PLM. However,PLMs enable huge breakthroughs in Natural
Language Processing tasks, such as Spoken and Natural LanguageUnderstanding,
classification, Question--Answering tasks. PLMs also have the advantage of
being multilingual, and,as far as we know, a multilingual version of compact
ALBERT models does not exist. Considering these facts, wepropose the free
release of the first version of a multilingual compact ALBERT model,
pre-trained using Wikipediadata, which complies with the ethical aspect of such
a language model. We also evaluate the model against classicalmultilingual PLMs
in classical NLP tasks. Finally, this paper proposes a rare study on the
subword tokenizationimpact on language performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 2024 Joint International Conference on Computational Linguistics,
  Language Resources and Evaluation, May 2024, Torino, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can LLMs Converse Formally? Automatically Assessing LLMs in Translating
  and Interpreting Formal Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18327v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18327v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushang Karia, Daksh Dobhal, Daniel Bramblett, Pulkit Verma, Siddharth Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stakeholders often describe system requirements using natural language which
are then converted to formal syntax by a domain-expert leading to increased
design costs. This paper assesses the capabilities of Large Language Models
(LLMs) in converting between natural language descriptions and formal
specifications. Existing work has evaluated the capabilities of LLMs in
generating formal syntax such as source code but such experiments are typically
hand-crafted and use problems that are likely to be in the training set of
LLMs, and often require human-annotated datasets. We propose an approach that
can use two copies of an LLM in conjunction with an off-the-shelf verifier to
automatically evaluate its translation abilities without any additional human
input. Our approach generates formal syntax using language grammars to
automatically generate a dataset. We conduct an empirical evaluation to measure
the accuracy of this translation task and show that SOTA LLMs cannot adequately
solve this task, limiting their current utility in the design of complex
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chinese Offensive Language Detection:Current Status and Future
  Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Xiao, Houda Bouamor, Wajdi Zaghouani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the considerable efforts being made to monitor and regulate
user-generated content on social media platforms, the pervasiveness of
offensive language, such as hate speech or cyberbullying, in the digital space
remains a significant challenge. Given the importance of maintaining a
civilized and respectful online environment, there is an urgent and growing
need for automatic systems capable of detecting offensive speech in real time.
However, developing effective systems for processing languages such as Chinese
presents a significant challenge, owing to the language's complex and nuanced
nature, which makes it difficult to process automatically. This paper provides
a comprehensive overview of offensive language detection in Chinese, examining
current benchmarks and approaches and highlighting specific models and tools
for addressing the unique challenges of detecting offensive language in this
complex language. The primary objective of this survey is to explore the
existing techniques and identify potential avenues for further research that
can address the cultural and linguistic complexities of Chinese.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Recommender System for NFT Collectibles with Item Feature <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18305v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18305v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong, Yongjae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been actively studied and applied in various domains
to deal with information overload. Although there are numerous studies on
recommender systems for movies, music, and e-commerce, comparatively less
attention has been paid to the recommender system for NFTs despite the
continuous growth of the NFT market. This paper presents a recommender system
for NFTs that utilizes a variety of data sources, from NFT transaction records
to external item features, to generate precise recommendations that cater to
individual preferences. We develop a data-efficient graph-based recommender
system to efficiently capture the complex relationship between each item and
users and generate node(item) embeddings which incorporate both node feature
information and graph structure. Furthermore, we exploit inputs beyond
user-item interactions, such as image feature, text feature, and price feature.
Numerical experiments verify the performance of the graph-based recommender
system improves significantly after utilizing all types of item features as
side information, thereby outperforming all other baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shrinivas Ramasubramanian, Harsh Rangwani, Sho Takemori, Kunal Samanta, Yuhei Umeda, Venkatesh Babu Radhakrishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise in internet usage has led to the generation of massive amounts of
data, resulting in the adoption of various supervised and semi-supervised
machine learning algorithms, which can effectively utilize the colossal amount
of data to train models. However, before deploying these models in the real
world, these must be strictly evaluated on performance measures like worst-case
recall and satisfy constraints such as fairness. We find that current
state-of-the-art empirical techniques offer sub-optimal performance on these
practical, non-decomposable performance objectives. On the other hand, the
theoretical techniques necessitate training a new model from scratch for each
performance objective. To bridge the gap, we propose SelMix, a selective
mixup-based inexpensive fine-tuning technique for pre-trained models, to
optimize for the desired objective. The core idea of our framework is to
determine a sampling distribution to perform a mixup of features between
samples from particular classes such that it optimizes the given objective. We
comprehensively evaluate our technique against the existing empirical and
theoretically principled methods on standard benchmark datasets for imbalanced
classification. We find that proposed SelMix fine-tuning significantly improves
the performance for various practical non-decomposable objectives across
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024 SpotLight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic
  Communication Paradigm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunhang Zheng, Kechao Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional approaches to semantic communication tasks rely on the knowledge
of the signal-to-noise ratio (SNR) to mitigate channel noise. However, these
methods necessitate training under specific SNR conditions, entailing
considerable time and computational resources. In this paper, we propose GeNet,
a Graph Neural Network (GNN)-based paradigm for semantic communication aimed at
combating noise, thereby facilitating Task-Oriented Communication (TOC). We
propose a novel approach where we first transform the input data image into
graph structures. Then we leverage a GNN-based encoder to extract semantic
information from the source data. This extracted semantic information is then
transmitted through the channel. At the receiver's end, a GNN-based decoder is
utilized to reconstruct the relevant semantic information from the source data
for TOC. Through experimental evaluation, we show GeNet's effectiveness in
anti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's
performance by varying the number of nodes, revealing its versatility as a new
paradigm for semantic communication. Additionally, we show GeNet's robustness
to geometric transformations by testing it with different rotation angles,
without resorting to data augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Recalibration of Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Urvashi Khandelwal, Kelvin Guu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has uncovered promising ways to extract well-calibrated
confidence estimates from language models (LMs), where the model's confidence
score reflects how likely it is to be correct. However, while LMs may appear
well-calibrated over broad distributions, this often hides significant
miscalibration within narrower slices (e.g., systemic over-confidence in math
can balance out systemic under-confidence in history, yielding perfect
calibration in aggregate). To attain well-calibrated confidence estimates for
any slice of a distribution, we propose a new framework for few-shot
slice-specific recalibration. Specifically, we train a recalibration model that
takes in a few unlabeled examples from any given slice and predicts a curve
that remaps confidence scores to be more accurate for that slice. Our trained
model can recalibrate for arbitrary new slices, without using any labeled data
from that slice. This enables us to identify domain-specific confidence
thresholds above which the LM's predictions can be trusted, and below which it
should abstain. Experiments show that our few-shot recalibrator consistently
outperforms existing calibration methods, for instance improving calibration
error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identification and Uses of Deep Learning Backbones via Pattern Mining <span class="chip">SDM24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Livanos, Ian Davidson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning is extensively used in many areas of data mining as a black-box
method with impressive results. However, understanding the core mechanism of
how deep learning makes predictions is a relatively understudied problem. Here
we explore the notion of identifying a backbone of deep learning for a given
group of instances. A group here can be instances of the same class or even
misclassified instances of the same class. We view each instance for a given
group as activating a subset of neurons and attempt to find a subgraph of
neurons associated with a given concept/group. We formulate this problem as a
set cover style problem and show it is intractable and presents a highly
constrained integer linear programming (ILP) formulation. As an alternative, we
explore a coverage-based heuristic approach related to pattern mining, and show
it converges to a Pareto equilibrium point of the ILP formulation.
Experimentally we explore these backbones to identify mistakes and improve
performance, explanation, and visualization. We demonstrate application-based
results using several challenging data sets, including Bird Audio Detection
(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic
MNIST data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures, published SIAM SDM24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSF-GAN: DownStream Feedback Generative Adversarial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oriel Perets, Nadav Rappoport
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utility and privacy are two crucial measurements of the quality of synthetic
tabular data. While significant advancements have been made in privacy
measures, generating synthetic samples with high utility remains challenging.
To enhance the utility of synthetic samples, we propose a novel architecture
called the DownStream Feedback Generative Adversarial Network (DSF-GAN). This
approach incorporates feedback from a downstream prediction model during
training to augment the generator's loss function with valuable information.
Thus, DSF-GAN utilizes a downstream prediction task to enhance the utility of
synthetic samples. To evaluate our method, we tested it using two popular
datasets. Our experiments demonstrate improved model performance when training
on synthetic samples generated by DSF-GAN, compared to those generated by the
same GAN architecture without feedback. The evaluation was conducted on the
same validation set comprising real samples. All code and datasets used in this
research will be made openly available for ease of reproduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generative Class Incremental Learning Performance with Model
  Forgetting Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taro Togo, Ren Togo, Keisuke Maeda, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a novel approach to Generative Class Incremental Learning
(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing
class information for better adaptation to streaming data. GCIL is one of the
hot topics in the field of computer vision, and this is considered one of the
crucial tasks in society, specifically the continual learning of generative
models. The ability to forget is a crucial brain function that facilitates
continual learning by selectively discarding less relevant information for
humans. However, in the field of machine learning models, the concept of
intentionally forgetting has not been extensively investigated. In this study
we aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,
thereby examining their impact on the models' ability to learn in continual
learning. Through our experiments, we have found that integrating the
forgetting mechanisms significantly enhances the models' performance in
acquiring new knowledge, underscoring the positive role that strategic
forgetting plays in the process of continual learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulating Neural Path Planners via Slight Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Xiong, Suresh Jagannathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven neural path planners are attracting increasing interest in the
robotics community. However, their neural network components typically come as
black boxes, obscuring their underlying decision-making processes. Their
black-box nature exposes them to the risk of being compromised via the
insertion of hidden malicious behaviors. For example, an attacker may hide
behaviors that, when triggered, hijack a delivery robot by guiding it to a
specific (albeit wrong) destination, trapping it in a predefined region, or
inducing unnecessary energy expenditure by causing the robot to repeatedly
circle a region. In this paper, we propose a novel approach to specify and
inject a range of hidden malicious behaviors, known as backdoors, into neural
path planners. Our approach provides a concise but flexible way to define these
behaviors, and we show that hidden behaviors can be triggered by slight
perturbations (e.g., inserting a tiny unnoticeable object), that can
nonetheless significantly compromise their integrity. We also discuss potential
techniques to identify these backdoors aimed at alleviating such risks. We
demonstrate our approach on both sampling-based and search-based neural path
planners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwu Zhong, Zi-Yuan Hu, Michael R. Lyu, Liwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual representation learning has been a cornerstone in computer vision,
evolving from supervised learning with human-annotated labels to aligning
image-text pairs from the Internet. Despite recent advancements in multi-modal
large language models (MLLMs), the visual representations they rely on, such as
CLIP embeddings, often lack access to external world knowledge critical for
real-world visual reasoning. In this work, we propose Visual Table, a novel
visual representation tailored for MLLMs. It provides hierarchical text
descriptions of holistic visual scenes, consisting of a scene description and
multiple object-centric descriptions that encompass categories, attributes, and
knowledge at instance level. We further develop a scalable generator for visual
table generation and train it on small-scale annotations from GPT4V. Extensive
evaluations demonstrate that, with generated visual tables as additional visual
representations, our model can consistently outperform the state-of-the-art
(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone
visual representations, our model can closely match or even beat the SOTA MLLMs
that are built on CLIP visual embeddings. Our code is available at
https://github.com/LaVi-Lab/Visual-Table.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/LaVi-Lab/Visual-Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Conversational Question Answering with Fine-Grained
  Retrieval-Augmentation and Self-Check 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie Zhou, Liang He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) aims to generate more reliable and
accurate responses, by augmenting large language models (LLMs) with the
external vast and dynamic knowledge. Most previous work focuses on using RAG
for single-round question answering, while how to adapt RAG to the complex
conversational setting wherein the question is interdependent on the preceding
context is not well studied. In this paper, we propose a conversation-level RAG
approach, which incorporates fine-grained retrieval augmentation and self-check
for conversational question answering (CQA). In particular, our approach
consists of three components, namely conversational question refiner,
fine-grained retriever and self-check based response generator, which work
collaboratively for question understanding and relevant information acquisition
in conversational settings. Extensive experiments demonstrate the great
advantages of our approach over the state-of-the-art baselines. Moreover, we
also release a Chinese CQA dataset with new features including reformulated
question, extracted keyword, retrieved paragraphs and their helpfulness, which
facilitates further researches in RAG enhanced CQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,
  Reconstruction, and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruikai Cui, Weizhe Liu, Weixuan Sun, Senbo Wang, Taizhang Shang, Yang Li, Xibin Song, Han Yan, Zhennan Wu, Shenzhou Chen, Hongdong Li, Pan Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D shape generation aims to produce innovative 3D content adhering to
specific conditions and constraints. Existing methods often decompose 3D shapes
into a sequence of localized components, treating each element in isolation
without considering spatial consistency. As a result, these approaches exhibit
limited versatility in 3D data representation and shape generation, hindering
their ability to generate highly diverse 3D shapes that comply with the
specified constraints. In this paper, we introduce a novel spatial-aware 3D
shape generation framework that leverages 2D plane representations for enhanced
3D shape modeling. To ensure spatial coherence and reduce memory usage, we
incorporate a hybrid shape representation technique that directly learns a
continuous signed distance field representation of the 3D shape using
orthogonal 2D planes. Additionally, we meticulously enforce spatial
correspondences across distinct planes using a transformer-based autoencoder
structure, promoting the preservation of spatial relationships in the generated
3D shapes. This yields an algorithm that consistently outperforms
state-of-the-art 3D shape generation methods on various tasks, including
unconditional shape generation, multi-modal shape completion, single-view
reconstruction, and text-to-shape synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Need Consultants for Reasoning: Becoming an Expert
  in a Complex Human System Through Behavior <span class="highlight-title">Simulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuwen Wang, Shirong Zeng, Cheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), in conjunction with various reasoning
reinforcement methodologies, have demonstrated remarkable capabilities
comparable to humans in fields such as mathematics, law, coding, common sense,
and world knowledge. In this paper, we delve into the reasoning abilities of
LLMs within complex human systems. We propose a novel reasoning framework,
termed ``Mosaic Expert Observation Wall'' (MEOW) exploiting
generative-agents-based simulation technique. In the MEOW framework, simulated
data are utilized to train an expert model concentrating ``experience'' about a
specific task in each independent time of simulation. It is the accumulated
``experience'' through the simulation that makes for an expert on a task in a
complex human system. We conduct the experiments within a communication game
that mirrors real-world security scenarios. The results indicate that our
proposed methodology can cooperate with existing methodologies to enhance the
reasoning abilities of LLMs in complex human systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Framework for Payload Malware Detection and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Stein, Arash Mahyari, Guillermo Francia III, Eman El-Sheikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious cyber threats become more sophisticated in breaching computer
networks, the need for effective intrusion detection systems (IDSs) becomes
crucial. Techniques such as Deep Packet Inspection (DPI) have been introduced
to allow IDSs analyze the content of network packets, providing more context
for identifying potential threats. IDSs traditionally rely on using
anomaly-based and signature-based detection techniques to detect unrecognized
and suspicious activity. Deep learning techniques have shown great potential in
DPI for IDSs due to their efficiency in learning intricate patterns from the
packet content being transmitted through the network. In this paper, we propose
a revolutionary DPI algorithm based on transformers adapted for the purpose of
detecting malicious traffic with a classifier head. Transformers learn the
complex content of sequence data and generalize them well to similar scenarios
thanks to their self-attention mechanism. Our proposed method uses the raw
payload bytes that represent the packet contents and is deployed as
man-in-the-middle. The payload bytes are used to detect malicious packets and
classify their types. Experimental results on the UNSW-NB15 and CIC-IOT23
datasets demonstrate that our transformer-based model is effective in
distinguishing malicious from benign traffic in the test dataset, attaining an
average accuracy of 79\% using binary classification and 72\% on the
multi-classification experiment, both using solely payload bytes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Two-Dimensional to Three-Dimensional Environment with Q-Learning:
  Modeling Autonomous Navigation with <span class="highlight-title">Reinforcement Learning</span> and no Libraries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ergon Cugler de Moraes Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) algorithms have become indispensable tools in
artificial intelligence, empowering agents to acquire optimal decision-making
policies through interactions with their environment and feedback mechanisms.
This study explores the performance of RL agents in both two-dimensional (2D)
and three-dimensional (3D) environments, aiming to research the dynamics of
learning across different spatial dimensions. A key aspect of this
investigation is the absence of pre-made libraries for learning, with the
algorithm developed exclusively through computational mathematics. The
methodological framework centers on RL principles, employing a Q-learning agent
class and distinct environment classes tailored to each spatial dimension. The
research aims to address the question: How do reinforcement learning agents
adapt and perform in environments of varying spatial dimensions, particularly
in 2D and 3D settings? Through empirical analysis, the study evaluates agents'
learning trajectories and adaptation processes, revealing insights into the
efficacy of RL algorithms in navigating complex, multi-dimensional spaces.
Reflections on the findings prompt considerations for future research,
particularly in understanding the dynamics of learning in higher-dimensional
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Fuzzy String Matching in Political
  Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fuzzy string matching remains a key issue when political scientists combine
data from different sources. Existing matching methods invariably rely on
string distances, such as Levenshtein distance and cosine similarity. As such,
they are inherently incapable of matching strings that refer to the same entity
with different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and
''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In
this letter, we propose to use large language models to entirely sidestep this
problem in an easy and intuitive manner. Extensive experiments show that our
proposed methods can improve the state of the art by as much as 39% in terms of
average precision while being substantially easier and more intuitive to use by
political scientists. Moreover, our results are robust against various
temperatures. We further note that enhanced prompting can lead to additional
performance improvements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preference-Based Planning in Stochastic Environments: From
  Partially-Ordered Temporal Goals to Most Preferred Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hazhar Rahmani, Abhishek N. Kulkarni, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human preferences are not always represented via complete linear orders: It
is natural to employ partially-ordered preferences for expressing incomparable
outcomes. In this work, we consider decision-making and probabilistic planning
in stochastic systems modeled as Markov decision processes (MDPs), given a
partially ordered preference over a set of temporally extended goals.
Specifically, each temporally extended goal is expressed using a formula in
Linear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially
ordered preference, we introduce order theory to map a preference over temporal
goals to a preference over policies for the MDP. Accordingly, a most preferred
policy under a stochastic ordering induces a stochastic nondominated
probability distribution over the finite paths in the MDP. To synthesize a most
preferred policy, our technical approach includes two key steps. In the first
step, we develop a procedure to transform a partially ordered preference over
temporal goals into a computational model, called preference automaton, which
is a semi-automaton with a partial order over acceptance conditions. In the
second step, we prove that finding a most preferred policy is equivalent to
computing a Pareto-optimal policy in a multi-objective MDP that is constructed
from the original MDP, the preference automaton, and the chosen stochastic
ordering relation. Throughout the paper, we employ running examples to
illustrate the proposed preference specification and solution approaches. We
demonstrate the efficacy of our algorithm using these examples, providing
detailed analysis, and then discuss several potential future directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2209.12267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long and Short-Term Constraints Driven Safe <span class="highlight-title">Reinforcement Learning</span> for
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemin Hu, Pan Chen, Yijun Wen, Bo Tang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has been widely used in decision-making tasks,
but it cannot guarantee the agent's safety in the training process due to the
requirements of interaction with the environment, which seriously limits its
industrial applications such as autonomous driving. Safe RL methods are
developed to handle this issue by constraining the expected safety violation
costs as a training objective, but they still permit unsafe state occurrence,
which is unacceptable in autonomous driving tasks. Moreover, these methods are
difficult to achieve a balance between the cost and return expectations, which
leads to learning performance degradation for the algorithms. In this paper, we
propose a novel algorithm based on the long and short-term constraints (LSTC)
for safe RL. The short-term constraint aims to guarantee the short-term state
safety that the vehicle explores, while the long-term constraint ensures the
overall safety of the vehicle throughout the decision-making process. In
addition, we develop a safe RL method with dual-constraint optimization based
on the Lagrange multiplier to optimize the training process for end-to-end
autonomous driving. Comprehensive experiments were conducted on the MetaDrive
simulator. Experimental results demonstrate that the proposed method achieves
higher safety in continuous state and action tasks, and exhibits higher
exploration performance in long-distance decision-making tasks compared with
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Evolutionary Network Architecture Search Framework with <span class="highlight-title">Adaptive</span>
  Multimodal Fusion for Hand Gesture Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhang Xia, Shihao Song, Zhanglu Hou, Junwen Xu, Juan Zou, Yuan Liu, Shengxiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hand gesture recognition (HGR) based on multimodal data has attracted
considerable attention owing to its great potential in applications. Various
manually designed multimodal deep networks have performed well in multimodal
HGR (MHGR), but most of existing algorithms require a lot of expert experience
and time-consuming manual trials. To address these issues, we propose an
evolutionary network architecture search framework with the adaptive multimodel
fusion (AMF-ENAS). Specifically, we design an encoding space that
simultaneously considers fusion positions and ratios of the multimodal data,
allowing for the automatic construction of multimodal networks with different
architectures through decoding. Additionally, we consider three input streams
corresponding to intra-modal surface electromyography (sEMG), intra-modal
accelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to
various datasets, the ENAS framework is designed to automatically search a MHGR
network with appropriate fusion positions and ratios. To the best of our
knowledge, this is the first time that ENAS has been utilized in MHGR to tackle
issues related to the fusion position and ratio of multimodal data.
Experimental results demonstrate that AMF-ENAS achieves state-of-the-art
performance on the Ninapro DB2, DB3, and DB7 datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Privacy Protection Capabilities of Chinese Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Yang, Xiaowen Huang, Jitao Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), renowned for their impressive capabilities in
various tasks, have significantly advanced artificial intelligence. Yet, these
advancements have raised growing concerns about privacy and security
implications. To address these issues and explain the risks inherent in these
models, we have devised a three-tiered progressive framework tailored for
evaluating privacy in language systems. This framework consists of
progressively complex and in-depth privacy test tasks at each tier. Our primary
objective is to comprehensively evaluate the sensitivity of large language
models to private information, examining how effectively they discern, manage,
and safeguard sensitive data in diverse scenarios. This systematic evaluation
helps us understand the degree to which these models comply with privacy
protection guidelines and the effectiveness of their inherent safeguards
against privacy breaches. Our observations indicate that existing Chinese large
language models universally show privacy protection shortcomings. It seems that
at the moment this widespread issue is unavoidable and may pose corresponding
privacy risks in applications based on these models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nisha Pillai, Athish Ram Das, Moses Ayoola, Ganga Gireesan, Bindu Nanduri, Mahalingam Ramkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) techniques are widely applied in the life
sciences. However, applying innovative AI techniques to understand and
deconvolute biological complexity is hindered by the learning curve for life
science scientists to understand and use computing languages. An open-source,
user-friendly interface for AI models, that does not require programming skills
to analyze complex biological data will be extremely valuable to the
bioinformatics community. With easy access to different sequencing technologies
and increased interest in different 'omics' studies, the number of biological
datasets being generated has increased and analyzing these high-throughput
datasets is computationally demanding. The majority of AI libraries today
require advanced programming skills as well as machine learning, data
preprocessing, and visualization skills. In this research, we propose a
web-based end-to-end pipeline that is capable of preprocessing, training,
evaluating, and visualizing machine learning (ML) models without manual
intervention or coding expertise. By integrating traditional machine learning
and deep neural network models with visualizations, our library assists in
recognizing, classifying, clustering, and predicting a wide range of
multi-modal, multi-sensor datasets, including images, languages, and
one-dimensional numerical data, for drug discovery, pathogen classification,
and medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024 7th International Conference on Information and Computer
  Technologies (ICICT)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking Beyond What You See: An Empirical Analysis on Subgroup
  Intersectional Fairness for Multi-label Chest X-ray Classification Using
  Social Determinants of Racial Health Inequities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dana Moukheiber, Saurabh Mahindre, Lama Moukheiber, Mira Moukheiber, Mingchen Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There has been significant progress in implementing deep learning models in
disease diagnosis using chest X- rays. Despite these advancements, inherent
biases in these models can lead to disparities in prediction accuracy across
protected groups. In this study, we propose a framework to achieve accurate
diagnostic outcomes and ensure fairness across intersectional groups in
high-dimensional chest X- ray multi-label classification. Transcending
traditional protected attributes, we consider complex interactions within
social determinants, enabling a more granular benchmark and evaluation of
fairness. We present a simple and robust method that involves retraining the
last classification layer of pre-trained models using a balanced dataset across
groups. Additionally, we account for fairness constraints and integrate
class-balanced fine-tuning for multi-label settings. The evaluation of our
method on the MIMIC-CXR dataset demonstrates that our framework achieves an
optimal tradeoff between accuracy and fairness compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV CVAMD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly
  Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Kaichen Zhou, jinhong Chen, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous assembly in robotics and 3D vision presents significant
challenges, particularly in ensuring assembly correctness. Presently,
predominant methods such as MEPNet focus on assembling components based on
manually provided images. However, these approaches often fall short in
achieving satisfactory results for tasks requiring long-term planning.
Concurrently, we observe that integrating a self-correction module can
partially alleviate such issues. Motivated by this concern, we introduce the
single-step assembly error correction task, which involves identifying and
rectifying misassembled components. To support research in this area, we
present the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising
manual images for assembly steps and instances of assembly failures.
Additionally, we propose the Self-Correct Assembly Network (SCANet), a novel
method to address this task. SCANet treats assembled components as queries,
determining their correctness in manual images and providing corrections when
necessary. Finally, we utilize SCANet to correct the assembly results of
MEPNet. Experimental results demonstrate that SCANet can identify and correct
MEPNet's misassembled results, significantly improving the correctness of
assembly. Our code and dataset are available at
https://github.com/Yaser-wyx/SCANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Models Appreciate Document Aesthetics? An Exploration of
  Legibility and Layout Quality in Relation to <span class="highlight-title">Prediction</span> Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsiu-Wei Yang, Abhinav Agrawal, Pavlos Fragkogiannis, Shubham Nitin Mulay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-designed document communicates not only through its words but also
through its visual eloquence. Authors utilize aesthetic elements such as
colors, fonts, graphics, and layouts to shape the perception of information.
Thoughtful document design, informed by psychological insights, enhances both
the visual appeal and the comprehension of the content. While state-of-the-art
document AI models demonstrate the benefits of incorporating layout and image
data, it remains unclear whether the nuances of document aesthetics are
effectively captured. To bridge the gap between human cognition and AI
interpretation of aesthetic elements, we formulated hypotheses concerning AI
behavior in document understanding tasks, specifically anchored in document
design principles. With a focus on legibility and layout quality, we tested
four aspects of aesthetic effects: noise, font-size contrast, alignment, and
complexity, on model confidence using correlational analysis. The results and
observations highlight the value of model analysis rooted in document design
theories. Our work serves as a trailhead for further studies and we advocate
for continued research in this topic to deepen our understanding of how AI
interprets document aesthetics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanisms of non-factual hallucinations in language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Meng Cao, Jackie Chi Kit Cheung, Yue Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art language models (LMs) sometimes generate non-factual
hallucinations that misalign with world knowledge. Despite extensive efforts to
detect and mitigate hallucinations, understanding their internal mechanisms
remains elusive. Our study investigates the mechanistic causes of
hallucination, specifically non-factual ones where the LM incorrectly predicts
object attributes in response to subject-relation queries. With causal
mediation analysis and embedding space projection, we identify two general
mechanistic causes of hallucinations shared across LMs of various scales and
designs: 1) insufficient subject attribute knowledge in lower layer MLPs, and
2) failing to select the correct object attribute in upper layer attention
heads and MLPs. These two mechanisms exhibit varying degrees of subject-object
association, predictive uncertainty and perturbation robustness. Additionally,
we scrutinize LM pre-training checkpoints, revealing distinct learning dynamics
for the two mechanistic causes of hallucinations. We also highlight how
attribution features from our causal analysis can effectively construct
hallucination detectors. Our work proposes a mechanistic understanding of LM
factual errors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Prajwal Singhania, Aditya K. Ranjan, Zack Sating, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large communication costs are a critical bottleneck in training
state-of-the-art neural networks on distributed systems. This paper introduces
AxoNN, a novel four-dimensional (4D) parallelization approach, inspired by
Agarwal's algorithm for matrix multiplication, for parallelizing tensor
computations in deep learning, AxoNN employs two key strategies to minimize
communication overhead. First, we optimize communication by overlapping
expensive collective operations (reduce-scatter, all-gather, all-reduce) with
computations. Our experiments with a 20-billion parameter transformer model
demonstrate that these optimizations deliver nearly 53\% improvement. Second,
we present an analytical model to assist users in identifying
communication-minimizing configurations within the vast search space defined by
our 4D algorithm. This model empowers practitioners by simplifying the tuning
process for their specific training workloads. When training an 80-billion
parameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a
state-of-the-art framework, by a significant 26%. Additionally, it achieves 57%
of the theoretical peak FLOP/s.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting to Machine Supervision: Annotation-Efficient Semi and
  Self-Supervised Learning for Automatic Medical Image Segmentation and
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10319v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10319v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Singh, Raviteja Chukkapalli, Shravan Chaudhari, Luoyao Chen, Mei Chen, Jinqian Pan, Craig Smuda, Jacopo Cirrone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in clinical treatment are increasingly constrained by the
limitations of supervised learning techniques, which depend heavily on large
volumes of annotated data. The annotation process is not only costly but also
demands substantial time from clinical specialists. Addressing this issue, we
introduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)
pipeline, a novel approach that leverages advancements in self-supervised and
semi-supervised learning. These techniques engage in auxiliary tasks that do
not require labeling, thus simplifying the scaling of machine supervision
compared to fully-supervised methods. Our study benchmarks these techniques on
three distinct medical imaging datasets to evaluate their effectiveness in
classification and segmentation tasks. Notably, we observed that self
supervised learning significantly surpassed the performance of supervised
methods in the classification of all evaluated datasets. Remarkably, the
semi-supervised approach demonstrated superior outcomes in segmentation,
outperforming fully-supervised methods while using 50% fewer labels across all
datasets. In line with our commitment to contributing to the scientific
community, we have made the S4MI code openly accessible, allowing for broader
application and further development of these methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Seventeen pages (incl. references), five figures, and one table.
  (Under Review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LLM-based Agent</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization Bounds: Perspectives from Information Theory and
  PAC-Bayes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fredrik Hellström, Giuseppe Durisi, Benjamin Guedj, Maxim Raginsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental question in theoretical machine learning is generalization.
Over the past decades, the PAC-Bayesian approach has been established as a
flexible framework to address the generalization capabilities of machine
learning algorithms, and design new ones. Recently, it has garnered increased
interest due to its potential applicability for a variety of learning
algorithms, including deep neural networks. In parallel, an
information-theoretic view of generalization has developed, wherein the
relation between generalization and various information measures has been
established. This framework is intimately connected to the PAC-Bayesian
approach, and a number of results have been independently discovered in both
strands. In this monograph, we highlight this strong connection and present a
unified treatment of PAC-Bayesian and information-theoretic generalization
bounds. We present techniques and results that the two perspectives have in
common, and discuss the approaches and interpretations that differ. In
particular, we demonstrate how many proofs in the area share a modular
structure, through which the underlying ideas can be intuited. We pay special
attention to the conditional mutual information (CMI) framework; analytical
studies of the information complexity of learning algorithms; and the
application of the proposed methods to deep learning. This monograph is
intended to provide a comprehensive introduction to information-theoretic
generalization bounds and their connection to PAC-Bayes, serving as a
foundation from which the most recent developments are accessible. It is aimed
broadly towards researchers with an interest in generalization and theoretical
machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>228 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Data Consistency with Diffusion Purification for Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.06054v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.06054v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Soo Min Kwon, Ismail R. Alkhouri, Saiprasad Ravishankar, Qing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained traction as a powerful class of deep
generative priors, excelling in a wide range of image restoration tasks due to
their exceptional ability to model data distributions. To solve image
restoration problems, many existing techniques achieve data consistency by
incorporating additional likelihood gradient steps into the reverse sampling
process of diffusion models. However, the additional gradient steps pose a
challenge for real-world practical applications as they incur a large
computational overhead, thereby increasing inference time. They also present
additional difficulties when using accelerated diffusion model samplers, as the
number of data consistency steps is limited by the number of reverse sampling
steps. In this work, we propose a novel diffusion-based image restoration
solver that addresses these issues by decoupling the reverse process from the
data consistency steps. Our method involves alternating between a
reconstruction phase to maintain data consistency and a refinement phase that
enforces the prior via diffusion purification. Our approach demonstrates
versatility, making it highly adaptable for efficient problem-solving in latent
space. Additionally, it reduces the necessity for numerous sampling steps
through the integration of consistency models. The efficacy of our approach is
validated through comprehensive experiments across various image restoration
tasks, including image denoising, deblurring, inpainting, and super-resolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedSN: A Novel Federated Learning Framework over LEO Satellite Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01483v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01483v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a large number of Low Earth Orbit (LEO) satellites have been
launched and deployed successfully in space by commercial companies, such as
SpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve
not only for communication but also for various machine learning applications,
such as space modulation recognition, remote sensing image classification, etc.
However, the ground station (GS) may be incapable of downloading such a large
volume of raw sensing data for centralized model training due to the limited
contact time with LEO satellites (e.g. 5 minutes). Therefore, federated
learning (FL) has emerged as the promising solution to address this problem via
on-device training. Unfortunately, to enable FL on LEO satellites, we still
face three critical challenges that are i) heterogeneous computing and memory
capabilities, ii) limited uplink rate, and iii) model staleness. To this end,
we propose FedSN as a general FL framework to tackle the above challenges, and
fully explore data diversity on LEO satellites. Specifically, we first present
a novel sub-structure scheme to enable heterogeneous local model training
considering different computing, memory, and communication constraints on LEO
satellites. Additionally, we propose a pseudo-synchronous model aggregation
strategy to dynamically schedule model aggregation for compensating model
staleness. To further demonstrate the effectiveness of the FedSN, we evaluate
it using space modulation recognition and remote sensing image classification
tasks by leveraging the data from real-world satellite networks. Extensive
experimental results demonstrate that FedSN framework achieves higher accuracy,
lower computing, and communication overhead than the state-of-the-art
benchmarks and the effectiveness of each components in FedSN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear Control Allocation: A Learning Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.06180v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.06180v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hafiz Zeeshan Iqbal Khan, Surrayya Mobeen, Jahanzeb Rajput, Jamshed Riaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern aircraft are designed with redundant control effectors to cater for
fault tolerance and maneuverability requirements. This leads to aircraft being
over-actuated and requires control allocation schemes to distribute the control
commands among control effectors. Traditionally, optimization-based control
allocation schemes are used; however, for nonlinear allocation problems, these
methods require large computational resources. In this work, an artificial
neural network (ANN) based nonlinear control allocation scheme is proposed. The
proposed scheme is composed of learning the inverse of the control
effectiveness map through ANN, and then implementing it as an allocator instead
of solving an online optimization problem. Stability conditions are presented
for closed-loop systems incorporating the allocator, and computational
challenges are explored with piece-wise linear effectiveness functions and
ANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is
compared with a standard quadratic programming-based method for control
allocation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE Conference on Decision and Control (CDC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03100v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03100v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqian Ju, Yuancheng Wang, Kai Shen, Xu Tan, Detai Xin, Dongchao Yang, Yanqing Liu, Yichong Leng, Kaitao Song, Siliang Tang, Zhizheng Wu, Tao Qin, Xiang-Yang Li, Wei Ye, Shikun Zhang, Jiang Bian, Lei He, Jinyu Li, Sheng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent large-scale text-to-speech (TTS) models have achieved
significant progress, they still fall short in speech quality, similarity, and
prosody. Considering speech intricately encompasses various attributes (e.g.,
content, prosody, timbre, and acoustic details) that pose significant
challenges for generation, a natural idea is to factorize speech into
individual subspaces representing different attributes and generate them
individually. Motivated by it, we propose NaturalSpeech 3, a TTS system with
novel factorized diffusion models to generate natural speech in a zero-shot
way. Specifically, 1) we design a neural codec with factorized vector
quantization (FVQ) to disentangle speech waveform into subspaces of content,
prosody, timbre, and acoustic details; 2) we propose a factorized diffusion
model to generate attributes in each subspace following its corresponding
prompt. With this factorization design, NaturalSpeech 3 can effectively and
efficiently model intricate speech with disentangled subspaces in a
divide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the
state-of-the-art TTS systems on quality, similarity, prosody, and
intelligibility, and achieves on-par quality with human recordings.
Furthermore, we achieve better performance by scaling to 1B parameters and 200K
hours of training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Achieving human-level quality and naturalness on multi-speaker
  datasets (e.g., LibriSpeech) in a zero-shot way</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and
  Ethics) Evaluation: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev, Weizheng Wang, Lewis Nkenyereye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is another large language model (LLM) vastly available for the
consumers on their devices but due to its performance and ability to converse
effectively, it has gained a huge popularity amongst research as well as
industrial community. Recently, many studies have been published to show the
effectiveness, efficiency, integration, and sentiments of chatGPT and other
LLMs. In contrast, this study focuses on the important aspects that are mostly
overlooked, i.e. sustainability, privacy, digital divide, and ethics and
suggests that not only chatGPT but every subsequent entry in the category of
conversational bots should undergo Sustainability, PrivAcy, Digital divide, and
Ethics (SPADE) evaluation. This paper discusses in detail the issues and
concerns raised over chatGPT in line with aforementioned characteristics. We
also discuss the recent EU AI Act briefly in accordance with the SPADE
evaluation. We support our hypothesis by some preliminary data collection and
visualizations along with hypothesized facts. We also suggest mitigations and
recommendations for each of the concerns. Furthermore, we also suggest some
policies and recommendations for EU AI policy act concerning ethics, digital
divide, and sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incorporating simulated spatial context information improves the
  effectiveness of contrastive learning models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15120v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15120v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhen Zhu, James Z. Wang, Wonseuk Lee, Brad Wyble
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Approximation with Delayed Updates: Finite-Time Rates under
  Markovian Sampling <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Adibi, Nicolo Dal Fabbro, Luca Schenato, Sanjeev Kulkarni, H. Vincent Poor, George J. Pappas, Hamed Hassani, Aritra Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by applications in large-scale and multi-agent reinforcement
learning, we study the non-asymptotic performance of stochastic approximation
(SA) schemes with delayed updates under Markovian sampling. While the effect of
delays has been extensively studied for optimization, the manner in which they
interact with the underlying Markov process to shape the finite-time
performance of SA remains poorly understood. In this context, our first main
contribution is to show that under time-varying bounded delays, the delayed SA
update rule guarantees exponentially fast convergence of the \emph{last
iterate} to a ball around the SA operator's fixed point. Notably, our bound is
\emph{tight} in its dependence on both the maximum delay $\tau_{max}$, and the
mixing time $\tau_{mix}$. To achieve this tight bound, we develop a novel
inductive proof technique that, unlike various existing delayed-optimization
analyses, relies on establishing uniform boundedness of the iterates. As such,
our proof may be of independent interest. Next, to mitigate the impact of the
maximum delay on the convergence rate, we provide the first finite-time
analysis of a delay-adaptive SA scheme under Markovian sampling. In particular,
we show that the exponent of convergence of this scheme gets scaled down by
$\tau_{avg}$, as opposed to $\tau_{max}$ for the vanilla delayed SA rule; here,
$\tau_{avg}$ denotes the average delay across all iterations. Moreover, the
adaptive scheme requires no prior knowledge of the delay sequence for step-size
tuning. Our theoretical findings shed light on the finite-time effects of
delays for a broad class of algorithms, including TD learning, Q-learning, and
stochastic gradient descent under Markovian sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024!</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Adaptive</span> Negative Evidential Deep Learning for Open-set Semi-supervised
  Learning <span class="chip">AAAI2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12091v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12091v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yu, Danruo Deng, Furui Liu, Yueming Jin, Qi Dou, Guangyong Chen, Pheng-Ann Heng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled
data and test data are from the same distribution. Open-set semi-supervised
learning (Open-set SSL) considers a more practical scenario, where unlabeled
data and test data contain new categories (outliers) not observed in labeled
data (inliers). Most previous works focused on outlier detection via binary
classifiers, which suffer from insufficient scalability and inability to
distinguish different types of uncertainty. In this paper, we propose a novel
framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these
limitations. Concretely, we first introduce evidential deep learning (EDL) as
an outlier detector to quantify different types of uncertainty, and design
different uncertainty metrics for self-training and inference. Furthermore, we
propose a novel adaptive negative optimization strategy, making EDL more
tailored to the unlabeled dataset containing both inliers and outliers. As
demonstrated empirically, our proposed method outperforms existing
state-of-the-art methods across four datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental
  Health Sensing Studies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Choube, Vedant Das Swain, Varun Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in mobile and wearable technologies have enabled the potential to
passively monitor a person's mental, behavioral, and affective health. These
approaches typically rely on longitudinal collection of self-reported outcomes,
e.g., depression, stress, and anxiety, to train machine learning (ML) models.
However, the need to continuously self-report adds a significant burden on the
participants, often resulting in attrition, missing labels, or insincere
responses. In this work, we introduce the Scale Scores Simulation using Mental
Models (SeSaMe) framework to alleviate participants' burden in digital mental
health studies. By leveraging pre-trained large language models (LLMs), SeSaMe
enables the simulation of participants' responses on psychological scales. In
SeSaMe, researchers can prompt LLMs with information on participants' internal
behavioral dispositions, enabling LLMs to construct mental models of
participants to simulate their responses on psychological scales. We
demonstrate an application of SeSaMe, where we use GPT-4 to simulate responses
on one scale using responses from another as behavioral information. We also
evaluate the alignment between human and SeSaMe-simulated responses to
psychological scales. Then, we present experiments to inspect the utility of
SeSaMe-simulated responses as ground truth in training ML models by replicating
established depression and anxiety screening tasks from a previous study. Our
results indicate SeSaMe to be a promising approach, but its alignment may vary
across scales and specific prediction objectives. We also observed that model
performance with simulated data was on par with using the real data for
training in most evaluation scenarios. We conclude by discussing the potential
implications of SeSaMe in addressing some challenges researchers face with
ground-truth collection in passive sensing studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Byzantine-resilient Federated Learning With Adaptivity to Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyuan Zuo, Xingrun Yan, Rongfei Fan, Han Hu, Hangguan Shan, Tony Q. S. Quek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deals with federated learning (FL) in the presence of malicious
Byzantine attacks and data heterogeneity. A novel Robust Average Gradient
Algorithm (RAGA) is proposed, which leverages the geometric median for
aggregation and can freely select the round number for local updating.
Different from most existing resilient approaches, which perform convergence
analysis based on strongly-convex loss function or homogeneously distributed
dataset, we conduct convergence analysis for not only strongly-convex but also
non-convex loss function over heterogeneous dataset. According to our
theoretical analysis, as long as the fraction of dataset from malicious users
is less than half, RAGA can achieve convergence at rate
$\mathcal{O}({1}/{T^{2/3- \delta}})$ where $T$ is the iteration number and
$\delta \in (0, 2/3)$ for non-convex loss function, and at linear rate for
strongly-convex loss function. Moreover, stationary point or global optimal
solution is proved to obtainable as data heterogeneity vanishes. Experimental
results corroborate the robustness of RAGA to Byzantine attacks and verifies
the advantage of RAGA over baselines on convergence performance under various
intensity of Byzantine attacks, for heterogeneous dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demystifying Misconceptions in Social Bots Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on social bots aims at advancing knowledge and providing solutions
to one of the most debated forms of online manipulation. Yet, social bot
research is plagued by widespread biases, hyped results, and misconceptions
that set the stage for ambiguities, unrealistic expectations, and seemingly
irreconcilable findings. Overcoming such issues is instrumental towards
ensuring reliable solutions and reaffirming the validity of the scientific
method. In this contribution, we review some recent results in social bots
research, highlighting and revising factual errors as well as methodological
and conceptual biases. More importantly, we demystify common misconceptions,
addressing fundamental points on how social bots research is discussed. Our
analysis surfaces the need to discuss research about online disinformation and
manipulation in a rigorous, unbiased, and responsible way. This article
bolsters such effort by identifying and refuting common fallacious arguments
used by both proponents and opponents of social bots research, as well as
providing directions toward sound methodologies for future research in the
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMachining: Online <span class="highlight-title">Prediction</span> of Machining Errors of Lathe Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang-Li Lu, Hwai-Jung Hsu, Che-Wei Chou, H. T. Kung, Chen-Hsin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe DeepMachining, a deep learning-based AI system for online
prediction of machining errors of lathe machine operations. We have built and
evaluated DeepMachining based on manufacturing data from factories.
Specifically, we first pretrain a deep learning model for a given lathe
machine's operations to learn the salient features of machining states. Then,
we fine-tune the pretrained model to adapt to specific machining tasks. We
demonstrate that DeepMachining achieves high prediction accuracy for multiple
tasks that involve different workpieces and cutting tools. To the best of our
knowledge, this work is one of the first factory experiments using pre-trained
deep-learning models to predict machining errors of lathe machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Guided Large Language Model for SQL Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13284v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13284v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Junnan Dong, Hao Chen, Wentao Li, Feiran Huang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating accurate Structured Querying Language (SQL) is a long-standing
problem, especially in matching users' semantic queries with structured
databases and then generating structured SQL. Existing models typically input
queries and database schemas into the LLM and rely on the LLM to perform
semantic-structure matching and generate structured SQL. However, such
solutions overlook the structural information within user queries and
databases, which can be utilized to enhance the generation of structured SQL.
This oversight can lead to inaccurate or unexecutable SQL generation. To fully
exploit the structure, we propose a structure-to-SQL framework, which leverages
the inherent structure information to improve the SQL generation of LLMs.
Specifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.
SGU-SQL first links user queries and databases in a structure-enhanced manner.
It then decomposes complicated linked structures with grammar trees to guide
the LLM to generate the SQL step by step. Extensive experiments on two
benchmark datasets illustrate that SGU-SQL can outperform sixteen SQL
generation baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Action Transformer with Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09459v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09459v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexey Staroverov, Egor Cherepanov, Dmitry Yudin, Alexey K. Kovalev, Aleksandr I. Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the use of transformers in offline reinforcement learning has
become a rapidly developing area. This is due to their ability to treat the
agent's trajectory in the environment as a sequence, thereby reducing the
policy learning problem to sequence modeling. In environments where the agent's
decisions depend on past events, it is essential to capture both the event
itself and the decision point in the context of the model. However, the
quadratic complexity of the attention mechanism limits the potential for
context expansion. One solution to this problem is to enhance transformers with
memory mechanisms. In this paper, we propose the Recurrent Action Transformer
with Memory (RATE) - a model that incorporates recurrent memory. To evaluate
our model, we conducted extensive experiments on both memory-intensive
environments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo
control environments. The results show that the use of memory can significantly
improve performance in memory-intensive environments while maintaining or
improving results in classic environments. We hope that our findings will
stimulate research on memory mechanisms for transformers applicable to offline
reinforcement learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Pre-Training of Time-Series Data for Unsupervised Fault
  Detection in Semiconductor Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sewoong Lee, JinKyou Choi, Min Su Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces TRACE-GPT, which stands for Time-seRies
Anomaly-detection with Convolutional Embedding and Generative Pre-trained
Transformers. TRACE-GPT is designed to pre-train univariate time-series sensor
data and detect faults on unlabeled datasets in semiconductor manufacturing. In
semiconductor industry, classifying abnormal time-series sensor data from
normal data is important because it is directly related to wafer defect.
However, small, unlabeled, and even mixed training data without enough
anomalies make classification tasks difficult. In this research, we capture
features of time-series data with temporal convolutional embedding and
Generative Pre-trained Transformer (GPT) to classify abnormal sequences from
normal sequences using cross entropy loss. We prove that our model shows better
performance than previous unsupervised models with both an open dataset, the
University of California Riverside (UCR) time-series classification archive,
and the process log of our Chemical Vapor Deposition (CVD) equipment. Our model
has the highest F1 score at Equal Error Rate (EER) across all datasets and is
only 0.026 below the supervised state-of-the-art baseline on the open dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacks, Defenses and Evaluations for LLM Conversation Safety: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are now commonplace in conversation
applications. However, their risks of misuse for generating harmful responses
have raised serious societal concerns and spurred recent research on LLM
conversation safety. Therefore, in this survey, we provide a comprehensive
overview of recent studies, covering three critical aspects of LLM conversation
safety: attacks, defenses, and evaluations. Our goal is to provide a structured
summary that enhances understanding of LLM conversation safety and encourages
further investigation into this important subject. For easy reference, we have
categorized all the studies mentioned in this survey according to our taxonomy,
available at: https://github.com/niconi19/LLM-conversation-safety.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Glinsky, Alexey Sokolsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ABScribe: Rapid Exploration & Organization of Multiple Writing
  Variations in Human-AI Co-Writing Tasks using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00117v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00117v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohi Reza, Nathan Laundry, Ilya Musabirov, Peter Dushniku, Zhi Yuan "Michael" Yu, Kashish Mittal, Tovi Grossman, Michael Liut, Anastasia Kuzminykh, Joseph Jay Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring alternative ideas by rewriting text is integral to the writing
process. State-of-the-art Large Language Models (LLMs) can simplify writing
variation generation. However, current interfaces pose challenges for
simultaneous consideration of multiple variations: creating new variations
without overwriting text can be difficult, and pasting them sequentially can
clutter documents, increasing workload and disrupting writers' flow. To tackle
this, we present ABScribe, an interface that supports rapid, yet visually
structured, exploration and organization of writing variations in human-AI
co-writing tasks. With ABScribe, users can swiftly modify variations using LLM
prompts, which are auto-converted into reusable buttons. Variations are stored
adjacently within text fields for rapid in-place comparisons using mouse-over
interactions on a popup toolbar. Our user study with 12 writers shows that
ABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances
user perceptions of the revision process (d = 2.41, p < 0.001) compared to a
popular baseline workflow, and provides insights into how writers explore
variations using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10365v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10365v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyu Tian, Hongxin Wei, Yiqun Wang, Lei Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial-label learning (PLL) is an important weakly supervised learning
problem, which allows each training example to have a candidate label set
instead of a single ground-truth label. Identification-based methods have been
widely explored to tackle label ambiguity issues in PLL, which regard the true
label as a latent variable to be identified. However, identifying the true
labels accurately and completely remains challenging, causing noise in pseudo
labels during model training. In this paper, we propose a new method called
CroSel, which leverages historical predictions from the model to identify true
labels for most training examples. First, we introduce a cross selection
strategy, which enables two deep models to select true labels of partially
labeled data for each other. Besides, we propose a novel consistency
regularization term called co-mix to avoid sample waste and tiny noise caused
by false selection. In this way, CroSel can pick out the true labels of most
examples with high precision. Extensive experiments demonstrate the superiority
of CroSel, which consistently outperforms previous state-of-the-art methods on
benchmark datasets. Additionally, our method achieves over 90\% accuracy and
quantity for selecting true labels on CIFAR-type datasets under various
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Challenging Common Paradigms in Multi-Task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cathrin Elich, Lukas Kirchdorfer, Jan M. Köhler, Lukas Schott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While multi-task learning (MTL) has gained significant attention in recent
years, its underlying mechanisms remain poorly understood. Recent methods did
not yield consistent performance improvements over single task learning (STL)
baselines, underscoring the importance of gaining more profound insights about
challenges specific to MTL. In our study, we challenge paradigms in MTL in the
context of STL: First, the impact of the choice of optimizer has only been
mildly investigated in MTL. We show the pivotal role of common STL tools such
as the Adam optimizer in MTL empirically in various experiments. To further
investigate Adam's effectiveness, we theoretical derive a partial loss-scale
invariance under mild assumptions. Second, the notion of gradient conflicts has
often been phrased as a specific problem in MTL. We delve into the role of
gradient conflicts in MTL and compare it to STL. For angular gradient alignment
we find no evidence that this is a unique problem in MTL. We emphasize
differences in gradient magnitude as the main distinguishing factor. Lastly, we
compare the transferability of features learned through MTL and STL on common
image corruptions, and find light evidence that MTL can lead to superior
transferability. Overall, we find surprising similarities between STL and MTL
suggesting to consider methods from both fields in a broader context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>-</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving a Real-World Package Delivery Routing Problem Using Quantum
  Annealers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eneko Osaba, Esther Villar-Rodriguez, Antón Asla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research focused on the conjunction between quantum computing and routing
problems has been very prolific in recent years. Most of the works revolve
around classical problems such as the Traveling Salesman Problem or the Vehicle
Routing Problem. Even though working on these problems is valuable, it is also
undeniable that their academic-oriented nature falls short of real-world
requirements. The main objective of this research is to present a solving
method for realistic instances, avoiding problem relaxations or technical
shortcuts. Instead, a quantum-classical hybrid solver has been developed,
coined Q4RPD, that considers a set of real constraints such as a heterogeneous
fleet of vehicles, priority deliveries, and capacities characterized by two
values: weight and dimensions of the packages. Q4RPD resorts to the Leap
Constrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the
application of Q4RPD, an experimentation composed of six different instances
has been conducted, aiming to serve as illustrative examples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures and 4 tables. Paper submitted for review in
  Scientific Reports</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Li, Mengyuan Liu, Hong Liu, Pichao Wang, Jialun Cai, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been successfully applied in the field of video-based 3D
human pose estimation. However, the high computational costs of these video
pose transformers (VPTs) make them impractical on resource-constrained devices.
In this paper, we present a plug-and-play pruning-and-recovering framework,
called Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose
estimation from videos. Our HoT begins with pruning pose tokens of redundant
frames and ends with recovering full-length tokens, resulting in a few pose
tokens in the intermediate transformer blocks and thus improving the model
efficiency. To effectively achieve this, we propose a token pruning cluster
(TPC) that dynamically selects a few representative tokens with high semantic
diversity while eliminating the redundancy of video frames. In addition, we
develop a token recovering attention (TRA) to restore the detailed
spatio-temporal information based on the selected tokens, thereby expanding the
network output to the original full-length temporal resolution for fast
inference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and
MPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and
estimation accuracy compared to the original VPT models. For instance, applying
to MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs
without sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,
respectively. Code and models are available at
https://github.com/NationalGAILab/HoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024, Open Sourced</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on
  Prompt-based Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xu, Wenjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning is a new language model training paradigm that adapts
the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes
the performance benchmarks across various natural language processing (NLP)
tasks. Instead of using a fixed prompt template to fine-tune the model, some
research demonstrates the effectiveness of searching for the prompt via
optimization. Such prompt optimization process of prompt-based learning on PLMs
also gives insight into generating adversarial prompts to mislead the model,
raising concerns about the adversarial vulnerability of this paradigm. Recent
studies have shown that universal adversarial triggers (UATs) can be generated
to alter not only the predictions of the target PLMs but also the prediction of
corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based
learning paradigm. However, UATs found in previous works are often unreadable
tokens or characters and can be easily distinguished from natural texts with
adaptive defenses. In this work, we consider the naturalness of the UATs and
develop $\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs
by a gradient-based beam search algorithm that not only effectively attacks the
target PLMs and PFMs but also maintains the naturalness among the trigger
tokens. Extensive results demonstrate the effectiveness of
$\textit{LinkPrompt}$, as well as the transferability of UATs generated by
$\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and
API-accessed LLM GPT-3.5-turbo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the main conference of NAACL2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLatrieval: LLM-Verified Retrieval for Verifiable Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07838v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07838v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue Yin, Tianxiang Sun, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verifiable generation aims to let the large language model (LLM) generate
text with supporting documents, which enables the user to flexibly verify the
answer and makes the LLM's output more reliable. Retrieval plays a crucial role
in verifiable generation. Specifically, the retrieved documents not only
supplement knowledge to help the LLM generate correct answers, but also serve
as supporting evidence for the user to verify the LLM's output. However, the
widely used retrievers become the bottleneck of the entire pipeline and limit
the overall performance. Their capabilities are usually inferior to LLMs since
they often have much fewer parameters than the large language model and have
not been demonstrated to scale well to the size of LLMs. If the retriever does
not correctly find the supporting documents, the LLM can not generate the
correct and verifiable answer, which overshadows the LLM's remarkable
abilities. To address these limitations, we propose \LLatrieval (Large Language
Model Verified Retrieval), where the LLM updates the retrieval result until it
verifies that the retrieved documents can sufficiently support answering the
question. Thus, the LLM can iteratively provide feedback to retrieval and
facilitate the retrieval result to fully support verifiable generation.
Experiments show that LLatrieval significantly outperforms extensive baselines
and achieves state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2024 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Object Coherence in Layout-to-Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.10522v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.10522v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wang, Weizhong Zhang, Jianwei Zheng, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel's generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuzhao Xue, Zian Zheng, Yao Fu, Jinjie Ni, Zangwei Zheng, Wangchunshu Zhou, Yang You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To help the open-source community have a better understanding of
Mixture-of-Experts (MoE) based large language models (LLMs), we train and
release OpenMoE, a series of fully open-sourced and reproducible decoder-only
MoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T
tokens. Our investigation confirms that MoE-based LLMs can offer a more
favorable cost-effectiveness trade-off than dense LLMs, highlighting the
potential effectiveness for future LLM development.
  One more important contribution of this study is an in-depth analysis of the
routing mechanisms within our OpenMoE models, leading to three significant
findings: Context-Independent Specialization, Early Routing Learning, and
Drop-towards-the-End. We discovered that routing decisions in MoE models are
predominantly based on token IDs, with minimal context relevance. The
token-to-expert assignments are determined early in the pre-training phase and
remain largely unchanged. This imperfect routing can result in performance
degradation, particularly in sequential tasks like multi-turn conversations,
where tokens appearing later in a sequence are more likely to be dropped.
Finally, we rethink our design based on the above-mentioned observations and
analysis. To facilitate future MoE LLM development, we propose potential
strategies for mitigating the issues we found and further improving
off-the-shelf MoE LLM designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulan Hu, Sheng Ouyang, Zhirui Yang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class imbalance in graph data presents significant challenges for node
classification. While existing methods, such as SMOTE-based approaches,
partially mitigate this issue, they still exhibit limitations in constructing
imbalanced graphs. Generative self-supervised learning (SSL) methods,
exemplified by graph autoencoders (GAEs), offer a promising solution by
directly generating minority nodes from the data itself, yet their potential
remains underexplored. In this paper, we delve into the shortcomings of
SMOTE-based approaches in the construction of imbalanced graphs. Furthermore,
we introduce VIGraph, a simple yet effective generative SSL approach that
relies on the Variational GAE as the fundamental model. VIGraph strictly
adheres to the concept of imbalance when constructing imbalanced graphs and
innovatively leverages the variational inference (VI) ability of Variational
GAE to generate nodes for minority classes. VIGraph introduces comprehensive
training strategies, including cross-view contrastive learning at the decoding
phase to capture semantic knowledge, adjacency matrix reconstruction to
preserve graph structure, and alignment strategy to ensure stable training.
VIGraph can generate high-quality nodes directly usable for classification,
eliminating the need to integrate the generated nodes back to the graph as well
as additional retraining found in SMOTE-based methods. We conduct extensive
experiments, results from which demonstrate the superiority and generality of
our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Quadruped Locomotion Using Differentiable <span class="highlight-title">Simulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14864v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14864v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Song, Sangbae Kim, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While most recent advancements in legged robot control have been driven by
model-free reinforcement learning, we explore the potential of differentiable
simulation. Differentiable simulation promises faster convergence and more
stable training by computing low-variant first-order gradients using the robot
model, but so far, its use for legged robot control has remained limited to
simulation. The main challenge with differentiable simulation lies in the
complex optimization landscape of robotic tasks due to discontinuities in
contact-rich environments, e.g., quadruped locomotion. This work proposes a
new, differentiable simulation framework to overcome these challenges. The key
idea involves decoupling the complex whole-body simulation, which may exhibit
discontinuities due to contact, into two separate continuous domains.
Subsequently, we align the robot state resulting from the simplified model with
a more precise, non-differentiable simulator to maintain sufficient simulation
accuracy. Our framework enables learning quadruped walking in minutes using a
single simulated robot without any parallelization. When augmented with GPU
parallelization, our approach allows the quadruped robot to master diverse
locomotion skills, including trot, pace, bound, and gallop, on challenging
terrains in minutes. Additionally, our policy achieves robust locomotion
performance in the real world zero-shot. To the best of our knowledge, this
work represents the first demonstration of using differentiable simulation for
controlling a real quadruped robot. This work provides several important
insights into using differentiable simulations for legged locomotion in the
real world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) showcase impressive capabilities but encounter
challenges like hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the generation,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval, the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces up-to-date evaluation framework and
benchmark. At the end, this article delineates the challenges currently faced
and points out prospective avenues for research and development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ World Models via Policy-Guided Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08533v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08533v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Rigter, Jun Yamada, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for short trajectories, with the exception of autoregressive diffusion. For
short trajectories, PolyGRAD obtains similar errors to autoregressive
diffusion, but with lower computational requirements. For long trajectories,
PolyGRAD obtains comparable performance to baselines. Our experiments
demonstrate that PolyGRAD enables performant policies to be trained via
on-policy RL in imagination for MuJoCo continuous control domains. Thus,
PolyGRAD introduces a new paradigm for accurate on-policy world modelling
without autoregressive sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in TMLR, March 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, Cécile Rousseau, Alessandra Pascale, John Dinsmore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Concept-Based Causal Transition and Symbolic Reasoning for
  Visual Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03325v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03325v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilue Qian, Peiyu Yu, Ying Nian Wu, Yao Su, Wei Wang, Lifeng Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual planning simulates how humans make decisions to achieve desired goals
in the form of searching for visual causal transitions between an initial
visual state and a final visual goal state. It has become increasingly
important in egocentric vision with its advantages in guiding agents to perform
daily tasks in complex environments. In this paper, we propose an interpretable
and generalizable visual planning framework consisting of i) a novel
Substitution-based Concept Learner (SCL) that abstracts visual inputs into
disentangled concept representations, ii) symbol abstraction and reasoning that
performs task planning via the self-learned symbols, and iii) a Visual Causal
Transition model (ViCT) that grounds visual causal transitions to semantically
similar real-world actions. Given an initial state, we perform goal-conditioned
visual planning with a symbolic reasoning method fueled by the learned
representations and causal transitions to reach the goal state. To verify the
effectiveness of the proposed model, we collect a large-scale visual planning
dataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this
challenging dataset demonstrate the superior performance of our method in
visual task planning. Empirically, we show that our framework can generalize to
unseen task trajectories, unseen object categories, and real-world data.
Further details of this work are provided at
https://fqyqc.github.io/ConTranPlan/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying the Correlation Between Language Distance and Cross-Lingual
  Transfer in a Multilingual Representation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fred Philippy, Siwen Guo, Shohreh Haddadan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research has investigated the impact of various linguistic features on
cross-lingual transfer performance. In this study, we investigate the manner in
which this effect can be mapped onto the representation space. While past
studies have focused on the impact on cross-lingual alignment in multilingual
language models during fine-tuning, this study examines the absolute evolution
of the respective language representation spaces produced by MLLMs. We place a
specific emphasis on the role of linguistic characteristics and investigate
their inter-correlation with the impact on representation spaces and
cross-lingual transfer performance. Additionally, this paper provides
preliminary evidence of how these findings can be leveraged to enhance transfer
to linguistically distant languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGTYP Workshop 2023 (co-located with EACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Effects of Mixed Sample Data Augmentation are Class Dependent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeil Lee, Hansang Lee, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and
PuzzleMix, have been widely acknowledged for enhancing performance in a variety
of tasks. A previous study reported the class dependency of traditional data
augmentation (DA), where certain classes benefit disproportionately compared to
others. This paper reveals a class dependent effect of MSDA, where some classes
experience improved performance while others experience degraded performance.
This research addresses the issue of class dependency in MSDA and proposes an
algorithm to mitigate it. The approach involves training on a mixture of MSDA
and non-MSDA data, which not only mitigates the negative impact on the affected
classes, but also improves overall accuracy. Furthermore, we provide in-depth
analysis and discussion of why MSDA introduced class dependencies and which
classes are most likely to have them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 18 figures, Overall Revision</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18920v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18920v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Marvin Eisenberger, Nafie El Amrani, Daniel Cremers, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although 3D shape matching and interpolation are highly interrelated, they
are often studied separately and applied sequentially to relate different 3D
shapes, thus resulting in sub-optimal performance. In this work we present a
unified framework to predict both point-wise correspondences and shape
interpolation between 3D shapes. To this end, we combine the deep functional
map framework with classical surface deformation models to map shapes in both
spectral and spatial domains. On the one hand, by incorporating spatial maps,
our method obtains more accurate and smooth point-wise correspondences compared
to previous functional map methods for shape matching. On the other hand, by
introducing spectral maps, our method gets rid of commonly used but
computationally expensive geodesic distance constraints that are only valid for
near-isometric shape deformations. Furthermore, we propose a novel test-time
adaptation scheme to capture both pose-dominant and shape-dominant
deformations. Using different challenging datasets, we demonstrate that our
method outperforms previous state-of-the-art methods for both shape matching
and interpolation, even compared to supervised approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12609v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12609v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xudong Shen, Hannah Brown, Jiashu Tao, Martin Strobel, Yao Tong, Akshay Narayan, Harold Soh, Finale Doshi-Velez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is increasing attention being given to how to regulate AI systems. As
governing bodies grapple with what values to encapsulate into regulation, we
consider the technical half of the question: To what extent can AI experts vet
an AI system for adherence to regulatory requirements? We investigate this
question through the lens of two public sector procurement checklists,
identifying what we can do now, what should be possible with technical
innovation, and what requirements need a more interdisciplinary approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>scheduled for publication in the Communications of the ACM, titled
  "Directions of Technical Innovation for Regulatable AI Systems"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMP++: Motion Manifold Primitives with Parametric Curve Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17072v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17072v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion Manifold Primitives (MMP), a manifold-based approach for encoding
basic motion skills, can produce diverse trajectories, enabling the system to
adapt to unseen constraints. Nonetheless, we argue that current MMP models lack
crucial functionalities of movement primitives, such as temporal and via-points
modulation, found in traditional approaches. This shortfall primarily stems
from MMP's reliance on discrete-time trajectories. To overcome these
limitations, we introduce Motion Manifold Primitives++ (MMP++), a new model
that integrates the strengths of both MMP and traditional methods by
incorporating parametric curve representations into the MMP framework.
Furthermore, we identify a significant challenge with MMP++: performance
degradation due to geometric distortions in the latent space, meaning that
similar motions are not closely positioned. To address this, Isometric Motion
Manifold Primitives++ (IMMP++) is proposed to ensure the latent space
accurately preserves the manifold's geometry. Our experimental results across
various applications, including 2-DoF planar motions, 7-DoF robot arm motions,
and SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing
methods in trajectory generation tasks, achieving substantial improvements in
some cases. Moreover, they enable the modulation of latent coordinates and
via-points, thereby allowing efficient online adaptation to dynamic
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages. This work has been submitted to the IEEE for possible
  publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret-Based Defense in Adversarial <span class="highlight-title">Reinforcement Learning</span> <span class="chip">AAMAS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06912v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06912v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Belaire, Pradeep Varakantham, Thanh Nguyen, David Lo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable
to small adversarial noise in observations. Such adversarial noise can have
disastrous consequences in safety-critical environments. For instance, a
self-driving car receiving adversarially perturbed sensory observations about
nearby signs (e.g., a stop sign physically altered to be perceived as a speed
limit sign) or objects (e.g., cars altered to be recognized as trees) can be
fatal. Existing approaches for making RL algorithms robust to an
observation-perturbing adversary have focused on reactive approaches that
iteratively improve against adversarial examples generated at each iteration.
While such approaches have been shown to provide improvements over regular RL
methods, they are reactive and can fare significantly worse if certain
categories of adversarial examples are not generated during training. To that
end, we pursue a more proactive approach that relies on directly optimizing a
well-studied robustness measure, regret instead of expected value. We provide a
principled approach that minimizes maximum regret over a "neighborhood" of
observations to the received "observation". Our regret criterion can be used to
modify existing value- and policy-based Deep RL methods. We demonstrate that
our approaches provide a significant improvement in performance across a wide
variety of benchmarks against leading approaches for robust Deep RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAMAS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA4DIV: <span class="highlight-title">Multi-Agent</span> <span class="highlight-title">Reinforcement Learning</span> for Search Result
  Diversification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Chen, Jiaxin Mao, Yi Zhang, Dehong Ma, Long Xia, Jun Fan, Daiting Shi, Zhicong Cheng, Simiu Gu, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of search result diversification (SRD) is to ensure that
selected documents cover as many different subtopics as possible. Existing
methods primarily utilize a paradigm of "greedy selection", i.e., selecting one
document with the highest diversity score at a time. These approaches tend to
be inefficient and are easily trapped in a suboptimal state. In addition, some
other methods aim to approximately optimize the diversity metric, such as
$\alpha$-NDCG, but the results still remain suboptimal. To address these
challenges, we introduce Multi-Agent reinforcement learning (MARL) for search
result DIVersity, which called MA4DIV. In this approach, each document is an
agent and the search result diversification is modeled as a cooperative task
among multiple agents. This approach allows for directly optimizing the
diversity metrics, such as $\alpha$-NDCG, while achieving high training
efficiency. We conducted preliminary experiments on public TREC datasets to
demonstrate the effectiveness and potential of MA4DIV. Considering the limited
number of queries in public TREC datasets, we construct a large-scale dataset
from industry sources and show that MA4DIV achieves substantial improvements in
both effectiveness and efficiency than existing baselines on a industrial scale
dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs Are Few-Shot In-Context Low-Resource Language Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Pascale Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) empowers large language models (LLMs) to perform
diverse tasks in underrepresented languages using only short in-context
information, offering a crucial avenue for narrowing the gap between
high-resource and low-resource languages. Nonetheless, there is only a handful
of works explored ICL for low-resource languages with most of them focusing on
relatively high-resource languages, such as French and Spanish. In this work,
we extensively study ICL and its cross-lingual variation (X-ICL) on 25
low-resource and 7 relatively higher-resource languages. Our study not only
assesses the effectiveness of ICL with LLMs in low-resource languages but also
identifies the shortcomings of in-context label alignment, and introduces a
more effective alternative: query alignment. Moreover, we provide valuable
insights into various facets of ICL for low-resource languages. Our study
concludes the significance of few-shot in-context information on enhancing the
low-resource understanding quality of LLMs through semantically relevant
information by closing the language gap in the target language and aligning the
semantics between the targeted low-resource and the high-resource language that
the model is proficient in. Our work highlights the importance of advancing ICL
research, particularly for low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Programming Education with ChatGPT: A Case Study on Student
  Perceptions and Interactions in a Python Course 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxaun Ma, Li Chen, Shin'ichi Konomi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of ChatGPT as a supportive tool in education, notably in
programming courses, addresses the unique challenges of programming education
by providing assistance with debugging, code generation, and explanations.
Despite existing research validating ChatGPT's effectiveness, its application
in university-level programming education and a detailed understanding of
student interactions and perspectives remain limited. This paper explores
ChatGPT's impact on learning in a Python programming course tailored for
first-year students over eight weeks. By analyzing responses from surveys,
open-ended questions, and student-ChatGPT dialog data, we aim to provide a
comprehensive view of ChatGPT's utility and identify both its advantages and
limitations as perceived by students. Our study uncovers a generally positive
reception toward ChatGPT and offers insights into its role in enhancing the
programming education experience. These findings contribute to the broader
discourse on AI's potential in education, suggesting paths for future research
and application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the remarkable achievements in image generation through diffusion
models, the research community has shown increasing interest in extending these
models to video generation. Recent diffusion models for video generation have
predominantly utilized attention layers to extract temporal features. However,
attention layers are limited by their memory consumption, which increases
quadratically with the length of the sequence. This limitation presents
significant challenges when attempting to generate longer video sequences using
diffusion models. To overcome this challenge, we propose leveraging state-space
models (SSMs). SSMs have recently gained attention as viable alternatives due
to their linear memory consumption relative to sequence length. In the
experiments, we first evaluate our SSM-based model with UCF101, a standard
benchmark of video generation. In addition, to investigate the potential of
SSMs for longer video generation, we perform an experiment using the MineRL
Navigate dataset, varying the number of frames to 64, 200, and 400. In these
settings, our SSM-based model can considerably save memory consumption for
longer sequences, while maintaining competitive FVD scores to the
attention-based models. Our codes are available at
https://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as workshop paper at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly Supervised AUC Optimization: A Unified Partial AUC Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Xie, Yu Liu, Hao-Yuan He, Ming Li, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since acquiring perfect supervision is usually difficult, real-world machine
learning tasks often confront inaccurate, incomplete, or inexact supervision,
collectively referred to as weak supervision. In this work, we present WSAUC, a
unified framework for weakly supervised AUC optimization problems, which covers
noisy label learning, positive-unlabeled learning, multi-instance learning, and
semi-supervised learning scenarios. Within the WSAUC framework, we first frame
the AUC optimization problems in various weakly supervised scenarios as a
common formulation of minimizing the AUC risk on contaminated sets, and
demonstrate that the empirical risk minimization problems are consistent with
the true AUC. Then, we introduce a new type of partial AUC, specifically, the
reversed partial AUC (rpAUC), which serves as a robust training objective for
AUC maximization in the presence of contaminated labels. WSAUC offers a
universal solution for AUC optimization in various weakly supervised scenarios
by maximizing the empirical rpAUC. Theoretical and experimental results under
multiple settings support the effectiveness of WSAUC on a range of weakly
supervised AUC optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate
  Professional and Non-Professional Styled Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zong, Yuyan Chen, Weiming Lu, Jian Shao, Yueting Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated efficacy in various linguistic
applications, including text summarization and controlled text generation.
However, studies into their capacity of switching between styles via
fine-tuning remain underexplored. This study concentrates on textual
professionalism and introduces a novel methodology, named ProSwitch, which
equips a language model with the ability to produce both professional and
non-professional responses through knowledge-guided instruction tuning.
ProSwitch unfolds across three phases: data preparation for gathering domain
knowledge and training corpus; instruction tuning for optimizing language
models with multiple levels of instruction formats; and comprehensive
evaluation for assessing the professionalism discrimination and reference-based
quality of generated text. Comparative analysis of ProSwitch against both
general and specialized language models reveals that our approach outperforms
baselines in switching between professional and non-professional text
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Re2LLM: Reflective Reinforcement Large Language Model for Session-based
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16427v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16427v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Wang, Yingpeng Du, Zhu Sun, Haoyan Chua, Kaidong Feng, Wenya Wang, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are emerging as promising approaches to enhance
session-based recommendation (SBR), where both prompt-based and
fine-tuning-based methods have been widely investigated to align LLMs with SBR.
However, the former methods struggle with optimal prompts to elicit the correct
reasoning of LLMs due to the lack of task-specific feedback, leading to
unsatisfactory recommendations. Although the latter methods attempt to
fine-tune LLMs with domain-specific knowledge, they face limitations such as
high computational costs and reliance on open-source backbones. To address such
issues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for
SBR, guiding LLMs to focus on specialized knowledge essential for more accurate
recommendations effectively and efficiently. In particular, we first design the
Reflective Exploration Module to effectively extract knowledge that is readily
understandable and digestible by LLMs. To be specific, we direct LLMs to
examine recommendation errors through self-reflection and construct a knowledge
base (KB) comprising hints capable of rectifying these errors. To efficiently
elicit the correct reasoning of LLMs, we further devise the Reinforcement
Utilization Module to train a lightweight retrieval agent. It learns to select
hints from the constructed KB based on the task-specific feedback, where the
hints can serve as guidance to help correct LLMs reasoning for better
recommendations. Extensive experiments on multiple real-world datasets
demonstrate that our method consistently outperforms state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Most
existing works primarily focus on post-training and fine-tuning tailored for
cross-encoders. However, there are no post-training methods tailored for dense
encoders in dialogue response selection. We argue that when the current
language model, based on dense dialogue systems (such as BERT), is employed as
a dense encoder, it separately encodes dialogue context and response, leading
to a struggle to achieve the alignment of both representations. Thus, we
propose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward
yet effective post-training technique tailored for dense encoders in dialogue
response selection. Dial-MAE uses an asymmetric encoder-decoder architecture to
compress the dialogue semantics into dense vectors, which achieves better
alignment between the features of the dialogue context and response. Our
experiments have demonstrated that Dial-MAE is highly effective, achieving
state-of-the-art performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by NAACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoftTiger: A Clinical Foundation Model for Healthcare Workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Chen, Igor Couto, Wei Cai, Cong Fu, Bruno Dorneles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SoftTiger, a clinical large language model (CLaM) designed as a
foundation model for healthcare workflows. The narrative and unstructured
nature of clinical notes is a major obstacle for healthcare intelligentization.
We address a critical problem of structuring clinical notes into clinical data,
according to international interoperability standards. We collect and annotate
data for three subtasks, namely, international patient summary, clinical
impression and medical encounter. We then supervised fine-tuned a
state-of-the-art LLM using public and credentialed clinical data. The training
is orchestrated in a way that the target model can first support basic clinical
tasks such as abbreviation expansion and temporal information extraction, and
then learn to perform more complex downstream clinical tasks. Moreover, we
address several modeling challenges in the healthcare context, e.g., extra long
context window. Our blind pairwise evaluation shows that SoftTiger outperforms
other popular open-source models and GPT-3.5, comparable to Gemini-pro, with a
mild gap from GPT-4. We believe that LLMs may become a step-stone towards
healthcare digitalization and democratization. Therefore, we publicly release
SoftTiger models at scales of 13 billion and 70 billion parameters, as well as
datasets and code for our innovative scalable evaluation, hopefully, making a
significant contribution to the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Multimodal Large Language Models for Global and Local Semantic
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.17304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.17304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of Multimodal Large Language Models (MLLMs) has greatly
accelerated the development of applications in understanding integrated texts
and images. Recent works leverage image-caption datasets to train MLLMs,
achieving state-of-the-art performance on image-to-text tasks. However, there
are few studies exploring which layers of MLLMs make the most effort to the
global image information, which plays vital roles in multimodal comprehension
and generation. In this study, we find that the intermediate layers of models
can encode more global semantic information, whose representation vectors
perform better on visual-language entailment tasks, rather than the topmost
layers. We further probe models regarding local semantic representations
through object recognition tasks. We find that the topmost layers may
excessively focus on local information, leading to a diminished ability to
encode global information. Our code and data are released via
https://github.com/kobayashikanna01/probing_MLLM_rep.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC-COLING 2024 as a short paper (Camera Ready)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Distribution and Out-of-Distribution Self-supervised ECG
  Representation Learning for Arrhythmia Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Soltanieh, Javad Hashemi, Ali Etemad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a systematic investigation into the effectiveness of
Self-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia
detection. We begin by conducting a novel analysis of the data distributions on
three popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To
the best of our knowledge, our study is the first to quantitatively explore and
characterize these distributions in the area. We then perform a comprehensive
set of experiments using different augmentations and parameters to evaluate the
effectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG
representation learning, where we observe the best performance achieved by
SwAV. Furthermore, our analysis shows that SSL methods achieve highly
competitive results to those achieved by supervised state-of-the-art methods.
To further assess the performance of these methods on both In-Distribution (ID)
and Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and
testing experiments. Our comprehensive experiments show almost identical
results when comparing ID and OOD schemes, indicating that SSL techniques can
learn highly effective representations that generalize well across different
OOD datasets. This finding can have major implications for ECG-based arrhythmia
detection. Lastly, to further analyze our results, we perform detailed
per-disease studies on the performance of the SSL methods on the three
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been published in the IEEE Journal of Biomedical and
  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.
  Hashemi and A. Etemad, "In-Distribution and Out-of-Distribution
  Self-Supervised ECG Representation Learning for Arrhythmia Detection," in
  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.
  789-800, Feb. 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Cost-Constrained Behaviors in <span class="highlight-title">Reinforcement Learning</span> <span class="chip">ICAPS-24</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17456v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17456v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Shao, Pradeep Varakantham, Shih-Fen Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex planning and scheduling problems have long been solved using various
optimization or heuristic approaches. In recent years, imitation learning that
aims to learn from expert demonstrations has been proposed as a viable
alternative to solving these problems. Generally speaking, imitation learning
is designed to learn either the reward (or preference) model or directly the
behavioral policy by observing the behavior of an expert. Existing work in
imitation learning and inverse reinforcement learning has focused on imitation
primarily in unconstrained settings (e.g., no limit on fuel consumed by the
vehicle). However, in many real-world domains, the behavior of an expert is
governed not only by reward (or preference) but also by constraints. For
instance, decisions on self-driving delivery vehicles are dependent not only on
the route preferences/rewards (depending on past demand data) but also on the
fuel in the vehicle and the time available. In such problems, imitation
learning is challenging as decisions are not only dictated by the reward model
but are also dependent on a cost-constrained model. In this paper, we provide
multiple methods that match expert distributions in the presence of trajectory
cost constraints through (a) Lagrangian-based method; (b) Meta-gradients to
find a good trade-off between expected return and minimizing constraint
violation; and (c) Cost-violation-based alternating gradient. We empirically
show that leading imitation learning approaches imitate cost-constrained
behaviors poorly and our meta-gradient-based approach achieves the best
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 34th International Conference on Automated Planning
  and Scheduling (ICAPS-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Innovation Paradox: Concept Space Expansion with Diminishing
  Originality and the Promise of Creative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhad Sarica, Jianxi Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innovation, typically spurred by reusing, recombining, and synthesizing
existing concepts, is expected to result in an exponential growth of the
concept space over time. However, our statistical analysis of TechNet, which is
a comprehensive technology semantic network encompassing over four million
concepts derived from patent texts, reveals a linear rather than exponential
expansion of the overall technological concept space. Moreover, there is a
notable decline in the originality of newly created concepts. These trends can
be attributed to the constraints of human cognitive abilities to innovate
beyond an ever-growing space of prior art, among other factors. Integrating
creative artificial intelligence (CAI) into the innovation process holds the
potential to overcome these limitations and alter the observed trends in the
future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Forthcoming on the Design Science</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs in Political Science: Heralding a New Era of Visual Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interest is increasing among political scientists in leveraging the extensive
information available in images. However, the challenge of interpreting these
images lies in the need for specialized knowledge in computer vision and access
to specialized hardware. As a result, image analysis has been limited to a
relatively small group within the political science community. This landscape
could potentially change thanks to the rise of large language models (LLMs).
This paper aims to raise awareness of the feasibility of using Gemini for image
content analysis. A retrospective analysis was conducted on a corpus of 688
images. Content reports were elicited from Gemini for each image and then
manually evaluated by the authors. We find that Gemini is highly accurate in
performing object detection, which is arguably the most common and fundamental
task in image analysis for political scientists. Equally important, we show
that it is easy to implement as the entire command consists of a single prompt
in natural language; it is fast to run and should meet the time budget of most
researchers; and it is free to use and does not require any specialized
hardware. In addition, we illustrate how political scientists can leverage
Gemini for other image understanding tasks, including face identification,
sentiment analysis, and caption generation. Our findings suggest that Gemini
and other similar LLMs have the potential to drastically stimulate and
accelerate image research in political science and social sciences more
broadly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16915v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16915v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atsushi Keyaki, Ribeka Keyaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning in information retrieval systems using pre-trained language
models (PLM-based IR) requires learning query representations and
query-document relations, in addition to downstream task-specific learning.
This study introduces coarse-tuning as an intermediate learning stage that
bridges pre-training and fine-tuning. By learning query representations and
query-document relations in coarse-tuning, we aim to reduce the load of
fine-tuning and improve the learning effect of downstream IR tasks. We propose
Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the
appropriateness of query-document pairs. Evaluation experiments show that the
proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc
document retrieval datasets. Furthermore, the results of the query prediction
task suggested that coarse-tuning facilitated learning of query representation
and query-document relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at LREC-COLING 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Before You Leap: Problem Elaboration Prompting Improves
  Mathematical Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liao, Jidong Tian, Shaohua Hu, Hao He, Yaohui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) still grapple with complex tasks like
mathematical reasoning. Despite significant efforts invested in improving
prefix prompts or reasoning process, the crucial role of problem context might
have been neglected. Accurate recognition of inputs is fundamental for solving
mathematical tasks, as ill-formed problems could potentially mislead LLM's
reasoning. In this study, we propose a new approach named Problem Elaboration
Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,
PEP decomposes and elucidates the problem context before reasoning, therefore
enhancing the context modeling and parsing efficiency. Experiments across
datasets and models demonstrate promising performances: (1) PEP demonstrates an
overall enhancement in various mathematical tasks. For instance, with the
GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through
greedy decoding and self-consistency, respectively. (2) PEP can be easily
implemented and integrated with other prompting methods. (3) PEP shows
particular strength in handling distraction problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follower Agnostic Methods for Stackelberg Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01421v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01421v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Maheshwari, James Cheng, S. Shankar Sasty, Lillian Ratliff, Eric Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present an efficient algorithm to solve online Stackelberg
games, featuring multiple followers, in a follower-agnostic manner. Unlike
previous works, our approach works even when leader has no knowledge about the
followers' utility functions or strategy space. Our algorithm introduces a
unique gradient estimator, leveraging specially designed strategies to probe
followers. In a departure from traditional assumptions of optimal play, we
model followers' responses using a convergent adaptation rule, allowing for
realistic and dynamic interactions. The leader constructs the gradient
estimator solely based on observations of followers' actions. We provide both
non-asymptotic convergence rates to stationary points of the leader's objective
and demonstrate asymptotic convergence to a \emph{local Stackelberg
equilibrium}. To validate the effectiveness of our algorithm, we use this
algorithm to solve the problem of incentive design on a large-scale
transportation network, showcasing its robustness even when the leader lacks
access to followers' demand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Act without Actions <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Schmidt, Minqi Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training large models on vast amounts of web data has proven to be an
effective approach for obtaining powerful, general models in domains such as
language and vision. However, this paradigm has not yet taken hold in
reinforcement learning. This is because videos, the most abundant form of
embodied behavioral data on the web, lack the action labels required by
existing methods for imitating behavior from demonstrations. We introduce
Latent Action Policies (LAPO), a method for recovering latent action
information, and thereby latent-action policies, world models, and inverse
dynamics models, purely from videos. LAPO is the first method able to recover
the structure of the true action space just from observed dynamics, even in
challenging procedurally-generated environments. LAPO enables training
latent-action policies that can be rapidly fine-tuned into expert-level
policies, either offline using a small action-labeled dataset, or online with
rewards. LAPO takes a first step towards pre-training powerful, generalist
policies and world models on the vast amounts of videos readily available on
the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024 (spotlight). The code can be found at
  http://github.com/schmidtdominik/LAPO</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Delaunay Refinement Algorithm for the Particle Finite Element Method
  applied to Free Surface Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Leyssens, Michel Henry, Jonathan Lambrechts, Jean-Francois Remacle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes two contributions to the calculation of free surface
flows using the particle finite element method (PFEM). The PFEM is based on a
Lagrangian approach: a set of particles defines the fluid. Then, unlike a pure
Lagrangian method, all the particles are connected by a triangular mesh. The
difficulty lies in locating the free surface from this mesh. It is a matter of
deciding which of the elements in the mesh are part of the fluid domain, and to
define a boundary - the free surface. Then, the incompressible Navier-Stokes
equations are solved on the fluid domain and the particles' position is updated
using the resulting velocity vector. Our first contribution is to propose an
approach to adapt the mesh with theoretical guarantees of quality: the mesh
generation community has acquired a lot of experience and understanding about
mesh adaptation approaches with guarantees of quality on the final mesh. We use
here a Delaunay refinement strategy, allowing to insert and remove nodes while
gradually improving mesh quality. We show that this allows to create stable and
smooth free surface geometries. Our PFEM approach models the topological
evolution of one fluid. It is nevertheless necessary to apply conditions on the
domain boundaries. When a boundary is a free surface, the flow on the other
side is not modelled, it is represented by an external pressure. On the
external free surface boundary, atmospheric pressure can be imposed.
Nevertheless, there may be internal free surfaces: the fluid can fully
encapsulate cavities to form bubbles. The pressure required to maintain the
volume of those bubbles is a priori unknown. We propose a multi-point
constraint approach to enforce global incompressibility of those empty bubbles.
This approach allows to accurately model bubbly flows that involve two fluids
with large density differences, while only modelling the heavier fluid.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Damage Mechanics Challenge: <span class="highlight-title">Prediction</span>s based on the phase field
  fracture model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Navidtehrani, R. Duddu, E. Martínez-Pañeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we describe our contribution to the Purdue-SANDIA-LLNL
\emph{Damage Mechanics Challenge}. The phase field fracture model is adopted to
blindly estimate the failure characteristics of the challenge test, an
unconventional three-point bending experiment on an additively manufactured
rock resembling a type of gypsum. The model is formulated in a variationally
consistent fashion, incorporating a volumetric-deviatoric strain energy
decomposition, and the numerical implementation adopts a monolithic
unconditionally stable solution scheme. Our focus is on providing an efficient
and simple yet rigorous approach capable of delivering accurate predictions
based solely on physical parameters. Model inputs are Young's modulus $E$,
Poisson's ratio $\nu$, toughness $G_c$ and strength $\sigma_c$ (as determined
by the choice of phase field length scale $\ell$). We show that a single mode I
three-point bending test is sufficient to calibrate the model, and that the
calibrated model can then reliably predict the force versus displacement
responses, crack paths and surface crack morphologies of more intricate
three-point bending experiments that are inherently mixed-mode. Importantly,
our peak load, crack trajectory and crack surface morphology predictions for
the challenge test, submitted before the experimental data was released, show a
remarkable agreement with experiments. The characteristics of the challenge,
and how changes in these can impact the predictive abilities of phase field
fracture models, are also discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A thermodynamically consistent physics-informed deep learning material
  model for short fiber/polymer nanocomposites 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Betim Bahtiri, Behrouz Arash, Sven Scheffler, Maximilian Jux, Raimund Rolfes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a physics-informed deep learning (PIDL)-based constitutive
model for investigating the viscoelastic-viscoplastic behavior of short
fiber-reinforced nanoparticle-filled epoxies under various ambient conditions.
The deep-learning model is trained to enforce thermodynamic principles, leading
to a thermodynamically consistent constitutive model. To accomplish this, a
long short-term memory network is combined with a feed-forward neural network
to predict internal variables required for characterizing the internal
dissipation of the nanocomposite materials. In addition, another feed-forward
neural network is used to indicate the free-energy function, which enables
defining the thermodynamic state of the entire system. The PIDL model is
initially developed for the three-dimensional case by generating synthetic data
from a classical constitutive model. The model is then trained by extracting
the data directly from cyclic loading-unloading experimental tests. Numerical
examples show that the PIDL model can accurately predict the mechanical
behavior of epoxy-based nanocomposites for different volume fractions of fibers
and nanoparticles under various hygrothermal conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2305.08102</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of different methods for identification of dominant
  oscillation mode 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maja Muftic Dedovic, Samir Avdakovic, Adnan Mujezinovic, Nedis Dautbasic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces and compares the various techniques for identification
and analysis of low frequency oscillations in a power system. Inter-area
electromechanical oscillations are the focus of this paper. After
multiresolution decomposition of characteristic signals, physical
characteristics of system oscillations in signal components are identified and
presented using the Fourier transform, Prony method, Matrix Pencil Analysis
Method, S-transform, Global Wavelet Spectrum and Hilbert Huang transform
(Hilbert Marginal Spectrum) in time-frequency domain representation. The
analyses were performed on real frequency signals obtained from FNET GridEye
system during the earthquake that triggered the shutdown of the North Anna
Nuclear Generating Station in the east coast of the United States. In addition,
according to the obtained results the proposed methods have proven to be
reliable for identification of the model parameters of low-frequency
oscillation in power systems. The relevant analyses are carried out in MATLAB
coding environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context, Composition, Automation, and Communication -- The C2AC Roadmap
  for Modeling and <span class="highlight-title">Simulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adelinde Uhrmacher, Peter Frazier, Reiner Hähnle, Franziska Klügl, Fabian Lorig, Bertram Ludäscher, Laura Nenzi, Cristina Ruiz-Martin, Bernhard Rumpe, Claudia Szabo, Gabriel A. Wainer, Pia Wilsdorf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation has become, in many application areas, a sine-qua-non. Most
recently, COVID-19 has underlined the importance of simulation studies and
limitations in current practices and methods. We identify four goals of
methodological work for addressing these limitations. The first is to provide
better support for capturing, representing, and evaluating the context of
simulation studies, including research questions, assumptions, requirements,
and activities contributing to a simulation study. In addition, the composition
of simulation models and other simulation studies' products must be supported
beyond syntactical coherence, including aspects of semantics and purpose,
enabling their effective reuse. A higher degree of automating simulation
studies will contribute to more systematic, standardized simulation studies and
their efficiency. Finally, it is essential to invest increased effort into
effectively communicating results and the processes involved in simulation
studies to enable their use in research and decision-making. These goals are
not pursued independently of each other, but they will benefit from and
sometimes even rely on advances in other subfields. In the present paper, we
explore the basis and interdependencies evident in current research and
practice and delineate future research directions based on these
considerations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Wang, P S Pravin, Zhe Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computational efficiency and non-adversarial robustness are critical factors
in real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters non-adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Networks. This model is explicitly designed for fast and robust
optimization-based tasks and outperforms existing recurrent units across a
spectrum of engineering tasks in terms of computational efficiency and
non-adversarial robustness, including real-world solar irradiance prediction
for Solar PV system planning at LHT Holdings in Singapore and real-time Model
Predictive Control optimization for a nonlinear chemical reactor.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-26T00:00:00Z">2024-03-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code Generation for Conic Model-Predictive Control on Microcontrollers
  with TinyMPC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Schoedel, Khai Nguyen, Elakhya Nedumaran, Brian Plancher, Zachary Manchester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conic constraints appear in many important control applications like legged
locomotion, robotic manipulation, and autonomous rocket landing. However,
current solvers for conic optimization problems have relatively heavy
computational demands in terms of both floating-point operations and memory
footprint, making them impractical for use on small embedded devices. We extend
TinyMPC, an open-source, high-speed solver targeting low-power embedded control
applications, to handle second-order cone constraints. We also present
code-generation software to enable deployment of TinyMPC on a variety of
microcontrollers. We benchmark our generated code against state-of-the-art
embedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed
increase over ECOS while consuming less memory. Finally, we demonstrate
TinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained
quadrotor with fast dynamics. TinyMPC and its code-generation tools are
publicly available at https://tinympc.org.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to CDC, 2024. First two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple Model Reference <span class="highlight-title">Adaptive</span> Control with Blending for Non-Square
  Multivariable Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Lovi, Baris Fidan, Christopher Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we develop a multiple model reference adaptive controller
(MMRAC) with blending. The systems under consideration are non-square, i.e.,
the number of inputs is not equal to the number of states; multi-input, linear,
time-invariant with uncertain parameters that lie inside of a known, compact,
and convex set. Moreover, the full state of the plant is available for
feedback. A multiple model online identification scheme for the plant's state
and input matrices is developed that guarantees the estimated parameters
converge to the underlying plant model under the assumption of persistence of
excitation. Using an exact matching condition, the parameter estimates are used
in a control law such that the plant's states asymptotically track the
reference signal generated by a state-space model reference. The control
architecture is proven to provide boundedness of all closed-loop signals and to
asymptotically drive the state tracking error to zero. Numerical simulations
illustrate the stability and efficacy of the proposed MMRAC scheme.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, IEEE Journal Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANOCA: AC Network-aware Optimal Curtailment Approach for Dynamic Hosting
  Capacity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel O. Badmus, Amritanshu Pandey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With exponential growth in distributed energy resources (DERs) coupled with
at-capacity distribution grid infrastructure, prosumers cannot always export
all extra power to the grid without violating technical limits. Consequently, a
slew of dynamic hosting capacity (DHC) algorithms have emerged for optimal
utilization of grid infrastructure while maximizing export from DERs. Most of
these DHC algorithms utilize the concept of operating envelopes (OE)}, where
the utility gives prosumers technical power export limits, and they are free to
export power within these limits. Recent studies have shown that OE-based
frameworks have drawbacks, as most develop power export limits based on convex
or linear grid models. As OEs must capture extreme operating conditions, both
convex and linear models can violate technical limits in practice because they
approximate grid physics. However, AC models are unsuitable because they may
not be feasible within the whole region of OE. We propose a new two-stage
optimization framework for DHC built on three-phase AC models to address the
current gaps. In this approach, the prosumers first run a receding horizon
multi-period optimization to identify optimal export power setpoints to
communicate with the utility. The utility then performs an infeasibility-based
optimization to either accept the prosumer's request or dispatch an optimal
curtail signal such that overall system technical constraints are not violated.
To explore various curtailment strategies, we develop an L1, L2, and Linf
norm-based dispatch algorithm with an exact three-phase AC model. We test our
framework on a 1420 three-phase node meshed distribution network and show that
the proposed algorithm optimally curtails DERs while guaranteeing the AC
feasibility of the network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Sontag s to Cardano-Lyapunov Formula for Systems Not Affine in the
  Control: Convection-Enabled PDE Stabilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Camil Belhadjoudja, Miroslav Krstic, Mohamed Maghenem, Emmanuel Witrant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose the first generalization of Sontag s universal controller to
systems not affine in the control, particularly, to PDEs with boundary
actuation. We assume that the system admits a control Lyapunov function (CLF)
whose derivative, rather than being affine in the control, has either a
depressed cubic, quadratic, or depressed quartic dependence on the control. For
each case, a continuous universal controller that vanishes at the origin and
achieves global exponential stability is derived. We prove our result in the
context of convectionreaction-diffusion PDEs with Dirichlet actuation. We show
that if the convection has a certain structure, then the L2 norm of the state
is a CLF. In addition to generalizing Sontag s formula to some non-affine
systems, we present the first general Lyapunov approach for boundary control of
nonlinear PDEs. We illustrate our results via a numerical example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be presented at the 2024 American Control Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Integral Control with Rollout Clustering and Dynamic Obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Patrick, Efstathios Bakolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model Predictive Path Integral (MPPI) control has proven to be a powerful
tool for the control of uncertain systems (such as systems subject to
disturbances and systems with unmodeled dynamics). One important limitation of
the baseline MPPI algorithm is that it does not utilize simulated trajectories
to their fullest extent. For one, it assumes that the average of all
trajectories weighted by their performance index will be a safe trajectory. In
this paper, multiple examples are shown where the previous assumption does not
hold, and a trajectory clustering technique is presented that reduces the
chances of the weighted average crossing in an unsafe region. Secondly, MPPI
does not account for dynamic obstacles, so the authors put forward a novel cost
function that accounts for dynamic obstacles without adding significant
computation time to the overall algorithm. The novel contributions proposed in
this paper were evaluated with extensive simulations to demonstrate
improvements upon the state-of-the-art MPPI techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, extended version of ACC 2024 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Adaptive</span> Boundary Control of the Kuramoto-Sivashinsky Equation Under
  Intermittent Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Camil Belhadjoudja, Mohamed Maghenem, Emmanuel Witrant, Christophe Prieur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study in this paper boundary stabilization, in the L2 sense, of the
one-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing.
We assume that we measure the state of this spatio-temporal equation on a given
spatial subdomain during certain intervals of time, while we measure the state
on the remaining spatial subdomain during the remaining intervals of time. As a
result, we assign a feedback law at the boundary of the spatial domain and
force to zero the value of the state at the junction of the two subdomains.
Throughout the study, the destabilizing coefficient is assumed to be
space-dependent and bounded but unknown. Adaptive boundary controllers are
designed under different assumptions on the forcing term. In particular, when
the forcing term is null, we guarantee global exponential stability of the
origin. Furthermore, when the forcing term is bounded and admits a known upper
bound, we guarantee input-to-state stability, and only global uniform ultimate
boundedness is guaranteed when the upper bound is unknown. Numerical
simulations are performed to illustrate our results
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Automatica</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Piecewise Residuals of Control Barrier Functions for Safety of
  Switching Systems using Multi-Output Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aali, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control barrier functions (CBFs) have recently been introduced as a
systematic tool to ensure safety by establishing set invariance. When combined
with a control Lyapunov function (CLF), they form a safety-critical control
mechanism. However, the effectiveness of CBFs and CLFs is closely tied to the
system model. In practice, model uncertainty can jeopardize safety and
stability guarantees and may lead to undesirable performance. In this paper, we
develop a safe learning-based control strategy for switching systems in the
face of uncertainty. We focus on the case that a nominal model is available for
a true underlying switching system. This uncertainty results in piecewise
residuals for each switching surface, impacting the CLF and CBF constraints. We
introduce a batch multi-output Gaussian process (MOGP) framework to approximate
these piecewise residuals, thereby mitigating the adverse effects of
uncertainty. A particular structure of the covariance function enables us to
convert the MOGP-based chance constraints CLF and CBF into second-order cone
constraints, which leads to a convex optimization. We analyze the feasibility
of the resulting optimization and provide the necessary and sufficient
conditions for feasibility. The effectiveness of the proposed strategy is
validated through a simulation of a switching adaptive cruise control system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2403.09573</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Constructive Method for Designing Safe Multirate Controllers for
  Differentially-Flat Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Hardik Parwana, Ryan K. Cosner, Ugo Rosolia, Aaron D. Ames, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-rate control architecture that leverages fundamental
properties of differential flatness to synthesize controllers for
safety-critical nonlinear dynamical systems. We propose a two-layer
architecture, where the high-level generates reference trajectories using a
linear Model Predictive Controller, and the low-level tracks this reference
using a feedback controller. The novelty lies in how we couple these layers, to
achieve formal guarantees on recursive feasibility of the MPC problem, and
safety of the nonlinear system. Furthermore, using differential flatness, we
provide a constructive means to synthesize the multi-rate controller, thereby
removing the need to search for suitable Lyapunov or barrier functions, or to
approximately linearize/discretize nonlinear dynamics. We show the synthesized
controller is a convex optimization problem, making it amenable to real-time
implementations. The method is demonstrated experimentally on a ground rover
and a quadruped robotic system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, accepted at IEEE Control Systems Letters 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-Agent</span> Clarity-Aware Dynamic Coverage with Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Devansh R. Agrawal, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two algorithms for multi-agent dynamic coverage in
spatiotemporal environments, where the coverage algorithms are informed by the
method of data assimilation. In particular, we show that by considering the
information assimilation algorithm, here a Numerical Gaussian Process Kalman
Filter, the influence of measurements taken at one position on the uncertainty
of the estimate at another location can be computed. We use this relationship
to propose new coverage algorithms. Furthermore, we show that the controllers
naturally extend to the multi-agent context, allowing for a distributed-control
central-information paradigm for multi-agent coverage. Finally, we demonstrate
the algorithms through a realistic simulation of a team of UAVs collecting wind
data over a region in Austria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-Agent</span> Resilient Consensus under Intermittent Faulty and Malicious
  Transmissions (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarper Aydın, Orhan Eren Akgün, Stephanie Gil, Angelia Nedić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider the consensus problem in which legitimate agents
share their values over an undirected communication network in the presence of
malicious or faulty agents. Different from the previous works, we characterize
the conditions that generalize to several scenarios such as intermittent faulty
or malicious transmissions, based on trust observations. As the standard trust
aggregation approach based on a constant threshold fails to distinguish
intermittent malicious/faulty activity, we propose a new detection algorithm
utilizing time-varying thresholds and the random trust values available to
legitimate agents. Under these conditions, legitimate agents almost surely
determine their trusted neighborhood correctly with geometrically decaying
misclassification probabilities. We further prove that the consensus process
converges almost surely even in the presence of malicious agents. We also
derive the probabilistic bounds on the deviation from the nominal consensus
value that would have been achieved with no malicious agents in the system.
Numerical results verify the convergence among agents and exemplify the
deviation under different scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended Version of CDC '24 submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stealthy Deactivation of Safety Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Arnström, André M. H. Teixeira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety filters ensure that only safe control actions are executed. We propose
a simple and stealthy false-data injection attack for deactivating such safety
filters; in particular, we focus on deactivating safety filters that are based
on control-barrier functions. The attack injects false sensor measurements to
bias state estimates to the interior of a safety region, which makes the safety
filter accept unsafe control actions. To detect such attacks, we also propose a
detector that detects biases manufactured by the proposed attack policy, which
complements conventional detectors when safety filters are used. The proposed
attack policy and detector are illustrated on a double integrator example.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECC24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the Optimal Power Flow: Environment Design Matters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wolgast, Astrid Nieße
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)
emerges as a promising new approach. However, the RL-OPF literature is strongly
divided regarding the exact formulation of the OPF problem as an RL
environment. In this work, we collect and implement diverse environment design
decisions from the literature regarding training data, observation space,
episode definition, and reward function choice. In an experimental analysis, we
show the significant impact of these environment design options on RL-OPF
training performance. Further, we derive some first recommendations regarding
the choice of these design decisions. The created environment framework is
fully open-source and can serve as a benchmark for future research in the
RL-OPF field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Steering Feedback in Dynamic Driving Simulators: The Influence of
  Steering Wheel Vibration and Vehicle Motion Frequency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Böhle, Bernhard Schick, Steffen Müller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The validity of the subjective evaluation of steering feedback in driving
simulators is crucial for modern vehicle development. Although there are
established objective steering characteristics for the assessment of both
stationary and dynamic feedback behaviour, factors such as steering wheel
vibrations and vehicle body motion, particularly in high-frequency ranges,
present challenges in simulator fidelity. This work investigates the influence
of steering wheel vibration and vehicle body motion frequency content on the
subjective evaluation of steering feedback during closed-loop driving in a
dynamic driving simulator. A controlled subject study with 30 participants
consisting of a back-to-back comparison of a reference vehicle with an
electrical power steering system and three variants of its virtual
representation on a dynamic driving simulator was performed. Subjective
evaluation focused on the representation of road feedback in comparison to the
reference vehicle. The statistical analysis of subjective results show that
there is a significant influence of the frequency content of both steering
wheel torque and vehicle motion on the subjective evaluation of steering
feedback in a dynamic driving simulator. The results suggest an influence of
frequency content on the subjective evaluation quality of steering feedback
characteristics that are not associated with the dynamic feedback behaviour in
the context of established performance indicators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, 9 tables, submitted to the IEEE Transactions on
  Intelligent Vehicles</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Exponential Stabilization of Control-affine Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Zakwan, Liang Xu, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel learning-based approach for achieving exponential
stabilization of nonlinear control-affine systems. We leverage the Control
Contraction Metrics (CCMs) framework to co-synthesize Neural Contraction
Metrics (NCMs) and Neural Network (NN) controllers. First, we transform the
infinite-dimensional semi-definite program (SDP) for CCM computation into a
tractable inequality feasibility problem using element-wise bounds of
matrix-valued functions. The terms in the inequality can be efficiently
computed by our novel algorithms. Second, we propose a free parametrization of
NCMs guaranteeing positive definiteness and the satisfaction of a partial
differential equation, regardless of trainable parameters. Third, this
parametrization and the inequality condition enable the design of
contractivity-enforcing regularizers, which can be incorporated while designing
the NN controller for exponential stabilization of the underlying nonlinear
systems. Furthermore, when the training loss goes to zero, we provide formal
guarantees on verification of the NCM and the exponentional stabilization under
the NN controller. Finally, we validate our method through benchmark
experiments on set-point stabilization and increasing the region of attraction
of a locally pre-stabilized closed-loop system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted in CDC2024 for a possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A PAC-Bayesian Framework for Optimal Control with Stability Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahrokh Ghoddousi Boroujeni, Clara Lucía Galimberti, Andreas Krause, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost
function that averages out the random uncertainties affecting the dynamics of
nonlinear systems. For tractability reasons, this problem is typically
addressed by minimizing an empirical cost, which represents the average cost
across a finite dataset of sampled disturbances. However, this approach raises
the challenge of quantifying the control performance against out-of-sample
uncertainties. Particularly, in scenarios where the training dataset is small,
SNOC policies are prone to overfitting, resulting in significant discrepancies
between the empirical cost and the true cost, i.e., the average SNOC cost
incurred during control deployment. Therefore, establishing generalization
bounds on the true cost is crucial for ensuring reliability in real-world
applications. In this paper, we introduce a novel approach that leverages
PAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on
these bounds, we propose a new method for designing optimal controllers,
offering a principled way to incorporate prior knowledge into the synthesis
process, which aids in improving the control policy and mitigating overfitting.
Furthermore, by leveraging recent parametrizations of stabilizing controllers
for nonlinear systems, our framework inherently ensures closed-loop stability.
The effectiveness of our proposed method in incorporating prior knowledge and
combating overfitting is shown by designing neural network controllers for
tasks in cooperative robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Distributed Controllers with Port-Hamiltonian Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Zakwan, Giancarlo Ferrari-Trecate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controlling large-scale cyber-physical systems necessitates optimal
distributed policies, relying solely on local real-time data and limited
communication with neighboring agents. However, finding optimal controllers
remains challenging, even in seemingly simple scenarios. Parameterizing these
policies using Neural Networks (NNs) can deliver good performance, but their
sensitivity to small input changes can destabilize the closed-loop system. This
paper addresses this issue for a network of nonlinear dissipative systems.
Specifically, we leverage well-established port-Hamiltonian structures to
characterize deep distributed control policies with closed-loop stability
guarantees and a finite $\mathcal{L}_2$ gain, regardless of specific NN
parameters. This eliminates the need to constrain the parameters during
optimization and enables training with standard methods like stochastic
gradient descent. A numerical study on the consensus control of Kuramoto
oscillators demonstrates the effectiveness of the proposed controllers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is submitted in CDC2024 for a possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optical Flow Based Detection and Tracking of Moving Objects for
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        MReza Alipour Sormoli, Mehrdad Dianati, Sajjad Mozaffari, Roger woodman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate velocity estimation of surrounding moving objects and their
trajectories are critical elements of perception systems in
Automated/Autonomous Vehicles (AVs) with a direct impact on their safety. These
are non-trivial problems due to the diverse types and sizes of such objects and
their dynamic and random behaviour. Recent point cloud based solutions often
use Iterative Closest Point (ICP) techniques, which are known to have certain
limitations. For example, their computational costs are high due to their
iterative nature, and their estimation error often deteriorates as the relative
velocities of the target objects increase (>2 m/sec). Motivated by such
shortcomings, this paper first proposes a novel Detection and Tracking of
Moving Objects (DATMO) for AVs based on an optical flow technique, which is
proven to be computationally efficient and highly accurate for such problems.
\textcolor{black}{This is achieved by representing the driving scenario as a
vector field and applying vector calculus theories to ensure spatiotemporal
continuity.} We also report the results of a comprehensive performance
evaluation of the proposed DATMO technique, carried out in this study using
synthetic and real-world data. The results of this study demonstrate the
superiority of the proposed technique, compared to the DATMO techniques in the
literature, in terms of estimation accuracy and processing time in a wide range
of relative velocities of moving objects. Finally, we evaluate and discuss the
sensitivity of the estimation error of the proposed DATMO technique to various
system and environmental parameters, as well as the relative velocities of the
moving objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted as a regular paper in Transactions
  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Structural Non-commutativity in Affine Feedback of SISO Nonlinear
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkatesh G. S.
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The affine feedback connection of SISO nonlinear systems modeled by
Chen--Fliess series is shown to be a group action on the plant which is
isomorphic to the semi-direct product of shuffle and additive group of
non-commutative formal power series. The additive and multiplicative feedback
loops in an affine feedback connection are thus proven to be structurally
non-commutative. A flip in the order of these loops results in a net additive
feedback loop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to $26^{th}$ International Symposium on Mathematical Theory
  of Networks and Systems, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using quantum computers in control: interval matrix properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Schneider, Julian Berberich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing provides a powerful framework for tackling computational
problems that are classically intractable. The goal of this paper is to explore
the use of quantum computers for solving relevant problems in systems and
control theory. In the recent literature, different quantum algorithms have
been developed to tackle binary optimization, which plays an important role in
various control-theoretic problems. As a prototypical example, we consider the
verification of interval matrix properties such as non-singularity and
stability on a quantum computer. We present a quantum algorithm solving these
problems and we study its performance in simulation. Our results demonstrate
that quantum computers provide a promising tool for control whose applicability
to further computationally complex problems remains to be explored.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version, accepted for publication in Proc. European Control
  Conference (ECC), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prioritize Team Actions: <span class="highlight-title">Multi-Agent</span> Temporal Logic Task Planning with
  Ordering Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Ye, Jianing Zhao, Shaoyuan Li, Xiang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the problem of linear temporal logic (LTL) path
planning for multi-agent systems, introducing the new concept of \emph{ordering
constraints}. Specifically, we consider a generic objective function that is
defined for the path of each individual agent. The primary objective is to find
a global plan for the team of agents, ensuring they collectively meet the
specified LTL requirements. Simultaneously, we aim to maintain a pre-determined
order in the values of the objective function for each agent, which we refer to
as the ordering constraints. This new requirement stems from scenarios like
security-aware planning, where relative orders outweigh absolute values in
importance. We present an efficient algorithm to solve this problem, supported
by proofs of correctness that demonstrate the optimality of our solution.
Additionally, we provide a case study in security-aware path planning to
illustrate the practicality and effectiveness of our proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chattering Phenomena in Time-Optimal Control for High-Order
  Chain-of-Integrators Systems with Full State Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunan Wang, Chuxiong Hu, Zeyang Li, Yujie Lin, Shize Lin, Suqin He, Yu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal control for high-order chain-of-integrators systems with full
state constraints and arbitrary given terminal states remains an open and
challenging problem in optimal control theory domain. However, optimal
control's behaviors in high-order problems lack of precision characterization,
even where the existence of chattering phenomena remain unknown and overlooked.
This paper establishes a theoretical framework of chattering phenomena in the
problem, focusing on the uniqueness of state constraints inducing chattering,
the upper bound of switching times in an unconstrained arc during chattering,
and the convergence of states and costates to the chattering limit point. For
the first time, this paper proves the existence of chattering phenomena in the
problems. The chattering optimal control for 4th order problems with velocity
constraints is precisely solved, providing an approach to plan strictly
time-optimal snap-limited trajectories, while other cases of order $n\leq4$ are
proved to not allow chattering. The conclusions correct the longstanding
misconception in the industry regarding the time-optimality of S-shaped
trajectories with minimal switching times.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ultrafast <span class="highlight-title">Adaptive</span> Primary Frequency Tuning and Secondary Frequency
  Identification for S/S WPT system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Wei Han, Guangyu Yan, Bowang Zhang, Chunlin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic resonance wireless power transfer (WPT) technology is increasingly
being adopted across diverse applications. However, its effectiveness can be
significantly compromised by parameter shifts within the resonance network,
owing to its high system quality factor. Such shifts are inherent and
challenging to mitigate during the manufacturing process. In response, this
article introduces a rapid frequency tuning approach. Leveraging
switch-controlled capacitors (SCC) to adjust the resonance network and the
primary side's operating frequency, alongside a current zero-crossing detection
(ZCD) circuit for voltage-current phase determination, this method circumvents
the need for intricate knowledge of WPT system parameters. Moreover, it
obviates the necessity for inter-side communication for real-time
identification of the secondary side resonance frequency. The swift response of
SCC and two-step perturb-and-observe algorithm mitigate output disturbances,
thereby expediting the frequency tuning process. Experimental validation on a
200W Series-Series compensated WPT (SS-WPT) system demonstrates that the
proposed method achieves frequency recognition accuracy within 0.7kHz in less
than 1ms, increasing system efficiency up to 9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,16 figures,to be published in IEEE Transactions on
  Industrial Electronics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control
  via Spectral Method Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaolei Shen, Chiara Gabellieri, Antonio Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a model-based optimal boundary control design for an
aerial robotic system composed of a quadrotor carrying a flexible cable. The
whole system is modeled by partial differential equations (PDEs) combined with
boundary conditions described by ordinary differential equations (ODEs). The
proper orthogonal decomposition (POD) method is adopted to project the original
infinite-dimensional system on a subspace spanned by orthogonal basis
functions. Based on the reduced order model, nonlinear model predictive control
(NMPC) is implemented online to realize shape trajectory tracking of the
flexible cable in an optimal predictive fashion. The proposed reduced modeling
and optimal control paradigms are numerically verified against an accurate
high-dimensional FDM-based model in different scenarios and the controller's
superior performance is shown compared to an optimally tuned PID controller.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Stability for Multiagent Systems with Spatio-Temporally
  Correlated Packet Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17554v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17554v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Hespe, Adwait Datar, Herbert Werner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A problem with considering correlations in the analysis of multiagent system
with stochastic packet loss is that they induce dependencies between agents
that are otherwise decoupled, preventing the application of decomposition
methods required for efficient evaluation. To circumvent that issue, this paper
is proposing an approach based on analysing sets of networks with independent
communication links, only considering the correlations in an implicit fashion.
Combining ideas from the robust stabilization of Markov jump linear systems
with recently proposed techniques for analysing packet loss in multiagent
systems, we obtain a linear matrix inequality based stability condition which
is independent of the number of agents. The main result is that the set of
stabilized probability distributions has non-empty interior such that small
correlations cannot lead to instability, even though only distributions of
independent links were analysed. Moreover, two examples are provided to
demonstrate the applicability of the results to practically relevant scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cyclic pursuit formation control for arbitrary desired shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Fujioka, Masaki Ogura, Naoki Wakamiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A multi-agent system comprises numerous agents that autonomously make
decisions to collectively accomplish tasks, drawing significant attention for
their wide-ranging applications. Within this context, formation control emerges
as a prominent task, wherein agents collaboratively shape and maneuver while
preserving formation integrity. Our focus centers on cyclic pursuit, a method
facilitating the formation of circles, ellipses, and figure-eights under the
assumption that agents can only perceive the relative positions of those
preceding them. However, this method's scope has been restricted to these
specific shapes, leaving the feasibility of forming other shapes uncertain. In
response, our study proposes a novel method based on cyclic pursuit capable of
forming a broader array of shapes, enabling agents to individually shape while
pursuing preceding agents, thereby extending the repertoire of achievable
formations. We present two scenarios concerning the information available to
agents and devise formation control methods tailored to each scenario. Through
extensive simulations, we demonstrate the efficacy of our proposed method in
forming multiple shapes, including those represented as Fourier series, thereby
underscoring the versatility and effectiveness of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Kron reduction of Multi-phase Radial Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven H. Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of identifying the admittance matrix of a three-phase
radial network from voltage and current measurements at a subset of nodes.
These measurements are used to estimate a virtual network represented by the
Kron reduction (Schur complement) of the full admittance matrix. We focus on
recovering exactly the full admittance matrix from its Kron reduction, i.e.,
computing the inverse of Schur complement. The key idea is to decompose Kron
reduction into a sequence of iterations that maintains an invariance structure,
and exploit this structure to reverse each step of the iterative Kron
reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Moreau Envelope Approach for LQR Meta-Policy Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashwin Aravind, Mohammad Taha Toghani, César A. Uribe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of policy estimation for the Linear Quadratic Regulator
(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We
propose a Moreau Envelope-based surrogate LQR cost, built from a finite set of
realizations of the uncertain system, to define a meta-policy efficiently
adjustable to new realizations. Moreover, we design an algorithm to find an
approximate first-order stationary point of the meta-LQR cost function.
Numerical results show that the proposed approach outperforms naive averaging
of controllers on new realizations of the linear system. We also provide
empirical evidence that our method has better sample complexity than
Model-Agnostic Meta-Learning (MAML) approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Reinforcement Learning</span>-based Receding Horizon Control using <span class="highlight-title">Adaptive</span>
  Control Barrier Functions for Safety-Critical Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Sabouni, H. M. Sabbir Ahmad, Vittorio Giammarino, Christos G. Cassandras, Ioannis Ch. Paschalidis, Wenchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal control methods provide solutions to safety-critical problems but
easily become intractable. Control Barrier Functions (CBFs) have emerged as a
popular technique that facilitates their solution by provably guaranteeing
safety, through their forward invariance property, at the expense of some
performance loss. This approach involves defining a performance objective
alongside CBF-based safety constraints that must always be enforced.
Unfortunately, both performance and solution feasibility can be significantly
impacted by two key factors: (i) the selection of the cost function and
associated parameters, and (ii) the calibration of parameters within the
CBF-based constraints, which capture the trade-off between performance and
conservativeness. %as well as infeasibility. To address these challenges, we
propose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC)
approach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In
particular, we parameterize our controller and use bilevel optimization, where
RL is used to learn the optimal parameters while MPC computes the optimal
control input. We validate our method by applying it to the challenging
automated merging control problem for Connected and Automated Vehicles (CAVs)
at conflicting roadways. Results demonstrate improved performance and a
significant reduction in the number of infeasible cases compared to traditional
heuristic approaches used for tuning CBF-based controllers, showcasing the
effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Destination-Constrained Linear Dynamical System Modeling in Set-Valued
  Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaowei Yang, Haiqi Liu, Fanqin Meng, Xiaojing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directional motion towards a specified destination is a common occurrence in
physical processes and human societal activities. Utilizing this prior
information can significantly improve the control and predictive performance of
system models. This paper primarily focuses on reconstructing linear dynamic
system models based on destination constraints in the set-valued framework. We
treat destination constraints as inherent information in the state evolution
process and employ convex optimization techniques to construct a coherent and
robust state model. This refined model effectively captures the impact of
destination constraints on the state evolution at each time step. Furthermore,
we design an optimal weight matrix for the reconstructed model to ensure
smoother and more natural trajectories of state evolution. We also analyze the
theoretical guarantee of optimality for this weight matrix and the properties
of the reconstructed model. Finally, simulation experiments verify that the
reconstructed model has significant advantages over the unconstrained and
unoptimized weighted models and constrains the evolution of state trajectories
with different starting and ending points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional Inductive Invariant Based Verification of Neural Network
  Controlled Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhou, Stavros Tripakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of neural networks into safety-critical systems has shown
great potential in recent years. However, the challenge of effectively
verifying the safety of Neural Network Controlled Systems (NNCS) persists. This
paper introduces a novel approach to NNCS safety verification, leveraging the
inductive invariant method. Verifying the inductiveness of a candidate
inductive invariant in the context of NNCS is hard because of the scale and
nonlinearity of neural networks. Our compositional method makes this
verification process manageable by decomposing the inductiveness proof
obligation into smaller, more tractable subproblems. Alongside the high-level
method, we present an algorithm capable of automatically verifying the
inductiveness of given candidates by automatically inferring the necessary
decomposition predicates. The algorithm significantly outperforms the baseline
method and shows remarkable reductions in execution time in our case studies,
shortening the verification time from hours (or timeout) to seconds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Control of Grid-Interfacing Inverters With Current Magnitude
  Limits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trager Joswig-Jones, Baosen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grid-interfacing inverters act as the interface between renewable resources
and the electric grid, and have the potential to offer fast and programmable
controls compared to synchronous generators. With this flexibility there has
been significant research efforts into determining the best way to control
these inverters. Inverters are limited in their maximum current output in order
to protect semiconductor devices, presenting a nonlinear constraint that needs
to be accounted for in their control algorithms. Existing approaches either
simply saturate a controller that is designed for unconstrained systems, or
assume small perturbations and linearize a saturated system. These approaches
can lead to stability issues or limiting the control actions to be too
conservative.
  In this paper, we directly focus on a nonlinear system that explicitly
accounts for the saturation of the current magnitude. We use a Lyapunov
stability approach to determine a stability condition for the system,
guaranteeing that a class of controllers would be stabilizing if they satisfy a
simple SDP condition. With this condition we fit a linear-feedback controller
by sampling the output (offline) model predictive control problems. This
learned controller has improved performances with existing designs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 1 table. Submitted to CDC'2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Run-to-Run Feedforward Control of Electromechanical Switching
  Devices: a Sensitivity-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03300v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03300v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edgar Ramirez-Laboreo, Eduardo Moya-Lasheras, Eloy Serrano-Seco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electromechanical switching devices, such as solenoid valves, contactors, and
relays, suffer from undesirable phenomena like clicking, mechanical wear, and
contact bounce. Despite that, they are still widely used in industry due to
their various economic and technical advantages. This has encouraged the
development of controllers aimed at reducing the collisions that occur at the
end of the switching operations. One of the most successful approaches has been
the use of iterative techniques. However, these algorithms typically require a
large number of operations to converge, which is definitely a clear drawback.
This paper presents a strategy to improve the convergence rate of such
controllers. Our proposal, which is based on the sensitivity of the control law
with respect to the parameters, assumes that the performance of the system is
more heavily affected by some parameters than others. Thus, by avoiding
movements in the directions that have less impact, the search algorithm is
expected to drive the system to near-optimal behaviors using fewer operations.
Results obtained by simulation show significant improvement in the convergence
rate of a state-of-the-art run-to-run feedforward controller, which
demonstrates the high potential of the proposal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures. Final version, after peer review and acceptance,
  submitted to the 22nd European Control Conference (ECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Resilient source seeking with robot swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Acuaviva, Jesus Bautista, Weijia Yao, Juan Jimenez, Hector Garcia de Marina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a solution for locating the source, or maximum, of an unknown
scalar field using a swarm of mobile robots. Unlike relying on the traditional
gradient information, the swarm determines an ascending direction to approach
the source with arbitrary precision. The ascending direction is calculated from
measurements of the field strength at the robot locations and their relative
positions concerning the centroid. Rather than focusing on individual robots,
we focus the analysis on the density of robots per unit area to guarantee a
more resilient swarm, i.e., the functionality remains even if individuals go
missing or are misplaced during the mission. We reinforce the robustness of the
algorithm by providing sufficient conditions for the swarm shape so that the
ascending direction is almost parallel to the gradient. The swarm can respond
to an unexpected environment by morphing its shape and exploiting the existence
of multiple ascending directions. Finally, we validate our approach numerically
with hundreds of robots. The fact that a large number of robots always
calculate an ascending direction compensates for the loss of individuals and
mitigates issues arising from the actuator and sensor noises.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, submitted to CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency Regulation with Storage: On Losses and Profits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Lauinger, François Vuille, Daniel Kuhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-carbon societies will need to store vast amounts of electricity to
balance intermittent generation from wind and solar energy, for example,
through frequency regulation. Here, we derive an analytical solution to the
decision-making problem of storage operators who sell frequency regulation
power to grid operators and trade electricity on day-ahead markets.
Mathematically, we treat future frequency deviation trajectories as functional
uncertainties in a receding horizon robust optimization problem. We constrain
the expected terminal state-of-charge to be equal to some target to allow
storage operators to make good decisions not only for the present but also the
future. Thanks to this constraint, the amount of electricity traded on
day-ahead markets is an implicit function of the regulation power sold to grid
operators. The implicit function quantifies the amount of power that needs to
be purchased to cover the expected energy loss that results from providing
frequency regulation. We show how the marginal cost associated with the
expected energy loss decreases with roundtrip efficiency and increases with
frequency deviation dispersion. We find that the profits from frequency
regulation over the lifetime of energy-constrained storage devices are roughly
inversely proportional to the length of time for which regulation power must be
committed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02099v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02099v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruya Karagulle, Nikos Arechiga, Andrew Best, Jonathan DeCastro, Necmiye Ozay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a preference learning method that ensures adherence to
given specifications, with an application to autonomous vehicles. Our approach
incorporates the priority ordering of Signal Temporal Logic (STL) formulas
describing traffic rules into a learning framework. By leveraging Parametric
Weighted Signal Temporal Logic (PWSTL), we formulate the problem of
safety-guaranteed preference learning based on pairwise comparisons and propose
an approach to solve this learning problem. Our approach finds a feasible
valuation for the weights of the given PWSTL formula such that, with these
weights, preferred signals have weighted quantitative satisfaction measures
greater than their non-preferred counterparts. The feasible valuation of
weights given by our approach leads to a weighted STL formula that can be used
in correct-and-custom-by-construction controller synthesis. We demonstrate the
performance of our method with a pilot human subject study in two different
simulated driving scenarios involving a stop sign and a pedestrian crossing.
Our approach yields competitive results compared to existing preference
learning methods in terms of capturing preferences and notably outperforms them
when safety is considered.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 2 tables. This work has been published at IEEE
  Robotics and Automation Letters. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the Gap to Quadratic Invariance: a Regret Minimization Approach
  to Optimal Distributed Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.02068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.02068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniele Martinelli, Andrea Martin, Giancarlo Ferrari-Trecate, Luca Furieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we focus on the design of optimal controllers that must comply
with an information structure. State-of-the-art approaches do so based on the
H2 or Hinfty norm to minimize the expected or worst-case cost in the presence
of stochastic or adversarial disturbances. Large-scale systems often experience
a combination of stochastic and deterministic disruptions (e.g., sensor
failures, environmental fluctuations) that spread across the system and are
difficult to model precisely, leading to sub-optimal closed-loop behaviors.
Hence, we propose improving performance for these scenarios by minimizing the
regret with respect to an ideal policy that complies with less stringent
sensor-information constraints. This endows our controller with the ability to
approach the improved behavior of a more informed policy, which would detect
and counteract heterogeneous and localized disturbances more promptly.
Specifically, we derive convex relaxations of the resulting regret minimization
problem that are compatible with any desired controller sparsity, while we
reveal a renewed role of the Quadratic Invariance (QI) condition in designing
informative benchmarks to measure regret. Last, we validate our proposed method
through numerical simulations on controlling a multi-agent distributed system,
comparing its performance with traditional H2 and Hinfty policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation and publication in the proceedings of the
  2024 European Control Conference (ECC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Optimal Control for High-Order Chain-of-Integrators Systems with
  Full State Constraints and Arbitrary Terminal States (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07039v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07039v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunan Wang, Chuxiong Hu, Zeyang Li, Shize Lin, Suqin He, Ze Wang, Yu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal control for high-order chain-of-integrators systems with full
state constraints and arbitrary given terminal states remains a challenging
problem in the optimal control theory domain, yet to be resolved. To enhance
further comprehension of the problem, this paper establishes a novel notation
system and theoretical framework, providing the switching manifold for
high-order problems in the form of switching laws. Through deriving properties
of switching laws on signs and dimension, this paper proposes a definite
condition for time-optimal control. Guided by the developed theory, a
trajectory planning method named the manifold-intercept method (MIM) is
developed. The proposed MIM can plan time-optimal jerk-limited trajectories
with full state constraints, and can also plan near-optimal non-chattering
higher-order trajectories with negligible extra motion time compared to optimal
profiles. Numerical results indicate that the proposed MIM outperforms all
baselines in computational time, computational accuracy, and trajectory quality
by a large gap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discretized Distributed Optimization over Dynamic Digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Doostmohammadian, Wei Jiang, Muwahida Liaquat, Alireza Aghasi, Houman Zarrabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a discrete-time model of continuous-time distributed optimization
over dynamic directed-graphs (digraphs) with applications to distributed
learning. Our optimization algorithm works over general strongly connected
dynamic networks under switching topologies, e.g., in mobile multi-agent
systems and volatile networks due to link failures. Compared to many existing
lines of work, there is no need for bi-stochastic weight designs on the links.
The existing literature mostly needs the link weights to be stochastic using
specific weight-design algorithms needed both at the initialization and at all
times when the topology of the network changes. This paper eliminates the need
for such algorithms and paves the way for distributed optimization over
time-varying digraphs. We derive the bound on the gradient-tracking step-size
and discrete time-step for convergence and prove dynamic stability using
arguments from consensus algorithms, matrix perturbation theory, and Lyapunov
theory. This work, particularly, is an improvement over existing
stochastic-weight undirected networks in case of link removal or packet drops.
This is because the existing literature may need to rerun time-consuming and
computationally complex algorithms for stochastic design, while the proposed
strategy works as long as the underlying network is weight-symmetric and
balanced. The proposed optimization framework finds applications to distributed
classification and learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Robotics Meets Wireless Communications: An Introductory Tutorial 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.02021v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.02021v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles
(UAVs) within the research community, industry, and society is growing fast.
Many of these agents are nowadays equipped with communication systems that are,
in some cases, essential to successfully achieve certain tasks. In this
context, we have begun to witness the development of a new interdisciplinary
research field at the intersection of robotics and communications. This
research field has been boosted by the intention of integrating UAVs within the
5G and 6G communication networks. This research will undoubtedly lead to many
important applications in the near future. Nevertheless, one of the main
obstacles to the development of this research area is that most researchers
address these problems by oversimplifying either the robotics or the
communications aspect. This impedes the ability of reaching the full potential
of this new interdisciplinary research area. In this tutorial, we present some
of the modelling tools necessary to address problems involving both robotics
and communication from an interdisciplinary perspective. As an illustrative
example of such problems, we focus in this tutorial on the issue of
communication-aware trajectory planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 192 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Linear Subspace Identification: A Machine Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Loris Di Natale, Muhammad Zakwan, Bratislav Svetozarevic, Philipp Heer, Giancarlo Ferrari-Trecate, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) and linear System Identification (SI) have been
historically developed independently. In this paper, we leverage
well-established ML tools - especially the automatic differentiation framework
- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space
SI methods using backpropagation. SIMBa relies on a novel
Linear-Matrix-Inequality-based free parametrization of Schur matrices to ensure
the stability of the identified model.
  We show how SIMBa generally outperforms traditional linear state-space SI
methods, and sometimes significantly, although at the price of a higher
computational burden. This performance gap is particularly remarkable compared
to other SI methods with stability guarantees, where the gain is frequently
above 25% in our investigations, hinting at SIMBa's ability to simultaneously
achieve state-of-the-art fitting performance and enforce stability.
Interestingly, these observations hold for a wide variety of input-output
systems and on both simulated and real-world data, showcasing the flexibility
of the proposed approach. We postulate that this new SI paradigm presents a
great extension potential to identify structured nonlinear models from data,
and we hence open-source SIMBa on https://github.com/Cemempamoi/simba.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modelling Irrational Behaviour of Residential End Users using
  Non-Stationary Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nam Trong Dinh, Sahand Karimi-Arpanahi, Rui Yuan, S. Ali Pourmousavi, Mingyu Guo, Jon A. R. Liisberg, Julian Lemos-Vinasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demand response (DR) plays a critical role in ensuring efficient electricity
consumption and optimal use of network assets. Yet, existing DR models often
overlook a crucial element, the irrational behaviour of electricity end users.
In this work, we propose a price-responsive model that incorporates key aspects
of end-user irrationality, specifically loss aversion, time inconsistency, and
bounded rationality. To this end, we first develop a framework that uses
Multiple Seasonal-Trend decomposition using Loess (MSTL) and non-stationary
Gaussian processes to model the randomness in the electricity consumption by
residential consumers. The impact of this model is then evaluated through a
community battery storage (CBS) business model. Additionally, we apply a
chance-constrained optimisation model for CBS operation that deals with the
unpredictability of the end-user irrationality. Our simulations using
real-world data show that the proposed DR model provides a more realistic
estimate of end-user price-responsive behaviour when considering irrationality.
Compared to a deterministic model that cannot fully take into account the
irrational behaviour of end users, the chance-constrained CBS operation model
yields an additional 19% revenue. Lastly, the business model reduces the
electricity costs of solar end users by 11%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript has been accepted for publication in IEEE
  Transactions on Smart Grid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Full Attitude Intelligent Controller Design of a Heliquad under Complete
  Failure of an Actuator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.07529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.07529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eeshan Kulkarni, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we design a reliable Heliquad and develop an intelligent
controller to handle one actuators complete failure. Heliquad is a multi-copter
similar to Quadcopter, with four actuators diagonally symmetric from the
center. Each actuator has two control inputs; the first input changes the
propeller blades collective pitch (also called variable pitch), and the other
input changes the rotation speed. For reliable operation and high torque
characteristic requirement for yaw control, a cambered airfoil is used to
design propeller blades. A neural network-based control allocation is designed
to provide complete control authority even under a complete loss of one
actuator. Nonlinear quaternion based outer loop position control, with
proportional-derivative inner loop for attitude control and neural
network-based control allocation is used in controller design. The proposed
controller and Heliquad designs performance is evaluated using a
software-in-loop simulation to track the position reference command under
failure. The results clearly indicate that the Heliquad with an intelligent
controller provides necessary tracking performance even under a complete loss
of one actuator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, For video go to
  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Hook-Based Grasping and Transportation with Quadcopters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02444v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02444v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Péter Antal, Tamás Péni, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Payload grasping and transportation with quadcopters is an active research
area that has rapidly developed over the last decade. To grasp a payload
without human interaction, most state-of-the-art approaches apply robotic arms
that are attached to the quadcopter body. However, due to the large weight and
power consumption of these aerial manipulators, their agility and flight time
are limited. This paper proposes a motion control and planning method for
transportation with a lightweight, passive manipulator structure that consists
of a hook attached to a quadrotor using a 1 DoF revolute joint. To perform
payload grasping, transportation, and release, first, time-optimal reference
trajectories are designed through specific waypoints to ensure the fast and
reliable execution of the tasks. Then, a two-stage motion control approach is
developed based on a robust geometric controller for precise and reliable
reference tracking and a linear--quadratic payload regulator for rapid setpoint
stabilization of the payload swing. Furthermore, stability of the closed-loop
system is mathematically proven to give safety guarantee for its operation. The
proposed control architecture and design are evaluated in a high-fidelity
physical simulator, and also in real flight experiments, using a custom-made
quadrotor--hook manipulator platform.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness Evaluation of Localization Techniques for Autonomous Racing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.07658v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.07658v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Yi Lim, Edoardo Ghignone, Nicolas Baumann, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SynPF, an MCL-based algorithm tailored for high-speed
racing environments. Benchmarked against Cartographer, a state-of-the-art
pose-graph SLAM algorithm, SynPF leverages synergies from previous
particle-filtering methods and synthesizes them for the high-performance racing
domain. Our extensive in-field evaluations reveal that while Cartographer
excels under nominal conditions, it struggles when subjected to wheel-slip, a
common phenomenon in a racing scenario due to varying grip levels and
aggressive driving behaviour. Conversely, SynPF demonstrates robustness in
these challenging conditions and a low-latency computation time of 1.25 ms on
on-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled
autonomous racing vehicle, this work not only highlights the vulnerabilities of
existing algorithms in high-speed scenarios, tested up until 7.6 m/s, but also
emphasizes the potential of SynPF as a viable alternative, especially in
deteriorating odometry conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Design, Automation and Test in Europe Conference 2024
  as an extended abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credit vs. Discount-Based Congestion Pricing: A Comparison Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chih-Yuan Chiu, Devansh Jalota, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tolling, or congestion pricing, offers a promising traffic management policy
for regulating congestion, but has also attracted criticism for placing
outsized financial burdens on low-income users. Credit-based congestion pricing
(CBCP) and discount-based congestion pricing (DBCP) policies, which
respectively provide travel credits and toll discounts to low-income users on
tolled roads, have emerged as promising mechanisms for reducing traffic
congestion without worsening societal inequities. However, the optimal design
of CBCP and DBCP policies, as well as their relative advantages and
disadvantages, remain poorly understood. To address this, we study the effects
of implementing CBCP and DBCP policies to route users on a network of
multi-lane highways with tolled express lanes. We formulate a non-atomic
routing game framework in which a subset of eligible users is granted toll
relief in the form of a fixed budget or toll discount, while the remaining
ineligible users must pay out-of-pocket. We prove the existence of Nash
equilibrium traffic flow patterns corresponding to any given CBCP or DBCP
policy. Under the additional assumption that eligible users have time-invariant
VoTs, we provide a convex program to efficiently compute these equilibria. For
networks consisting of a single edge, we identify conditions under which CBCP
policies outperform DBCP policies (and vice versa), in the sense of improving
eligible users' access to the express lane. Finally, we present empirical
results from a CBCP pilot study of the San Mateo 101 Express Lane Project in
California. Our empirical results corroborate our theoretical analysis of the
impact of deploying credit-based and discount-based policies, and lend insights
into the sensitivity of their impact with respect to the travel demand and
users' VoTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achievable Sum Rate Optimization on NOMA-aided Cell-Free Massive MIMO
  with Finite Blocklength Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baolin Chong, Hancheng Lu, Yuang Chen, Langtian Qin, Fengqian Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-orthogonal multiple access (NOMA)-aided cell-free massive multiple-input
multiple-output (CFmMIMO) has been considered as a promising technology to
fulfill strict quality of service requirements for ultra-reliable low-latency
communications (URLLC). However, finite blocklength coding (FBC) in URLLC makes
it challenging to achieve the optimal performance in the NOMA-aided CFmMIMO
system. In this paper, we investigate the performance of the NOMA-aided CFmMIMO
system with FBC in terms of achievable sum rate (ASR). Firstly, we derive a
lower bound (LB) on the ergodic data rate. Then, we formulate an ASR
maximization problem by jointly considering power allocation and user equipment
(UE) clustering. To tackle such an intractable problem, we decompose it into
two sub-problems, i.e., the power allocation problem and the UE clustering
problem. A successive convex approximation (SCA) algorithm is proposed to solve
the power allocation problem by transforming it into a series of geometric
programming problems. Meanwhile, two algorithms based on graph theory are
proposed to solve the UE clustering problem by identifying negative loops.
Finally, alternative optimization is performed to find the maximum ASR of the
NOMA-aided CFmMIMO system with FBC. The simulation results demonstrate that the
proposed algorithms significantly outperform the benchmark algorithms in terms
of ASR under various scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal
  Propagation Analysis for Large Language Models <span class="chip">ICLR
  2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma, Harris Teague
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large generative models, such as large language models (LLMs) and diffusion
models have as revolutionized the fields of NLP and computer vision
respectively. However, their slow inference, high computation and memory
requirement makes it challenging to deploy them on edge devices. In this study,
we propose a light-weight quantization aware fine tuning technique using
knowledge distillation (KD-QAT) to improve the performance of 4-bit weight
quantized LLMs using commonly available datasets to realize a popular language
use case, on device chat applications. To improve this paradigm of finetuning,
as main contributions, we provide insights into stability of KD-QAT by
empirically studying the gradient propagation during training to better
understand the vulnerabilities of KD-QAT based approaches to low-bit
quantization errors. Based on our insights, we propose ov-freeze, a simple
technique to stabilize the KD-QAT process. Finally, we experiment with the
popular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that
ov-freeze results in near float-point precision performance, i.e., less than
0.7% loss of accuracy on Commonsense Reasoning benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Practical ML for Low Resource Settings Workshop at ICLR
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models Produce Responses Perceived to be Empathic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kyung Lee, Jina Suh, Hongli Zhan, Junyi Jessy Li, Desmond C. Ong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated surprising performance on many
tasks, including writing supportive messages that display empathy. Here, we had
these models generate empathic messages in response to posts describing common
life experiences, such as workplace situations, parenting, relationships, and
other anxiety- and anger-eliciting situations. Across two studies (N=192, 202),
we showed human raters a variety of responses written by several models (GPT4
Turbo, Llama2, and Mistral), and had people rate these responses on how
empathic they seemed to be. We found that LLM-generated responses were
consistently rated as more empathic than human-written responses. Linguistic
analyses also show that these models write in distinct, predictable ``styles",
in terms of their use of punctuation, emojis, and certain words. These results
highlight the potential of using LLMs to enhance human peer support in contexts
where empathy is important.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution <span class="chip">ICAPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Feng, Adittyo Paul, Zhe Chen, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One area of research in multi-agent path finding is to determine how
replanning can be efficiently achieved in the case of agents being delayed
during execution. One option is to reschedule the passing order of agents,
i.e., the sequence in which agents visit the same location. In response, we
propose Switchable-Edge Search (SES), an A*-style algorithm designed to find
optimal passing orders. We prove the optimality of SES and evaluate its
efficiency via simulations. The best variant of SES takes less than 1 second
for small- and medium-sized problems and runs up to 4 times faster than
baselines for large-sized problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICAPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Juru: Legal Brazilian Large Language Model from Reputable Sources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high computational cost associated with pretraining large language models
limits their research. Two strategies have emerged to address this issue:
domain specialization and pretraining with high-quality data. To explore these
strategies, we specialized the Sabi\'a-2 Small model with 1.9 billion unique
tokens from reputable Brazilian legal sources and conducted few-shot
evaluations on legal and general knowledge exams. Our model, Juru, demonstrates
the benefits of domain specialization with a reduced amount of pretraining
data. However, this specialization comes at the expense of degrading
performance in other knowledge areas within the same language. This study
contributes to the growing body of scientific evidence showing that pretraining
data selection may enhance the performance of large language models, enabling
the exploration of these models at a lower cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Securing GNNs: Explanation-Based Identification of Backdoored Training
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jane Downer, Ren Wang, Binghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet
they are vulnerable to backdoor attacks that can compromise their performance
and ethical application. The detection of these attacks is crucial for
maintaining the reliability and security of GNN classification tasks, but
effective detection techniques are lacking. Following an initial investigation,
we observed that while graph-level explanations can offer limited insights,
their effectiveness in detecting backdoor triggers is inconsistent and
incomplete. To bridge this gap, we extract and transform secondary outputs of
GNN explanation mechanisms, designing seven novel metrics that more effectively
detect backdoor attacks. Additionally, we develop an adaptive attack to
rigorously evaluate our approach. We test our method on multiple benchmark
datasets and examine its efficacy against various attack models. Our results
show that our method can achieve high detection performance, marking a
significant advancement in safeguarding GNNs against backdoor attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AE SemRL: Learning Semantic Association Rules with Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erkan Karabulut, Victoria Degeler, Paul Groth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Association Rule Mining (ARM) is the task of learning associations among data
features in the form of logical rules. Mining association rules from
high-dimensional numerical data, for example, time series data from a large
number of sensors in a smart environment, is a computationally intensive task.
In this study, we propose an Autoencoder-based approach to learn and extract
association rules from time series data (AE SemRL). Moreover, we argue that in
the presence of semantic information related to time series data sources,
semantics can facilitate learning generalizable and explainable association
rules. Despite enriching time series data with additional semantic features, AE
SemRL makes learning association rules from high-dimensional data feasible. Our
experiments show that semantic association rules can be extracted from a latent
representation created by an Autoencoder and this method has in the order of
hundreds of times faster execution time than state-of-the-art ARM approaches in
many scenarios. We believe that this study advances a new way of extracting
associations from representations and has the potential to inspire more
research in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation of data-free class-incremental learning algorithms by
  simulating future data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eva Feillet, Adrian Popescu, Céline Hudelot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Class-incremental learning deals with sequential data streams composed of
batches of classes. Various algorithms have been proposed to address the
challenging case where samples from past classes cannot be stored. However,
selecting an appropriate algorithm for a user-defined setting is an open
problem, as the relative performance of these algorithms depends on the
incremental settings. To solve this problem, we introduce an algorithm
recommendation method that simulates the future data stream. Given an initial
set of classes, it leverages generative models to simulate future classes from
the same visual domain. We evaluate recent algorithms on the simulated stream
and recommend the one which performs best in the user-defined incremental
setting. We illustrate the effectiveness of our method on three large datasets
using six algorithms and six incremental settings. Our method outperforms
competitive baselines, and performance is close to that of an oracle choosing
the best algorithm in each setting. This work contributes to facilitate the
practical deployment of incremental learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with
  Autoformalization <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM), such as Google's Minerva and OpenAI's GPT
families, are becoming increasingly capable of solving mathematical
quantitative reasoning problems. However, they still make unjustified logical
and computational errors in their reasoning steps and answers. In this paper,
we leverage the fact that if the training corpus of LLMs contained sufficiently
many examples of formal mathematics (e.g. in Isabelle, a formal theorem proving
environment), they can be prompted to translate i.e. autoformalize informal
mathematical statements into formal Isabelle code -- which can be verified
automatically for internal consistency. This provides a mechanism to
automatically reject solutions whose formalized versions are inconsistent
within themselves or with the formalized problem statement. We evaluate our
method on GSM8K, MATH and MultiArith datasets and demonstrate that our approach
provides a consistently better heuristic than vanilla majority voting -- the
previously best method to identify correct answers, by more than 12% on GSM8K.
In our experiments it improves results consistently across all datasets and LLM
model sizes. The code can be found at https://github.com/jinpz/dtv.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sabiá-2: A New Generation of Portuguese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.09887v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.09887v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira, Ramon Pires
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HIVE: Harnessing Human Feedback for Instructional Visual Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.09618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.09618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, Caiming Xiong, Ran Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating human feedback has been shown to be crucial to align text
generated by large language models to human preferences. We hypothesize that
state-of-the-art instructional image editing models, where outputs are
generated based on an input image and an editing instruction, could similarly
benefit from human feedback, as their outputs may not adhere to the correct
instructions and preferences of users. In this paper, we present a novel
framework to harness human feedback for instructional visual editing (HIVE).
Specifically, we collect human feedback on the edited images and learn a reward
function to capture the underlying user preferences. We then introduce scalable
diffusion model fine-tuning methods that can incorporate human preferences
based on the estimated reward. Besides, to mitigate the bias brought by the
limitation of data, we contribute a new 1M training dataset, a 3.6K reward
dataset for rewards learning, and a 1K evaluation dataset to boost the
performance of instructional image editing. We conduct extensive empirical
experiments quantitatively and qualitatively, showing that HIVE is favored over
previous state-of-the-art instructional image editing approaches by a large
margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In CVPR, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Batched Low-Rank Adaptation of Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeming Wen, Swarat Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning
foundation models by incorporating trainable low-rank matrices, thereby
reducing the number of trainable parameters. While LoRA offers numerous
advantages, its applicability for real-time serving to a diverse and global
user base is constrained by its incapability to handle multiple task-specific
adapters efficiently. This imposes a performance bottleneck in scenarios
requiring personalized, task-specific adaptations for each incoming request. To
mitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which
each input example in a minibatch can be associated with its unique low-rank
adaptation weights, allowing for efficient batching of heterogeneous requests.
We empirically demonstrate that FLoRA retains the performance merits of LoRA,
showcasing competitive results on the MultiPL-E code generation benchmark
spanning over 8 languages and a multilingual speech recognition task across 6
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shape Optimization of Geometrically Nonlinear Modal Coupling
  Coefficients: An Application to MEMS Gyroscopes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Schiwietz, Marian Hörsting, Eva Maria Weig, Matthias Wenzel, Peter Degenfeld-Schonburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro- and nanoelectromechanical system (MEMS and NEMS) resonators can
exhibit rich nonlinear dynamics as they are often operated at large amplitudes
with high quality factors and possess a high mode density with a variety of
nonlinear modal couplings. Their impact is strongly influenced by internal
resonance conditions and by the strength of the modal coupling coefficients. On
one hand, strong nonlinear couplings are of academic interest and promise novel
device concepts. On the other hand, however, they have the potential to disturb
the linear system behavior on which industrial devices such as gyroscopes and
micro mirrors are based on. In either case, being able to optimize the coupling
coefficients by design is certainly beneficial. A main source of nonlinear
modal couplings are geometric nonlinearities. In this work, we apply node-based
shape optimization to tune the geometrically nonlinear 3-wave coupling
coefficients of a MEMS gyroscope. We demonstrate that individual coupling
coefficients can be tuned over several orders of magnitude by shape
optimization, while satisfying typical constraints on manufacturability and
operability of the devices. The optimized designs contain unintuitive
geometrical features far away from any solution an experienced human MEMS or
NEMS designer could have thought of. Thus, this work demonstrates the power of
shape optimization for tailoring the complex nonlinear dynamic properties of
MEMS and NEMS resonators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Exact Solution for Allocating Car Parking Spaces on Campus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Oluwaseye Joel, Sawyerr A. Babatunde, Adewumi O. Aderemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All over the world, especially in the university environment, planning
managers and traffic engineers are constantly faced with the problem of
inadequate allocation of car parking spaces to demanded users. Users could
either prefer reserved parking spaces to unreserved parking spaces or vice
versa. This makes the campus parking manager to be faced with two basic problem
which are: the problem of allocating the actual number of available reserved
spaces to users without any conflict over the same parking space, and the
problem of determining the number of parking permit to be issued for parking
lot with unreserved spaces. Hence, an optimal or available solution to the
problem is required. This paper investigates a model for allocating car parking
spaces, adds a constraint to address the reserved parking policy in a
university environment and solves the parking allocation problem using an exact
solution method. The result obtained gives the value of the objective function
and the optimal allocation of users to each parking lot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An International Multidiscinary Conference on Research, Development
  and Practices in Science, Technology, Education, Arts, Management & the
  Social Science (iSTEAMS). Conference Centre, University of Ibandan, Nigeria.
  30 May - 01 June 2013</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh
  Transformer <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12467v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12467v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim, Kiseok Chang, Chang-Seung Woo, Ilho Kim, Seok-Woo Lee, Joon-Young Yang, Sooyoung Yoon, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many mesh-based graph neural network (GNN) models have been
proposed for modeling complex high-dimensional physical systems. Remarkable
achievements have been made in significantly reducing the solving time compared
to traditional numerical solvers. These methods are typically designed to i)
reduce the computational cost in solving physical dynamics and/or ii) propose
techniques to enhance the solution accuracy in fluid and rigid body dynamics.
However, it remains under-explored whether they are effective in addressing the
challenges of flexible body dynamics, where instantaneous collisions occur
within a very short timeframe. In this paper, we present Hierarchical Contact
Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn
long-range dependencies (occurred by collisions) among spatially distant
positions of a body -- two close positions in a higher-level mesh correspond to
two distant positions in a lower-level mesh. HCMT enables long-range
interactions, and the hierarchical mesh structure quickly propagates collision
effects to faraway positions. To this end, it consists of a contact mesh
Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively).
Lastly, we propose a flexible body dynamics dataset, consisting of trajectories
that reflect experimental settings frequently used in the display industry for
product designs. We also compare the performance of several baselines using
well-known benchmark datasets. Our results show that HCMT provides significant
performance improvements over existing methods. Our code is available at
https://github.com/yuyudeep/hcmt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-25T00:00:00Z">2024-03-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Operation of Reconfigurable Active Distribution Networks Aiming
  at Resiliency Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saeed Behzadi, Amir Bagheri, Abbas Rabiee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As natural disasters bring about power outage and financial losses, network
resiliency is an important challenge for distribution network operators (DNOs).
On the other side, power loss reduction during normal operating condition is a
major concern of DNOs. In this paper, optimal scheduling of active distribution
network (ADN) is addressed through simultaneous minimization of power loss in
normal condition and load shedding in critical condition after natural
disasters. A new formulation is developed for the network reconfiguration to
optimize the system operation in both normal and emergency conditions in the
presence of conventional and renewable-energy-based distributed generation (DG)
as well as energy storage systems (ESSs). The line flow based (LFB) algorithm
is used for the AC power flow calculations, and all the developed relations
have been convexified to construct a mixed-integer quadratically-constrained
programming (MIQCP) optimization model. The simulations have been implemented
on the IEEE 33-bus system in GAMS, and the results are investigated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact-Aware Bimanual Catching of Large-Momentum Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yan, Theodoros Stouraitis, João Moura, Wenfu Xu, Michael Gienger, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates one of the most challenging tasks in dynamic
manipulation -- catching large-momentum moving objects. Beyond the realm of
quasi-static manipulation, dealing with highly dynamic objects can
significantly improve the robot's capability of interacting with its
surrounding environment. Yet, the inevitable motion mismatch between the fast
moving object and the approaching robot will result in large impulsive forces,
which lead to the unstable contacts and irreversible damage to both the object
and the robot. To address the above problems, we propose an online optimization
framework to: 1) estimate and predict the linear and angular motion of the
object; 2) search and select the optimal contact locations across every surface
of the object to mitigate impact through sequential quadratic programming
(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and
contact force for both robots using multi-mode trajectory optimization (MMTO);
and 4) realise the impact-aware catching motion on the compliant robotic system
based on indirect force controller. We validate the impulse distribution,
contact selection, and impact-aware MMTO algorithms in simulation and
demonstrate the benefits of the proposed framework in real-world experiments
including catching large-momentum moving objects with well-defined motion,
constrained motion and free-flying motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DASA: Delay-<span class="highlight-title">Adaptive</span> <span class="highlight-title">Multi-Agent</span> Stochastic Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni, Aritra Mitra, George J. Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a setting in which $N$ agents aim to speedup a common Stochastic
Approximation (SA) problem by acting in parallel and communicating with a
central server. We assume that the up-link transmissions to the server are
subject to asynchronous and potentially unbounded time-varying delays. To
mitigate the effect of delays and stragglers while reaping the benefits of
distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm
for multi-agent Stochastic Approximation. We provide a finite-time analysis of
\texttt{DASA} assuming that the agents' stochastic observation processes are
independent Markov chains. Significantly advancing existing results,
\texttt{DASA} is the first algorithm whose convergence rate depends only on the
mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly
achieving an $N$-fold convergence speedup under Markovian sampling. Our work is
relevant for various SA applications, including multi-agent and distributed
temporal difference (TD) learning, Q-learning and stochastic optimization with
correlated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Discrete-Time Least-Squares <span class="highlight-title">Adaptive</span> State Tracking Control Scheme
  with A Mobile-Robot System Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianhong Zhao, Gang Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops an adaptive state tracking control scheme for
discrete-time systems, using the least-squares algorithm, as the new solution
to the long-standing discrete-time adaptive state tracking control problem to
which the Lyapunov method (well-developed for the continuous-time adaptive
state tracking problem) is not applicable. The new adaptive state tracking
scheme is based on a recently-developed new discrete-time error model which has
been used for gradient algorithm based state tracking control schemes, and uses
the least-squares algorithm for parameter adaptation. The new least-squares
algorithm is derived to minimize an accumulative estimation error, to ensure
certain optimality for parameter estimation. The system stability and output
tracking properties are studied. Technical results are presented in terms of
plant-model matching, error model, adaptive law, optimality formulation, and
stability and tracking analysis. The developed adaptive control scheme is
applied to a discrete-time multiple mobile robot system to meet an adaptive
state tracking objective. In addition, a collision avoidance mechanism is
proposed to prevent collisions in the whole tracking process. Simulation
results are presented, which verify the desired system state tracking
properties under the developed least-squares algorithm based adaptive control
scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling
  Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin S. Miller, Adam J. Thorpe, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an active learning algorithm for learning dynamics that leverages
side information by explicitly incorporating prior domain knowledge into the
sampling process. Our proposed algorithm guides the exploration toward regions
that demonstrate high empirical discrepancy between the observed data and an
imperfect prior model of the dynamics derived from side information. Through
numerical experiments, we demonstrate that this strategy explores regions of
high discrepancy and accelerates learning while simultaneously reducing model
uncertainty. We rigorously prove that our active learning algorithm yields a
consistent estimate of the underlying dynamics by providing an explicit rate of
convergence for the maximum predictive variance. We demonstrate the efficacy of
our approach on an under-actuated pendulum system and on the half-cheetah
MuJoCo environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ High-dimensional continuification control of large-scale <span class="highlight-title">multi-agent</span>
  systems under limited sensing and perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gian Carlo Maffettone, Mario di Bernardo, Maurizio Porfiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robustness of a novel high-dimensional
continuification control method for complex multi-agent systems. We begin by
formulating a partial differential equation describing the spatio-temporal
density dynamics of swarming agents. A stable control action for the density is
then derived and validated under nominal conditions. Subsequently, we
discretize this macroscopic strategy into actionable velocity inputs for the
system's agents. Our analysis demonstrates the robustness of the approach
beyond idealized assumptions of unlimited sensing and absence of perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2310.01573</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Finite-time Stabilization of Linear Systems with Limited State
  Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhou, Andrey Polyakov, Gang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the robust asymptotic stabilization of a linear
time-invariant (LTI) system by a static feedback with a static state
quantization. It is shown that the controllable LTI system can be stabilized to
zero in a finite time by means of a nonlinear feedback with a quantizer having
a limited (finite) number of values (quantization seeds) even when all
parameters of the controller and the quantizer are time-invariant. The control
design is based on generalized homogeneity. A homogeneous spherical quantizer
is introduced. The static homogeneous feedback is shown to be local (or global)
finite-time stabilizer for the linear system (dependently of the system
matrix). The tuning rules for both the quantizer and the feedback law are
obtained in the form of Linear Matrix Inequalities (LMIs). The closed-loop
system is proven to be robust with respect to some bounded matched and
vanishing mismatched perturbations. Theoretical results are supported by
numerical simulations. \
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Belief Samples Are All You Need For Social Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahyar JafariNodeh, Amir Ajorlou, Ali Jadbabaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of social learning, where a group of
agents embedded in a social network are interested in learning an underlying
state of the world. Agents have incomplete, noisy, and heterogeneous sources of
information, providing them with recurring private observations of the
underlying state of the world. Agents can share their learning experience with
their peers by taking actions observable to them, with values from a finite
feasible set of states. Actions can be interpreted as samples from the beliefs
which agents may form and update on what the true state of the world is.
Sharing samples, in place of full beliefs, is motivated by the limited
communication, cognitive, and information-processing resources available to
agents especially in large populations. Previous work (Salhab et al.) poses the
question as to whether learning with probability one is still achievable if
agents are only allowed to communicate samples from their beliefs. We provide a
definite positive answer to this question, assuming a strongly connected
network and a ``collective distinguishability'' assumption, which are both
required for learning even in full-belief-sharing settings. In our proposed
belief update mechanism, each agent's belief is a normalized weighted geometric
interpolation between a fully Bayesian private belief -- aggregating
information from the private source -- and an ensemble of empirical
distributions of the samples shared by her neighbors over time. By carefully
constructing asymptotic almost-sure lower/upper bounds on the frequency of
shared samples matching the true state/or not, we rigorously prove the
convergence of all the beliefs to the true state, with probability one.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG
  Direct Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spencer Kraisler, Mehran Mesbahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider direct policy optimization for the
linear-quadratic Gaussian (LQG) setting. Over the past few years, it has been
recognized that the landscape of stabilizing output-feedback controllers of
relevance to LQG has an intricate geometry, particularly as it pertains to the
existence of spurious stationary points. In order to address such challenges,
in this paper, we first adopt a Riemannian metric for the space of stabilizing
full-order minimal output-feedback controllers. We then proceed to prove that
the orbit of such controllers modulo coordinate transformation admits a
Riemannian quotient manifold structure. This geometric structure is then used
to develop a Riemannian gradient descent for the direct LQG policy
optimization. We prove a local convergence guarantee with linear rate and show
the proposed approach exhibits significantly faster and more robust numerical
performance as compared with ordinary gradient descent for LQG. Subsequently,
we provide reasons for this observed behavior; in particular, we argue that
optimizing over the orbit space of controllers is the right theoretical and
computational setup for direct LQG policy optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximation with Random Shallow ReLU Networks with Applications to
  Model Reference <span class="highlight-title">Adaptive</span> Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lamperski, Tyler Lekang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are regularly employed in adaptive control of nonlinear
systems and related methods o reinforcement learning. A common architecture
uses a neural network with a single hidden layer (i.e. a shallow network), in
which the weights and biases are fixed in advance and only the output layer is
trained. While classical results show that there exist neural networks of this
type that can approximate arbitrary continuous functions over bounded regions,
they are non-constructive, and the networks used in practice have no
approximation guarantees. Thus, the approximation properties required for
control with neural networks are assumed, rather than proved. In this paper, we
aim to fill this gap by showing that for sufficiently smooth functions, ReLU
networks with randomly generated weights and biases achieve $L_{\infty}$ error
of $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It
suffices to generate the weights uniformly over a sphere and the biases
uniformly over an interval. We show how the result can be used to get
approximations of required accuracy in a model reference adaptive control
application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review for Conference on Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Adaptive</span> Step Duration for Precise Foot Placement: Achieving Robust
  Bipedal Locomotion on Terrains with Restricted Footholds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Xiang, Victor Paredes, Ayonga Hereid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel multi-step preview foot placement planning
algorithm designed to enhance the robustness of bipedal robotic walking across
challenging terrains with restricted footholds. Traditional one-step preview
planning struggles to maintain stability when stepping areas are severely
limited, such as with random stepping stones. In this work, we developed a
discrete-time Model Predictive Control (MPC) based on the step-to-step discrete
evolution of the Divergent Component of Motion (DCM) of bipedal locomotion.
This approach adaptively changes the step duration for optimal foot placement
under constraints, thereby ensuring the robot's operational viability over
multiple future steps and significantly improving its ability to navigate
through environments with tight constraints on possible footholds. The
effectiveness of this planning algorithm is demonstrated through simulations
that include a variety of complex stepping-stone configurations and external
perturbations. These tests underscore the algorithm's improved performance for
navigating foothold-restricted environments, even with the presence of external
disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to CDC 2024, for associated simulation
  video, see https://youtu.be/2jhikPlZmbE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State-Augmented Linear Games with Antagonistic Error for
  High-Dimensional, Nonlinear Hamilton-Jacobi Reachability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Will Sharpless, Yat Tin Chow, Sylvia Herbert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton-Jacobi Reachability (HJR) is a popular method for analyzing the
liveness and safety of a dynamical system with bounded control and disturbance.
The corresponding HJ value function offers a robust controller and
characterizes the reachable sets, but is traditionally solved with Dynamic
Programming (DP) and limited to systems of dimension less than six. Recently,
the space-parallelizeable, generalized Hopf formula has been shown to also
solve the HJ value with a nearly three-log increase in dimension limit, but is
limited to linear systems. To extend this potential, we demonstrate how
state-augmented (SA) spaces, which are well-known for their improved
linearization accuracy, may be used to solve tighter, conservative
approximations of the value function with any linear model in this SA space.
Namely, we show that with a representation of the true dynamics in the SA
space, a series of inequalities confirms that the value of a SA linear game
with antagonistic error is a conservative envelope of the true value function.
It follows that if the optimal controller for the HJ SA linear game with error
may succeed, it will also succeed in the true system. Unlike previous methods,
this result offers the ability to safely approximate reachable sets and their
corresponding controllers with the Hopf formula in a non-convex manner.
Finally, we demonstrate this in the slow manifold system for clarity, and in
the controlled Van der Pol system with different lifting functions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Optimal Solution to Infinite Horizon Nonlinear Control Problems: Part
  II 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Naveed Gul Mohamed, Aayushman Sharma, Raman Goyal, Suman Chakravorty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the infinite horizon optimal control problem for
nonlinear systems. Under the condition of nonlinear controllability of the
system to any terminal set containing the origin and forward invariance of the
terminal set, we establish a regularized solution approach consisting of a
``finite free final time" optimal transfer problem to the terminal set which
renders the set globally asymptotically stable. Further, we show that the
approximations converge to the optimal infinite horizon cost as the size of the
terminal set decreases to zero. We also perform the analysis for the discounted
problem and show that the terminal set is asymptotically stable only for a
subset of the state space and not globally. The theory is empirically evaluated
on various nonholonomic robotic systems to show that the cost of our
approximate problem converges and the transfer time into the terminal set is
dependent on the initial state of the system, necessitating the free final time
formulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Methods for Trust in Collaborative <span class="highlight-title">Multi-Agent</span> Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        R. Spencer Hallyburton, Miroslav Pajic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent, collaborative sensor fusion is a vital component of a
multi-national intelligence toolkit. In safety-critical and/or contested
environments, adversaries may infiltrate and compromise a number of agents. We
analyze state of the art multi-target tracking algorithms under this
compromised agent threat model. We prove that the track existence probability
test ("track score") is significantly vulnerable to even small numbers of
adversaries. To add security awareness, we design a trust estimation framework
using hierarchical Bayesian updating. Our framework builds beliefs of trust on
tracks and agents by mapping sensor measurements to trust pseudomeasurements
(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case
studies, our trust estimation algorithm accurately estimates the
trustworthiness of tracks/agents, subject to observability limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via
  Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akua Dickson, Christos G. Cassandras, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an output feedback control-based motion planning technique for
agents to enable them to converge to a specified polynomial trajectory while
imposing a set of safety constraints on our controller to avoid collisions
within the free configuration space (polygonal environment). To achieve this,
we 1) decompose our polygonal environment into different overlapping cells 2)
write out our polynomial trajectories as the output of a reference dynamical
system with given initial conditions 3) formulate convergence and safety
constraints as Linear Matrix Inequalities (LMIs) on our controller using
Control Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) and 4)
solve a semi-definite programming (SDP) problem with convergence and safety
constraints imposed to synthesize a controller for each convex cell. Extensive
simulations are included to test our motion planning method under different
initial conditions and different reference trajectories. The synthesized
controller is robust to changes in initial conditions and is always safe
relative to the boundaries of the polygonal environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Space Models as Foundation Models: A Control Theoretic <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carmen Amo Alonso, Jerome Sieber, Melanie N. Zeilinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been a growing interest in integrating linear
state-space models (SSM) in deep neural network architectures of foundation
models. This is exemplified by the recent success of Mamba, showing better
performance than the state-of-the-art Transformer architectures in language
tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a
latent space in order to learn a compressed representation of the data. The
same goal has been pursued by control theorists using SSMs to efficiently model
dynamical systems. Therefore, SSMs can be naturally connected to deep sequence
modeling, offering the opportunity to create synergies between the
corresponding research areas. This paper is intended as a gentle introduction
to SSM-based architectures for control theorists and summarizes the latest
research developments. It provides a systematic review of the most successful
SSM proposals and highlights their main features from a control theoretic
perspective. Additionally, we present a comparative analysis of these models,
evaluating their performance on a standardized benchmark designed for assessing
a model's efficiency at learning long sequences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-Lagrangian Approach for Time and Energy Path Planning
  Optimization in Static Flow Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Víctor C. da S. Campos, Armando A. Neto, Douglas G. Macharet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient path planning for autonomous mobile robots is a critical problem
across numerous domains, where optimizing both time and energy consumption is
paramount. This paper introduces a novel methodology that considers the dynamic
influence of an environmental flow field and considers geometric constraints,
including obstacles and forbidden zones, enriching the complexity of the
planning problem. We formulate it as a multi-objective optimal control problem,
propose a novel transformation called Harmonic Transformation, and apply a
semi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is
obtained considering two distinct approaches: a deterministic method and an
evolutionary-based one, both of which are designed to make use of the proposed
Harmonic Transformation. Through an extensive analysis of these approaches, we
demonstrate their efficacy in finding optimized paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, initial paper submission; Preprint submitted to the IEEE
  Transactions on Intelligent Transportation Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic-Aware Remote Estimation of Multiple Markov Sources Under
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiping Luo, Nikolaos Pappas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies semantic-aware communication for remote estimation of
multiple Markov sources over a lossy and rate-constrained channel. Unlike most
existing studies that treat all source states equally, we exploit the semantics
of information and consider that the remote actuator has different tolerances
for the estimation errors of different states. We aim to find an optimal
scheduling policy that minimizes the long-term state-dependent costs of
estimation errors under a transmission frequency constraint. We theoretically
show the structure of the optimal policy by leveraging the average-cost
Constrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic
programming. By exploiting the optimal structural results, we develop a novel
policy search algorithm, termed intersection search plus relative value
iteration (Insec-RVI), that can find the optimal policy using only a few
iterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an
online low-complexity drift-plus-penalty (DPP) scheduling algorithm based on
the Lyapunov optimization theorem. We also design an efficient average-cost
Q-learning algorithm to estimate the optimal policy without knowing a priori
the channel and source statistics. Numerical results show that continuous
transmission is inefficient, and remarkably, our semantic-aware policies can
attain the optimum by strategically utilizing fewer transmissions by exploiting
the timing of the important information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Efficiency Optimization Method of WDM Visible Light Communication
  System for Indoor Broadcasting Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dayu Shi, Xun Zhang, Ziqi Liu, Xuanbang Chen, Jianghao Li, Xiaodong Liu, William Shieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel approach to optimize energy efficiency in
wavelength division multiplexing (WDM) Visible Light Communication (VLC)
systems designed for indoor broadcasting networks. A physics-based LED model is
integrated into system energy efficiency optimization, enabling quantitative
analysis of the critical issue of VLC energy efficiency: the nonlinear
interplay between illumination and communication performance. The optimization
jointly incorporates constraints on communication quality of each channel, and
illumination performance, standardized by the International Commission on
Illumination (CIE). The formulated nonlinear optimization problem is solved by
the Sequential Quadratic Programming (SQP) algorithm in an experiment-based
simulation. An integrated Red-Green-Blue-Yellow Light Emitting Diode (RGBY-LED)
is measured for model calibration and three different scenarios are simulated
to evaluate the generality of the proposed method. Results demonstrate a double
enhancement in performance and a high versatility in accommodating various
scenarios. Furthermore, it highlights the importance of balancing communication
and illumination imperatives in VLC systems, challenging conventional
perceptions focused solely on minimizing power consumption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A
  User-Centric Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Ji, Xiping Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets)
are an emerging indoor wireless communication paradigm, which combines the
advantages of the capacious optical spectra of LiFi and ubiquitous coverage of
WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource
management for such hybrid networks. The existing LB methods are mostly
network-centric, relying on a central unit to make a solution for the users all
at once. Consequently, the solution needs to be updated for all users at the
same pace, regardless of their moving status. This would affect the network
performance in two aspects: i) when the update frequency is low, it would
compromise the connectivity of fast-moving users; ii) when the update frequency
is high, it would cause unnecessary handovers as well as hefty feedback costs
for slow-moving users. Motivated by this, we investigate user-centric LB which
allows users to update their solutions at different paces. The research is
developed upon our previous work on adaptive target-condition neural network
(ATCNN), which can conduct LB for individual users in quasi-static channels. In
this paper, a deep neural network (DNN) model is designed to enable an adaptive
update interval for each individual user. This new model is termed as
mobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a
user-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is
proposed to handle resource management and mobility management simultaneously.
Results show that at the same level of average update interval, MS-ATCNN can
achieve a network throughput up to 215\% higher than conventional LB methods
such as game theory, especially for a larger number of users. In addition,
MS-ATCNN costs an ultra low runtime at the level of 100s $\mu$s, which is two
to three orders of magnitude lower than game theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 12 figures, 3 tables, submitted to IEEE TWC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scheduling Power-Intensive Operations of Battery Energy Storage Systems
  and Application to Hybrid Hydropower Plants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16821v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16821v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Cassano, Fabrizio Sossan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical schedulers for Battery Energy Storage Systems (BESSs) use static
power constraints, assuming that the BESS can provide the rated power at any
State-Of-Charge (SOC) level and that these are representative of the underlying
physical constraints of the system (BESS voltage and current). Static power
constraints, however, can generate unfeasible schedules, especially in
power-intensive applications, as demonstrated in this paper. This paper derives
a set of alternative constraints for the BESS power that are cognizant of the
physical limits of the BESS. It is shown that these constraints, developed by
leveraging an equivalent circuit model of the BESS, can be formulated as linear
inequalities in scheduling problems, thus leaving the properties of the
original problem (i.e., linearity and convexity) unaltered. A comparative
analysis against traditional schedulers from the literature shows significant
reductions in current violations and the generation of feasible schedules.
These findings underscore the crucial role of implementing more advanced power
constraints of BESSs in power-intensive applications, thereby enhancing the
reliability of BESS scheduling strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Yang, Marie Siew, Carlee Joe-Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing prevalence of Cyber-Physical Systems and the Internet of
Things (CPS-IoT) applications and Foundation Models are enabling new
applications that leverage real-time control of the environment. For example,
real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems
can reduce its usage when not needed for the comfort of human occupants, hence
reducing energy consumption. Collecting real-time feedback on human preferences
in such human-in-the-loop (HITL) systems, however, is difficult in practice. We
propose the use of large language models (LLMs) to deal with the challenges of
dynamic environments and difficult-to-obtain data in CPS optimization. In this
paper, we present a case study that employs LLM agents to mimic the behaviors
and thermal preferences of various population groups (e.g. young families, the
elderly) in a shopping mall. The aggregated thermal preferences are integrated
into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which
employs the LLM as a dynamic simulation of the physical environment to learn
how to balance between energy savings and occupant comfort. Our results show
that LLMs are capable of simulating complex population movements within large
open spaces. Besides, AitL-RL demonstrates superior performance compared to the
popular existing policy of set point control, suggesting that adaptive and
personalized decision-making is critical for efficient optimization in CPS-IoT
applications. Through this case study, we demonstrate the potential of
integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system
adaptability and efficiency. The project's code can be found on our GitHub
repository.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Workshop on Foundation Models for
  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at
  CPS-IoT Week 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Preservation by Intermittent Transmission in Cooperative LQG
  Control Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Lin, Yuqing Ni, Wen Yang, Chao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a cooperative linear quadratic Gaussian (LQG) control
system with a single user and a server. In this system, the user runs a process
and employs the server to meet the needs of computation. However, the user
regards its state trajectories as privacy. Therefore, we propose a privacy
scheme, in which the user sends data to the server intermittently. By this
scheme, the server's received information of the user is reduced, and
consequently the user's privacy is preserved. In this paper, we consider a
periodic transmission scheme. We analyze the performance of privacy
preservation and LQG control of different transmission periods. Under the given
threshold of the control performance loss, a trade-off optimization problem is
proposed. Finally, we give the solution to the optimization problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichuan Li, Junkai Zhao, Yixiao Li, Zheng Wu, Rui Cao, Masayoshi Tomizuka, Yunhui Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiency and reliability are critical in robotic bin-picking as they
directly impact the productivity of automated industrial processes. However,
traditional approaches, demanding static objects and fixed collisions, lead to
deployment limitations, operational inefficiencies, and process unreliability.
This paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges
traditional static assumptions. The DBPF endows the robot with the reactivity
to pick multiple moving arbitrary objects while avoiding dynamic obstacles,
such as the moving bin. Combined with scene-level pose generation, the proposed
pose selection metric leverages the Tendency-Aware Manipulability Network
optimizing suction pose determination. Heuristic task-specific designs like
velocity-matching, dynamic obstacle avoidance, and the resight policy, enhance
the picking success rate and reliability. Empirical experiments demonstrate the
importance of these components. Our method achieves an average 84% success
rate, surpassing the 60% of the most comparable baseline, crucially, with zero
collisions. Further evaluations under diverse dynamic scenarios showcase DBPF's
robust performance in dynamic bin-picking. Results suggest that our framework
offers a promising solution for efficient and reliable robotic bin-picking
under dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. This paper has been accepted by IEEE RA-L on
  2024-03-24. See the supplementary video at youtube:
  https://youtu.be/n5af2VsKhkg</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy Gradient-based Model Free Optimal LQG Control with a
  Probabilistic Risk Constraint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arunava Naha, Subhrakanti Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate a model-free optimal control design that
minimizes an infinite horizon average expected quadratic cost of states and
control actions subject to a probabilistic risk or chance constraint using
input-output data. In particular, we consider linear time-invariant systems and
design an optimal controller within the class of linear state feedback control.
Three different policy gradient (PG) based algorithms, natural policy gradient
(NPG), Gauss-Newton policy gradient (GNPG), and deep deterministic policy
gradient (DDPG), are developed, and compared with the optimal risk-neutral
linear-quadratic regulator (LQR) and a scenario-based model predictive control
(MPC) technique via numerical simulations. The convergence properties and the
accuracy of all the algorithms are compared numerically. We also establish
analytical convergence properties of the NPG and GNPG algorithms under the
known model scenario, while the proof of convergence for the unknown model
scenario is part of our ongoing work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE CDC2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Blotto Game Approach to Ride-hailing Markets with Electric Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16755v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16755v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marko Maljkovic, Gustav Nilsson, Nikolas Geroliminis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When a centrally operated ride-hailing company considers to enter a market
already served by another company, it has to make a strategic decision about
how to distribute its fleet among different regions in the area. This decision
will be influenced by the market share the company can secure and the costs
associated with charging the vehicles in each region, all while competing with
the company already operating in the area. In this paper, we propose a Colonel
Blotto-like game to model this decision-making. For the class of games that we
study, we first prove the existence and uniqueness of a Nash Equilibrium.
Subsequently, we provide its general characterization and present an algorithm
for computing the ones in the feasible set's interior. Additionally, for a
simplified scenario involving two regions, which would correspond to a city
area with a downtown and a suburban region, we also provide a method to check
for the equilibria on the feasible set's boundary. Finally, through a numerical
case study, we illustrate the impact of charging prices on the position of the
Nash equilibrium.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper accepted for presentation at the 2024
  European Control Conference (ECC2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Branch and Bound method for the exact parameter identification of the
  PK/PD model for anesthetic drugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Di Credico, Luca Consolini, Mattia Laurini, Marco Locatelli, Marco Milanesi, Michele Schiavo, Antonio Visioli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of parameter identification for the standard
pharmacokinetic/pharmacodynamic (PK/PD) model for anesthetic drugs. Our main
contribution is the development of a global optimization method that guarantees
finding the parameters that minimize the one-step ahead prediction error. The
method is based on a branch-and-bound algorithm, that can be applied to solve a
more general class of nonlinear regression problems. We present some simulation
results, based on a dataset of twelve patients. In these simulations, we are
always able to identify the exact parameters, despite the non-convexity of the
overall identification problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Looking back and forward: A retrospective and future directions on
  Software Engineering for systems-of-systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Everton Cavalcante, Thais Batista, Flavio Oquendo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern systems are increasingly connected and more integrated with other
existing systems, giving rise to systems-of-systems (SoS). An SoS consists of a
set of independent, heterogeneous systems that interact to provide new
functionalities and accomplish global missions through emergent behavior
manifested at runtime. The distinctive characteristics of SoS, when contrasted
to traditional systems, pose significant research challenges within Software
Engineering. These challenges motivate the need for a paradigm shift and the
exploration of novel approaches for designing, developing, deploying, and
evolving these systems. The International Workshop on Software Engineering for
Systems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific
forums addressing SoS from the Software Engineering perspective, becoming the
first venue for this purpose. This article presents a study aimed at outlining
the evolution and future trajectory of Software Engineering for SoS based on
the examination of 57 papers spanning the 11 editions of the SESoS workshop
(2013-2023). The study combined scoping review and scientometric analysis
methods to categorize and analyze the research contributions concerning
temporal and geographic distribution, topics of interest, research
methodologies employed, application domains, and research impact. Based on such
a comprehensive overview, this article discusses current and future directions
in Software Engineering for SoS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A data-based comparison of methods for reducing the peak volume flow
  rate in a district heating system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Agner, Ulrich Trabert, Anders Rantzer, Janybek Orozaliev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work concerns reduction of the peak flow rate of a district heating
grid, a key system property which is bounded by pipe dimensions and pumping
capacity. The peak flow rate constrains the number of additional consumers that
can be connected, and may be a limiting factor in reducing supply temperatures
when transitioning to the 4th generation of district heating. We evaluate a
full year of operational data from a subset of customer meters in a district
heating system in Germany. We consider the peak flow rate reduction that could
be achieved with full a posteriori knowledge of this data. Three strategies for
reducing the peak flow rate are investigated: A load shifting demand response
strategy, an upper limitation in substation return temperatures, and an upper
limitation on each substation's volume flow rate. We show that imposing up to
to 18 % load flexibility for the customers provides an equal reduction in the
peak system flow rate under the load shifting strategy. The limited return
temperature strategy is less efficient at curtailing the peak flow rate, but
provides an overall reduction of volume flow rates. Finally, the flow rate
limitation method can introduce new, higher flow rate peaks, reducing
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIS epidemics on open networks: A replacement-based approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renato Vizuete, Paolo Frasca, Elena Panteley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we analyze continuous-time SIS epidemics subject to arrivals
and departures of agents, by using an approximated process based on
replacements. In defining the SIS dynamics in an open network, we consider a
stochastic setting in which arrivals and departures take place according to
Poisson processes with similar rates, and the new value of the infection
probability of an arriving agent is drawn from a continuous distribution. Since
the system size changes with time, we define an approximated process, in which
replacements take place instead of arrivals and departures, and we focus on the
evolution of an aggregate measure of the level of infection. So long as the
reproduction number is less than one, the long-term behavior of this function
measures the impact of the changes of the set of agents in the epidemic. We
derive upper bounds for the expectation and variance of this function and we
include a numerical example to show that the approximated process is close to
the original SIS process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures, to appear in European Control Conference (ECC
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictable Interval MDPs through Entropy Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Menno van Zutphen, Giannis Delimpaltadakis, Maurice Heemels, Duarte Antunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regularization of control policies using entropy can be instrumental in
adjusting predictability of real-world systems. Applications benefiting from
such approaches range from, e.g., cybersecurity, which aims at maximal
unpredictability, to human-robot interaction, where predictable behavior is
highly desirable. In this paper, we consider entropy regularization for
interval Markov decision processes (IMDPs). IMDPs are uncertain MDPs, where
transition probabilities are only known to belong to intervals. Lately, IMDPs
have gained significant popularity in the context of abstracting stochastic
systems for control design. In this work, we address robust minimization of the
linear combination of entropy and a standard cumulative cost in IMDPs, thereby
establishing a trade-off between optimality and predictability. We show that
optimal deterministic policies exist, and devise a value-iteration algorithm to
compute them. The algorithm solves a number of convex programs at each step.
Finally, through an illustrative example we show the benefits of penalizing
entropy in IMDPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based
  Obstacle Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanna Müller, Victor Kartsch, Michele Magno, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nano-drones, distinguished by their agility, minimal weight, and
cost-effectiveness, are particularly well-suited for exploration in confined,
cluttered and narrow spaces. Recognizing transparent, highly reflective or
absorbing materials, such as glass and metallic surfaces is challenging, as
classical sensors, such as cameras or laser rangers, often do not detect them.
Inspired by bats, which can fly at high speeds in complete darkness with the
help of ultrasound, this paper introduces \textit{BatDeck}, a pioneering
sensor-deck employing a lightweight and low-power ultrasonic sensor for
nano-drone autonomous navigation. This paper first provides insights about
sensor characteristics, highlighting the influence of motor noise on the
ultrasound readings, then it introduces the results of extensive experimental
tests for obstacle avoidance (OA) in a diverse environment. Results show that
\textit{BatDeck} allows exploration for a flight time of 8 minutes while
covering 136m on average before crash in a challenging environment with
transparent and reflective obstacles, proving the effectiveness of ultrasonic
sensors for OA on nano-drones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Planning of Robotic Manipulator in Dynamic Environment
  Exploiting DRL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osama Ahmad, Zawar Hussain, Hammad Naeem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study is about the implementation of a reinforcement learning algorithm
in the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick
and place the randomly placed block at a random target point in an unknown
environment. The obstacle is randomly moving which creates a hurdle in picking
the object. The objective of the robot is to avoid the obstacle and pick the
block with constraints to a fixed timestamp. In this literature, we have
applied a deep deterministic policy gradient (DDPG) algorithm and compared the
model's efficiency with dense and sparse rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICIESTR-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for
  Computations in Matlab 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manel Velasco, Isiah Zaplana, Arnau Dória-Cerezo, Pau Martí
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geometric algebra (GA) is a mathematical tool for geometric computing,
providing a framework that allows a unified and compact approach to geometric
relations which in other mathematical systems are typically described using
different more complicated elements. This fact has led to an increasing
adoption of GA in applied mathematics and engineering problems. However, the
scarcity of symbolic implementations of GA and its inherent complexity,
requiring a specific mathematical background, make it challenging and less
intuitive for engineers to work with. This prevents wider adoption among more
applied professionals. To address this challenge, this paper introduces SUGAR
(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox
designed for Matlab and licensed under the MIT License. SUGAR facilitates the
translation of GA concepts into Matlab and provides a collection of
user-friendly functions tailored for GA computations, including support for
symbolic operations. It supports both numeric and symbolic computations in
high-dimensional GAs. Specifically tailored for applied mathematics and
engineering applications, SUGAR has been meticulously engineered to represent
geometric elements and transformations within two and three-dimensional
projective and conformal geometric algebras, aligning with established
computational methodologies in the literature. Furthermore, SUGAR efficiently
handles functions of multivectors, such as exponential, logarithmic,
sinusoidal, and cosine functions, enhancing its applicability across various
engineering domains, including robotics, control systems, and power
electronics. Finally, this work includes four distinct validation examples,
demonstrating SUGAR's capabilities across the above-mentioned fields and its
practical utility in addressing real-world applied mathematics and engineering
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures, journal paper submitted to ACM TOMS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Bayesian Optimization: Data-Efficient Controller Tuning with
  Digital Twin 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahdi Nobar, Jürg Keller, Alisa Rupenyan, Mohammad Khosravi, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents the guided Bayesian optimization algorithm as an
efficient data-driven method for iteratively tuning closed-loop controller
parameters using an event-triggered digital twin of the system based on
available closed-loop data. We define a controller tuning framework independent
of the controller or the plant structure. Our proposed methodology is
model-free, making it suitable for nonlinear and unmodelled plants with
measurement noise. The objective function consists of performance metrics
modeled by Gaussian processes. We utilize the available information in the
closed-loop system to identify and progressively maintain a digital twin that
guides the optimizer, improving the data efficiency of our method. Switching
the digital twin on and off is triggered by data-driven criteria related to the
digital twin's uncertainty estimations in the BO tuning framework. Effectively,
it replaces much of the exploration of the real system with exploration
performed on the digital twin. We analyze the properties of our method in
simulation and demonstrate its performance on two real closed-loop systems with
different plant and controller structures. The experimental results show that
our method requires fewer experiments on the physical plant than Bayesian
optimization to find the optimal controller parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The <span class="highlight-title">Adaptive</span> Workplace: Orchestrating Architectural Services around the
  Wellbeing of Individual Occupants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Vande Moere, Sara Arko, Alena Safrova Drasilova, Tomáš Ondráček, Ilaria Pigliautile, Benedetta Pioppi, Anna Laura Pisello, Jakub Prochazka, Paula Acuna Roncancio, Davide Schaumann, Marcel Schweiker, Binh Vinh Duc Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the academic consortia members of the EU Horizon project SONATA
("Situation-aware OrchestratioN of AdapTive Architecture"), we respond to the
workshop call for "Office Wellbeing by Design: Don't Stand for Anything Less"
by proposing the "Adaptive Workplace" concept. In essence, our vision aims to
adapt a workplace to the ever-changing needs of individual occupants, instead
of that occupants are expected to adapt to their workplace.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counter-example guided Imitation Learning of Feedback Controllers from
  Temporal Logic Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16593v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16593v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thao Dang, Alexandre Donzé, Inzemamul Haque, Nikolaos Kekatos, Indranil Saha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method for imitation learning for control requirements
expressed using Signal Temporal Logic (STL). More concretely we focus on the
problem of training a neural network to imitate a complex controller. The
learning process is guided by efficient data aggregation based on
counter-examples and a coverage measure. Moreover, we introduce a method to
evaluate the performance of the learned controller via parameterization and
parameter estimation of the STL requirements. We demonstrate our approach with
a flying robot case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy
  Approach with Performance Guarantee 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shumpei Nishida, Kunihisa Okano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a linear quadratic regulation problem with a constraint where the
control input can be nonzero only at a limited number of times. Given that this
constraint leads to a combinational optimization problem, we adopt a greedy
method to find a suboptimal solution. To quantify the performance of the greedy
algorithm, we employ two metrics that reflect the submodularity level of the
objective function: The submodularity ratio and curvature. We first present an
explicit form of the optimal control input that is amenable to evaluating these
metrics. Subsequently, we establish bounds on the submodularity ratio and
curvature, which enable us to offer a practical performance guarantee for the
greedy algorithm. The effectiveness of our guarantee is further demonstrated
through numerical simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupling parameter variation from noise: Biquadratic Lyapunov forms in
  data-driven LPV control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Verhoek, Jaap Eising, Florian Dörfler, Roland Tóth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A promising step from linear towards nonlinear data-driven control is via the
design of controllers for linear parameter-varying (LPV) systems, which are
linear systems whose parameters are varying along a measurable scheduling
signal. However, the interplay between uncertainty arising from corrupted data
and the parameter-varying nature of these systems impacts the stability
analysis, and limits the generalization of well-understood data-driven methods
for linear time-invariant systems. In this work, we decouple this interplay
using a recently developed variant of the Fundamental Lemma for LPV systems and
the viewpoint of data-informativity, in combination with biquadratic Lyapunov
forms. Together, these allow us to develop novel linear matrix inequality
conditions for the existence of scheduling-dependent Lyapunov functions,
incorporating the intrinsic nonlinearity. Appealingly, these results are stated
purely in terms of the collected data and bounds on the noise, and they are
computationally favorable to check.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for CDC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid low-dimensional limiting state of charge estimator for multi-cell
  lithium-ion batteries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16555v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16555v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mira Khalil, Romain Postoyan, Stéphane Raël, Dragan Nešić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state of charge (SOC) of lithium-ion batteries needs to be accurately
estimated for safety and reliability purposes. For battery packs made of a
large number of cells, it is not always feasible to design one SOC estimator
per cell due to limited computational resources. Instead, only the minimum and
the maximum SOC need to be estimated. The challenge is that the cells having
minimum and maximum SOC typically change over time. In this context, we present
a low-dimensional hybrid estimator of the minimum (maximum) SOC, whose
convergence is analytically guaranteed. We consider for this purpose a battery
consisting of cells interconnected in series, which we model by electric
equivalent circuit models. We then present the hybrid estimator, which runs an
observer designed for a single cell at any time instant, selected by a
switching-like logic mechanism. We establish a practical exponential stability
property for the estimation error on the minimum (maximum) SOC thereby
guaranteeing the ability of the hybrid scheme to generate accurate estimates of
the minimum (maximum) SOC. The analysis relies on non-smooth hybrid Lyapunov
techniques. A numerical illustration is provided to showcase the relevance of
the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatially temporally distributed informative path planning for
  multi-robot systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh Nguyen, Linh Nguyen, Truong X. Nghiem, Hung La, Jose Baca, Pablo Rangel, Miguel Cid Montoya, Thang Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the problem of informative path planning for a mobile
robotic sensor network in spatially temporally distributed mapping. The robots
are able to gather noisy measurements from an area of interest during their
movements to build a Gaussian Process (GP) model of a spatio-temporal field.
The model is then utilized to predict the spatio-temporal phenomenon at
different points of interest. To spatially and temporally navigate the group of
robots so that they can optimally acquire maximal information gains while their
connectivity is preserved, we propose a novel multistep prediction informative
path planning optimization strategy employing our newly defined local cost
functions. By using the dual decomposition method, it is feasible and practical
to effectively solve the optimization problem in a distributed manner. The
proposed method was validated through synthetic experiments utilizing
real-world data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Extrusion Force Control Tuning for 3D Printing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Guidetti, Ankita Mukne, Marvin Rueppel, Yannick Nagel, Efe C. Balta, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quality of 3D prints often varies due to different conditions inherent to
each print, such as filament type, print speed, and nozzle size. Closed-loop
process control methods improve the accuracy and repeatability of 3D prints.
However, optimal tuning of controllers for given process parameters and design
geometry is often a challenge with manually tuned controllers resulting in
inconsistent and suboptimal results. This work employs Bayesian optimization to
identify the optimal controller parameters. Additionally, we explore transfer
learning in the context of 3D printing by leveraging prior information from
past trials. By integrating optimized extrusion force control and transfer
learning, we provide a novel framework for closed-loop 3D printing and propose
an automated calibration routine that produces high-quality prints for a
desired combination of print settings, material, and shape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE CASE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Ge, Pieter van Goor, Robert Mahony
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic inference on Lie groups plays a key role in state estimation
problems, such as inertial navigation, visual inertial odometry, pose
estimation in virtual reality, etc. A key problem is fusing independent
concentrated Gaussian distributions defined at different reference points on
the group. In this paper we approximate distributions at different points in
the group in a single set of exponential coordinates and then use classical
Gaussian fusion to obtain the fused posteriori in those coordinates. We
consider several approximations including the exact Jacobian of the change of
coordinate map, first and second order Taylor's expansions of the Jacobian, and
parallel transport with and without curvature correction associated with the
underlying geometry of the Lie group. Preliminary results on SO(3) demonstrate
that a novel approximation using parallel transport with curvature correction
achieves similar accuracy to the state-of-the-art optimisation based algorithms
at a fraction of the computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint for L-CSS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Distributionally Robust Model Predictive Control for Static and
  Dynamic Uncertainties in Smart Grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Li, Ye Shi, Yuning Jiang, Yuanming Shi, Haoyu Wang, H. Vincent Poor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of various power sources, including renewables and electric
vehicles, into smart grids is expanding, introducing uncertainties that can
result in issues like voltage imbalances, load fluctuations, and power losses.
These challenges negatively impact the reliability and stability of online
scheduling in smart grids. Existing research often addresses uncertainties
affecting current states but overlooks those that impact future states, such as
the unpredictable charging patterns of electric vehicles. To distinguish
between these, we term them static uncertainties and dynamic uncertainties,
respectively. This paper introduces WDR-MPC, a novel approach that stands for
two-stage Wasserstein-based Distributionally Robust (WDR) optimization within a
Model Predictive Control (MPC) framework, aimed at effectively managing both
types of uncertainties in smart grids. The dynamic uncertainties are first
reformulated into ambiguity tubes and then the distributionally robust bounds
of both dynamic and static uncertainties can be established using WDR
optimization. By employing ambiguity tubes and WDR optimization, the stochastic
MPC system is converted into a nominal one. Moreover, we develop a convex
reformulation method to speed up WDR computation during the two-stage
optimization. The distinctive contribution of this paper lies in its holistic
approach to both static and dynamic uncertainties in smart grids. Comprehensive
experiment results on IEEE 38-bus and 94-bus systems reveal the method's
superior performance and the potential to enhance grid stability and
reliability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed RL for Maximal Safety Probability Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hikaru Hoshino, Yorie Nakahira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate risk quantification and reachability analysis are crucial for safe
control and learning, but sampling from rare events, risky states, or long-term
trajectories can be prohibitively costly. Motivated by this, we study how to
estimate the long-term safety probability of maximally safe actions without
sufficient coverage of samples from risky states and long-term trajectories.
The use of maximal safety probability in control and learning is expected to
avoid conservative behaviors due to over-approximation of risk. Here, we first
show that long-term safety probability, which is multiplicative in time, can be
converted into additive costs and be solved using standard reinforcement
learning methods. We then derive this probability as solutions of partial
differential equations (PDEs) and propose Physics-Informed Reinforcement
Learning (PIRL) algorithm. The proposed method can learn using sparse rewards
because the physics constraints help propagate risk information through
neighbors. This suggests that, for the purpose of extracting more information
for efficient learning, physics constraints can serve as an alternative to
reward shaping. The proposed method can also estimate long-term risk using
short-term samples and deduce the risk of unsampled states. This feature is in
stark contrast with the unconstrained deep RL that demands sufficient data
coverage. These merits of the proposed method are demonstrated in numerical
simulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time Adaptation for Condition Monitoring Signal <span class="highlight-title">Prediction</span> using
  Label-aware Neural Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyun Chung, Raed Al Kontar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a predictive model that rapidly adapts to real-time condition
monitoring (CM) signals is critical for engineering systems/units.
Unfortunately, many current methods suffer from a trade-off between
representation power and agility in online settings. For instance, parametric
methods that assume an underlying functional form for CM signals facilitate
efficient online prediction updates. However, this simplification leads to
vulnerability to model specifications and an inability to capture complex
signals. On the other hand, approaches based on over-parameterized or
non-parametric models can excel at explaining complex nonlinear signals, but
real-time updates for such models pose a challenging task. In this paper, we
propose a neural process-based approach that addresses this trade-off. It
encodes available observations within a CM signal into a representation space
and then reconstructs the signal's history and evolution for prediction. Once
trained, the model can encode an arbitrary number of observations without
requiring retraining, enabling on-the-spot real-time predictions along with
quantified uncertainty and can be readily updated as more online data is
gathered. Furthermore, our model is designed to incorporate qualitative
information (i.e., labels) from individual units. This integration not only
enhances individualized predictions for each unit but also enables joint
inference for both signals and their associated labels. Numerical studies on
both synthetic and real-world data in reliability engineering highlight the
advantageous features of our model in real-time adaptation, enhanced signal
prediction with uncertainty quantification, and joint prediction for labels and
signals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fault Localization for Buggy Deep Learning Framework Conversions in
  Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06157v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06157v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying Deep Neural Networks (DNNs), developers often convert models
from one deep learning framework to another (e.g., TensorFlow to PyTorch).
However, this process is error-prone and can impact target model accuracy. To
identify the extent of such impact, we perform and briefly present a
differential analysis against three DNNs widely used for image recognition
(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep
learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which
revealed numerous model crashes and output label discrepancies of up to 100%.
To mitigate such errors, we present a novel approach towards fault localization
and repair of buggy deep learning framework conversions, focusing on
pre-trained image recognition models. Our technique consists of four stages of
analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,
and 4) graph representation. In addition, we propose various strategies towards
fault repair of the faults detected. We implement our technique on top of the
Apache TVM deep learning compiler, and we test it by conducting a preliminary
fault localization analysis for the conversion of InceptionV3 from TF to
TFLite. Our approach detected a fault in a common DNN converter tool, which
introduced precision errors in weights, reducing model accuracy. After our
fault localization, we repaired the issue, reducing our conversion error to
zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeltaNN: Assessing the Impact of Computational Environment Parameters on
  the Performance of Image Recognition Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06208v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06208v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolaos Louloudakis, Perry Gibson, José Cano, Ajitha Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done using multiple software components like deep
learning frameworks, compilers, and device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not yet well understood.
  In this paper we present a differential testing framework, DeltaNN, that
allows us to assess the impact of different computational environment
parameters on the performance of image recognition models during deployment,
post training. DeltaNN generates different implementations of a given image
recognition model for variations in environment parameters, namely, deep
learning frameworks, compiler optimizations and hardware devices and analyzes
differences in model performance as a result. Using DeltaNN, we conduct an
empirical study of robustness analysis of three popular image recognition
models using the ImageNet dataset. We report the impact in terms of
misclassifications and inference time differences across different settings. In
total, we observed up to 100% output label differences across deep learning
frameworks, and up to 81% unexpected performance degradation in terms of
inference time, when applying compiler optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long Solution Times or Low Solution Quality: On Trade-Offs in Choosing a
  Power Flow Formulation for the Optimal Power Shutoff Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Haag, Noah Rhodes, Line Roald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Optimal Power Shutoff (OPS) problem is an optimization problem that makes
power line de-energization decisions in order to reduce the risk of igniting a
wildfire, while minimizing the load shed of customers. This problem, with DC
linear power flow equations, has been used in many studies in recent years.
However, using linear approximations for power flow when making decisions on
the network topology is known to cause challenges with AC feasibility of the
resulting network, as studied in the related contexts of optimal transmission
switching or grid restoration planning. This paper explores the accuracy of the
DC OPS formulation and the ability to recover an AC-feasible power flow
solution after de-energization decisions are made. We also extend the OPS
problem to include variants with the AC, Second-Order-Cone, and Network-Flow
power flow equations, and compare them to the DC approximation with respect to
solution quality and time. The results highlight that the DC approximation
overestimates the amount of load that can be served, leading to poor
de-energization decisions. The AC and SOC-based formulations are better, but
prohibitively slow to solve for even modestly sized networks thus demonstrating
the need for new solution methods with better trade-offs between computational
time and solution quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Robustness in Cyber-Physical Systems:
  Specification-Centric Analysis in the face of System Deviations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changjian Zhang, Parv Kapoor, Romulo Meira-Goes, David Garlan, Eunsuk Kang, Akila Ganlath, Shatadal Mishra, Nejib Ammar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of cyber-physical systems (CPS) is on the rise in complex
physical environments, encompassing domains such as autonomous vehicles, the
Internet of Things (IoT), and smart cities. A critical attribute of CPS is
robustness, denoting its capacity to operate safely despite potential
disruptions and uncertainties in the operating environment. This paper proposes
a novel specification-based robustness, which characterizes the effectiveness
of a controller in meeting a specified system requirement, articulated through
Signal Temporal Logic (STL) while accounting for possible deviations in the
system. This paper also proposes the robustness falsification problem based on
the definition, which involves identifying minor deviations capable of
violating the specified requirement. We present an innovative two-layer
simulation-based analysis framework designed to identify subtle robustness
violations. To assess our methodology, we devise a series of benchmark problems
wherein system parameters can be adjusted to emulate various forms of
uncertainties and disturbances. Initial evaluations indicate that our
falsification approach proficiently identifies robustness violations, providing
valuable insights for comparing robustness between conventional and
reinforcement learning (RL)-based controllers
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Feedback Law in Stochastic Optimal Nonlinear Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.01041v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.01041v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Naveed Gul Mohamed, Suman Chakravorty, Raman Goyal, Ran Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of nonlinear stochastic optimal control. This problem
is thought to be fundamentally intractable owing to Bellman's ``curse of
dimensionality". We present a result that shows that repeatedly solving an
open-loop deterministic problem from the current state with progressively
shorter horizons, similar to Model Predictive Control (MPC), results in a
feedback policy that is $O(\epsilon^4)$ near to the true global stochastic
optimal policy, \nxx{where $\epsilon$ is a perturbation parameter modulating
the noise.} We show that the optimal deterministic feedback problem has a
perturbation structure in that higher-order terms of the feedback law do not
affect lower-order terms, and that this structure is lost in the optimal
stochastic feedback problem. Consequently, solving the Stochastic Dynamic
Programming problem is highly susceptible to noise, even when tractable, and in
practice, the MPC-type feedback law offers superior performance even for
stochastic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2002.10505,
  arXiv:2002.09478</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Carbon Footprint Reduction for Sustainable Data Centers in Real-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumyendu Sarkar, Avisek Naug, Ricardo Luna, Antonio Guillen, Vineet Gundecha, Sahand Ghorbanpour, Sajad Mousavi, Dejan Markovikj, Ashwin Ramesh Babu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As machine learning workloads significantly increase energy consumption,
sustainable data centers with low carbon emissions are becoming a top priority
for governments and corporations worldwide. This requires a paradigm shift in
optimizing power consumption in cooling and IT loads, shifting flexible loads
based on the availability of renewable energy in the power grid, and leveraging
battery storage from the uninterrupted power supply in data centers, using
collaborative agents. The complex association between these optimization
strategies and their dependencies on variable external factors like weather and
the power grid carbon intensity makes this a hard problem. Currently, a
real-time controller to optimize all these goals simultaneously in a dynamic
real-world setting is lacking. We propose a Data Center Carbon Footprint
Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that
optimizes data centers for the multiple objectives of carbon footprint
reduction, energy consumption, and energy cost. The results show that the
DC-CFR MARL agents effectively resolved the complex interdependencies in
optimizing cooling, load shifting, and energy storage in real-time for various
locations under real-world dynamic weather and grid carbon intensity
conditions. DC-CFR significantly outperformed the industry standard ASHRAE
controller with a considerable reduction in carbon emissions (14.5%), energy
usage (14.4%), and energy cost (13.7%) when evaluated over one year across
multiple geographical regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identification of Energy Management Configuration Concepts from a Set of
  Pareto-optimal Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Lanfermann, Qiqi Liu, Yaochu Jin, Sebastian Schmitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implementing resource efficient energy management systems in facilities and
buildings becomes increasingly important in the transformation to a sustainable
society. However, selecting a suitable configuration based on multiple,
typically conflicting objectives, such as cost, robustness with respect to
uncertainty of grid operation, or renewable energy utilization, is a difficult
multi-criteria decision making problem. The recently developed concept
identification technique can facilitate a decision maker by sorting
configuration options into semantically meaningful groups (concepts). In this
process, the partitioning of the objectives and design parameters into
different sets (called description spaces) is a very important step. In this
study we focus on utilizing the concept identification technique for finding
relevant and viable energy management configurations from a very large data set
of Pareto-optimal solutions. The data set consists of 20000 realistic
Pareto-optimal building energy management configurations generated by a
many-objective evolutionary optimization of a high quality Digital Twin energy
management simulator. We analyze how the choice of description spaces, i.e.,
the partitioning of the objectives and parameters, impacts the type of
information that can be extracted. We show that the decision maker can
introduce constraints and biases into that process to meet expectations and
preferences. The iterative approach presented in this work allows for the
generation of valuable insights into trade-offs between specific objectives,
and constitutes a powerful and flexible tool to support the decision making
process when designing large and complex energy management systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 8 figures, accepted at Energy Conversion and Management: X</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid physics-informed metabolic cybergenetics: process rates augmented
  with machine-learning surrogates informed by flux balance analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00670v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00670v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastián Espinel-Ríos, José L. Avalos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metabolic cybergenetics is a promising concept that interfaces gene
expression and cellular metabolism with computers for real-time dynamic
metabolic control. The focus is on control at the transcriptional level,
serving as a means to modulate intracellular metabolic fluxes. Recent
strategies in this field have employed constraint-based dynamic models for
process optimization, control, and estimation. However, this results in bilevel
dynamic optimization problems, which pose considerable numerical and conceptual
challenges. In this study, we present an alternative hybrid physics-informed
dynamic modeling framework for metabolic cybergenetics, aimed at simplifying
optimization, control, and estimation tasks. By utilizing machine-learning
surrogates, our approach effectively embeds the physics of metabolic networks
into the process rates of structurally simpler macro-kinetic models coupled
with gene expression. These surrogates, informed by flux balance analysis, link
the domains of manipulatable intracellular enzymes to metabolic exchange
fluxes. This ensures that critical knowledge captured by the system's metabolic
network is preserved. The resulting models can be integrated into metabolic
cybergenetic schemes involving single-level optimizations. Additionally, the
hybrid modeling approach maintains the number of system states at a necessary
minimum, easing the burden of process monitoring and estimation. Our hybrid
physics-informed metabolic cybergenetic framework is demonstrated using a
computational case study on the optogenetically-assisted production of
itaconate by $\textit{Escherichia coli}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures, journal submission (reviewed/accepted version)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Peak Estimation of Rational Systems using Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Miller, Roy S. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents algorithms that upper-bound the peak value of a state
function along trajectories of a continuous-time system with rational dynamics.
The finite-dimensional but nonconvex peak estimation problem is cast as a
convex infinite-dimensional linear program in occupation measures. This
infinite-dimensional program is then truncated into finite-dimensions using the
moment-Sum-of-Squares (SOS) hierarchy of semidefinite programs. Prior work on
treating rational dynamics using the moment-SOS approach involves clearing
dynamics to common denominators or adding lifting variables to handle
reciprocal terms under new equality constraints. Our solution method uses a
sum-of-rational method based on absolute continuity of measures. The Moment-SOS
truncations of our program possess lower computational complexity and
(empirically demonstrated) higher accuracy of upper bounds on example systems
as compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Integral Consensus Control of <span class="highlight-title">Multi-Agent</span> Networks Perturbed by
  Matched and Unmatched Disturbances: The Case of Directed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.00262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.00262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Guadalupe Romero, David Navarro-Alarcon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a new method to design consensus controllers for perturbed
double integrator systems whose interconnection is described by a directed
graph containing a rooted spanning tree. We propose new robust controllers to
solve the consensus and synchronization problems when the systems are under the
effects of matched and unmatched disturbances. In both problems, we present
simple continuous controllers, whose integral actions allow us to handle the
disturbances. A rigorous stability analysis based on Lyapunov's direct method
for unperturbed networked systems is presented. To assess the performance of
our result, a representative simulation study is presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Ranging and Phase Offset Estimation for Multiple Drones using
  ADS-B Signatures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.05370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.05370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Mohammadkarimi, Geert Leus, Raj Thilak Rajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A new method for joint ranging and Phase Offset (PO) estimation of multiple
drones/aircrafts is proposed in this paper. The proposed method employs the
superimposed uncoordinated Automatic Dependent Surveillance Broadcast (ADS-B)
packets broadcasted by drones/aircrafts for joint range and PO estimation. It
jointly estimates range and PO prior to ADS-B packet decoding; thus, it can
improve air safety when packet decoding is infeasible due to packet collision.
Moreover, it enables coherent detection of ADS-B packets, which can result in
more reliable multiple target tracking in aviation systems using cooperative
sensors for detect and avoid (DAA). By minimizing the Kullback Leibler
Divergence (KLD) statistical distance measure, we show that the received
complex baseband signal coming from K uncoordinated drones corrupted by
Additive White Gaussian Noise (AWGN) at a single antenna receiver can be
approximated by an independent and identically distributed Gaussian Mixture
(GM) with 2 power K mixture components in the two dimensional (2D) plane. While
direct joint Maximum Likelihood Estimation (MLE) of range and PO from the
derived GM Probability Density Function (PDF) leads to an intractable
maximization, our proposed method employs the Expectation Maximization (EM)
algorithm to estimate the modes of the 2D Gaussian mixture followed by a
reordering estimation technique through combinatorial optimization to estimate
range and PO. An extension to a multiple antenna receiver is also investigated
in this paper. While the proposed estimator can estimate the range of multiple
drones with a single receive antenna, a larger number of drones can be
supported with higher accuracy by the use of multiple antennas at the receiver.
The effectiveness of the proposed estimator is supported by simulation results.
We show that the proposed estimator can jointly estimate the range of three
drones accurately.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Power<span class="highlight-title">Simulation</span>sDynamics.jl -- An Open Source Modeling Package for
  Modern Power Systems with Inverter-Based Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02921v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02921v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Daniel Lara, Rodrigo Henriquez-Auba, Matthew Bossart, Duncan S. Callaway, Clayton Barrows
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we present the development of an open-source simulation
toolbox, PowerSimulationsDynamics.jl, to study the dynamic response of power
systems, focusing on the requirements to model systems with high penetrations
of Inverter-Based Resources (IBRs). PowerSimulationsDynamics.jl is implemented
in Julia and features a rich library of synchronous generator, inverter, and
load models. In addition, it allows the study of quasi-static phasors and
electromagnetic dq models that use a dynamic network representation. Case
studies and validation exercises show that PowerSimulationsDynamics.jl results
closely match other commercial and open-source simulation tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Secure Control of Connected and Automated Vehicles Using Trust-Aware
  Robust Event-Triggered Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H M Sabbir Ahmad, Ehsan Sabouni, Akua Dickson, Wei Xiao, Christos G. Cassandras, Wenchao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the security of a network of Connected and Automated Vehicles
(CAVs) cooperating to safely navigate through a conflict area (e.g., traffic
intersections, merging roadways, roundabouts). Previous studies have shown that
such a network can be targeted by adversarial attacks causing traffic jams or
safety violations ending in collisions. We focus on attacks targeting the V2X
communication network used to share vehicle data and consider as well
uncertainties due to noise in sensor measurements and communication channels.
To combat these, motivated by recent work on the safe control of CAVs, we
propose a trust-aware robust event-triggered decentralized control and
coordination framework that can provably guarantee safety. We maintain a trust
metric for each vehicle in the network computed based on their behavior and
used to balance the tradeoff between conservativeness (when deeming every
vehicle as untrustworthy) and guaranteed safety and security. It is important
to highlight that our framework is invariant to the specific choice of the
trust framework. Based on this framework, we propose an attack detection and
mitigation scheme which has twofold benefits: (i) the trust framework is immune
to false positives, and (ii) it provably guarantees safety against false
positive cases. We use extensive simulations (in SUMO and CARLA) to validate
the theoretical guarantees and demonstrate the efficacy of our proposed scheme
to detect and mitigate adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2305.16818</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermally-Resilient Soft Gripper for On-Orbit Operations <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Ruiz, Begona Arrue, Anibal Ollero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research in soft manipulators has significantly enhanced object grasping
capabilities, thanks to their adaptability to various shapes and sizes.
Applying this technology to on-orbit servicing, especially during the capture
and containment stages of active space debris removal missions, might offer a
secure, adaptable, and cost-effective solution compared to the trend of
increasing the degrees of freedom and complexity of the manipulator (e.g.
ClearSpace, Astroscale). This work aims to conduct an experimental proof of
concept, for which challenges such as radiation, vacuum, and microgravity are
significant, but the predominant issue is ensuring effective operation in the
extreme temperature swings, where flexible materials may exhibit cryogenic
crystallization or drastic shifts in their elasticity. This work addresses this
challenge through an initial stage of analytical modeling of the thermal
dynamics inside the manipulator in orbit; which is then used for the
development of a first experimental prototype tested with liquid nitrogen and
heat guns. The multi-layered design for Low Earth Orbit (LEO) leverages the
properties of TPU at low infill rates for lightweight inherent flexibility,
silicone rubber ensuring structural integrity, PTFE (Teflon) for unparalleled
thermal stability, and aerogel for insulation. The tendon-actuated servo-driven
gripper is tested in the laboratory by varying the shape and size of objects
during the grasping. The results, based on servomotor force metrics to assess
the flexible manipulator's adaptability and object capture efficiency across
temperature changes, affirm the concept's viability. Forces increase up to
220$\%$ in cryogenic conditions and decrease by no more than 50$\%$ at high
temperatures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Directionality-Aware Mixture Model Parallel Sampling for Efficient
  Linear Parameter Varying Dynamical System Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02609v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02609v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunan Sun, Haihui Gao, Tianyu Li, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective
approach that learns stable, time-invariant motion policies using statistical
modeling and semi-definite optimization to encode complex motions for reactive
robot control. Despite its strengths, the LPV-DS learning approach faces
challenges in achieving a high model accuracy without compromising the
computational efficiency. To address this, we introduce the
Directionality-Aware Mixture Model (DAMM), a novel statistical model that
applies the Riemannian metric on the n-sphere $\mathbb{S}^n$ to efficiently
blend non-Euclidean directional data with $\mathbb{R}^m$ Euclidean states.
Additionally, we develop a hybrid Markov chain Monte Carlo technique that
combines Gibbs Sampling with Split/Merge Proposal, allowing for parallel
computation to drastically speed up inference. Our extensive empirical tests
demonstrate that LPV-DS integrated with DAMM achieves higher reproduction
accuracy, better model efficiency, and near real-time/online learning compared
to standard estimation methods on various datasets. Lastly, we demonstrate its
suitability for incrementally learning multi-behavior policies in real-world
robot experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stable Reduced-Rank VAR Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhui Rong, Victor Solo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vector autoregression (VAR) has been widely used in system
identification, econometrics, natural science, and many other areas. However,
when the state dimension becomes large the parameter dimension explodes. So
rank reduced modelling is attractive and is well developed. But a fundamental
requirement in almost all applications is stability of the fitted model. And
this has not been addressed in the rank reduced case. Here, we develop, for the
first time, a closed-form formula for an estimator of a rank reduced transition
matrix which is guaranteed to be stable. We show that our estimator is
consistent and asymptotically statistically efficient and illustrate it in
comparative simulations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based predictive modelling of transonic flow over an
  aerofoil 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li-Wei Chen, Nils Thuerey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively predicting transonic unsteady flow over an aerofoil poses
inherent challenges. In this study, we harness the power of deep neural network
(DNN) models using the attention U-Net architecture. Through efficient training
of these models, we achieve the capability to capture the complexities of
transonic and unsteady flow dynamics at high resolution, even when faced with
previously unseen conditions. We demonstrate that by leveraging the
differentiability inherent in neural network representations, our approach
provides a framework for assessing fundamental physical properties via global
instability analysis. This integration bridges deep neural network models and
traditional modal analysis, offering valuable insights into transonic flow
dynamics and enhancing the interpretability of neural network models in
flowfield diagnostics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An experimental evaluation of choices of SSA forecasting parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teodor Knapik, Adolphe Ratiarison, Hasina Razafindralambo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Six time series related to atmospheric phenomena are used as inputs for
experiments offorecasting with singular spectrum analysis (SSA). Existing
methods for SSA parametersselection are compared throughout their forecasting
accuracy relatively to an optimal aposteriori selection and to a naive
forecasting methods. The comparison shows that awidespread practice of
selecting longer windows leads often to poorer predictions. It alsoconfirms
that the choices of the window length and of the grouping are essential.
Withthe mean error of rainfall forecasting below 1.5%, SSA appears as a viable
alternative forhorizons beyond two weeks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Revue Africaine de Recherche en Informatique et Math{\'e}matiques
  Appliqu{\'e}es, In press, 40</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-24T00:00:00Z">2024-03-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Systems and Control
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dataset</span>s of Great Britain Primary Substations Integrated with Household
  Heating Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Zhou, Chaimaa Essayeh, Thomas Morstyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for electrified heating, electrified transportation, and
power-intensive data centres challenge distribution networks. If
electrification projects are carried out without considering electrical
distribution infrastructure, there could be unexpected blackouts and financial
losses. Datasets containing real-world distribution network information are
required to address this. On the other hand, social data, such as household
heating composition, are closely coupled with people's lives. Studying the
coupling between the energy system and society is important in promoting social
welfare. To fill these gaps, this paper introduces two datasets. The first is
the main dataset for the distribution networks in Great Britain (GB),
collecting information on firm capacity, peak demands, locations, and parent
transmission nodes (the Grid Supply Point, namely GSP) for all primary
substations (PSs). PSs are a crucial part of the UK distribution network and
are at the lowest voltage level (11 kV) with publicly available data for most
UK Distribution Network Operators (DNOs). Substation firm capacity and peak
demand facilitate an understanding of the remaining room of the existing
network. The parent GSP information helps link the dataset of distribution
networks to datasets of transmission networks. The second dataset extends the
main network dataset, linking each PS to information about the number of
households that use different types of central heating recorded in census data.
The derivation of the second dataset is based on locations of PSs collected in
the main dataset with appropriate assumptions. The derivation process may also
be replicated to integrate other social datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the journal "Data in Brief"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANN-Based <span class="highlight-title">Adaptive</span> NMPC for Uranium Extraction-Scrubbing Operation in
  Spent Nuclear Fuel Treatment Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duc-Tri Vo, Ionela Prodan, Laurent Lefèvre, Vincent Vanel, Sylvain Costenoble, Binh Dinh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the particularities in optimal control of the uranium
extraction-scrubbing operation in the PUREX process. The control problem
requires optimally stabilizing the system at a desired solvent saturation
level, guaranteeing constraints, disturbance rejection, and adapting to set
point variations. A qualified simulator named PAREX was developed by the French
Alternative Energies and Atomic Energy Commission (CEA) to simulate
liquid-liquid extraction operations in the PUREX process. However, since the
mathematical model is complex and is described by a system of nonlinear, stiff,
high-dimensional differential-algebraic equations (DAE), applying optimal
control methods will lead to a large-scale nonlinear programming problem with a
huge computational burden. The solution we propose in this work is to train a
neural network to predict the process outputs using the measurement history.
This neural network architecture, which employs the long short-term memory
(LSTM), linear regression and logistic regression networks, allows reducing the
number of state variables, thus reducing the complexity of the optimization
problems in the control scheme. Furthermore, nonlinear model predictive control
(NMPC) and moving horizon estimation (MHE) problems are developed and solved
using the PSO (Particle Swarm Optimization) algorithm. Simulation results show
that the proposed adaptive optimal control scheme satisfies the requirements of
the control problem and provides promise for experimental testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Control-Coherent Koopman Modeling: A Physical Modeling Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        H. Harry Asada, Jose A. Solano-Castellanos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling of nonlinear dynamics based on Koopman operator theory, which is
originally applicable only to autonomous systems with no control, is extended
to non-autonomous control system without approximation to input matrix B.
Prevailing methods using a least square estimate of the B matrix may result in
an erroneous input matrix, misinforming the controller about the structure of
the input matrix in a lifted space. Here, a new method for constructing a
Koopman model that comprises the exact input matrix B is presented. A set of
state variables are introduced so that the control inputs are linearly involved
in the dynamics of actuators. With these variables, a lifted linear model with
the exact control matrix, called a Control-Coherent Koopman Model, is
constructed by superposing control input terms, which are linear in local
actuator dynamics, to the Koopman operator of the associated autonomous
nonlinear system. The proposed method is applied to multi degree-of-freedom
robotic arms and multi-cable manipulation systems. Model Predictive Control is
applied to the former. It is demonstrated that the prevailing Dynamic Mode
Decomposition with Control (DMDc) using an approximate control matrix B does
not provide a satisfactory result, while the Control-Coherent Koopman Model
performs well with the correct B matrix.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modal-<span class="highlight-title">adaptive</span> Knowledge-enhanced Graph-based Financial <span class="highlight-title">Prediction</span> from
  Monetary Policy Conference Calls with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Ouyang, Yi Liu, Shicheng Li, Ruihan Bao, Keiko Harimoto, Xu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial prediction from Monetary Policy Conference (MPC) calls is a new yet
challenging task, which targets at predicting the price movement and volatility
for specific financial assets by analyzing multimodal information including
text, video, and audio. Although the existing work has achieved great success
using cross-modal transformer blocks, it overlooks the potential external
financial knowledge, the varying contributions of different modalities to
financial prediction, as well as the innate relations among different financial
assets. To tackle these limitations, we propose a novel Modal-Adaptive
kNowledge-enhAnced Graph-basEd financial pRediction scheme, named MANAGER.
Specifically, MANAGER resorts to FinDKG to obtain the external related
knowledge for the input text. Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit
BERT (HuBERT) to extract the video and audio features, respectively.
Thereafter, MANAGER introduces a novel knowledge-enhanced cross-modal graph
that fully characterizes the semantic relations among text, external knowledge,
video and audio, to adaptively utilize the information in different modalities,
with ChatGLM2 as the backbone. Extensive experiments on a publicly available
dataset Monopoly verify the superiority of our model over cutting-edge methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by LREC Coling 2024 -FinNLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-guided Machine Learning: Current Trends and Future Prospects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Karpatne, Xiaowei Jia, Vipin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an overview of scientific modeling and discusses the
complementary strengths and weaknesses of ML methods for scientific modeling in
comparison to process-based models. It also provides an introduction to the
current state of research in the emerging field of scientific knowledge-guided
machine learning (KGML) that aims to use both scientific knowledge and data in
ML frameworks to achieve better generalizability, scientific consistency, and
explainability of results. We discuss different facets of KGML research in
terms of the type of scientific knowledge used, the form of knowledge-ML
integration explored, and the method for incorporating scientific knowledge in
ML. We also discuss some of the common categories of use cases in environmental
sciences where KGML methods are being developed, using illustrative examples in
each category.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-23T00:00:00Z">2024-03-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyHexTop: a compact Python code for topology optimization using
  hexagonal elements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditi Agarwal, Anupam Saxena, Prabhat Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Python serves as an open-source and cost-effective alternative to the MATLAB
programming language. This paper introduces a concise topology optimization
Python code, named ``\texttt{PyHexTop}," primarily intended for educational
purposes. Code employs hexagonal elements to parameterize design domains as
such elements provide checkerboard-free optimized design naturally.
\texttt{PyHexTop} is developed based on the ``\texttt{HoneyTop90}" MATLAB
code~\cite{kumar2023honeytop90} and uses the \texttt{NumPy} and \texttt{SciPy}
libraries. Code is straightforward and easily comprehensible, proving a helpful
tool that can help people new in the topology optimization field to learn and
explore. \texttt{PyHexTop} is specifically tailored to address compliance
minimization with specified volume constraints. The paper provides a detailed
explanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and
extensions to solve problems different problems. The code is publicly shared
at: \url{https://github.com/PrabhatIn/PyHexTop.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NCMDAO 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-22T00:00:00Z">2024-03-22</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A gradient-enhanced univariate dimension reduction method for
  uncertainty propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingran Wang, Nicholas C. Orndorff, Mark Sperry, John T. Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The univariate dimension reduction (UDR) method stands as a way to estimate
the statistical moments of the output that is effective in a large class of
uncertainty quantification (UQ) problems. UDR's fundamental strategy is to
approximate the original function using univariate functions so that the UQ
cost only scales linearly with the dimension of the problem. Nonetheless, UDR's
effectiveness can diminish when uncertain inputs have high variance,
particularly when assessing the output's second and higher-order statistical
moments. This paper proposes a new method, gradient-enhanced univariate
dimension reduction (GUDR), that enhances the accuracy of UDR by incorporating
univariate gradient function terms into the UDR approximation function.
Theoretical results indicate that the GUDR approximation is expected to be one
order more accurate than UDR in approximating the original function, and it is
expected to generate more accurate results in computing the output's second and
higher-order statistical moments. Our proposed method uses a computational
graph transformation strategy to efficiently evaluate the GUDR approximation
function on tensor-grid quadrature inputs, and use the tensor-grid input-output
data to compute the statistical moments of the output. With an efficient
automatic differentiation method to compute the gradients, our method preserves
UDR's linear scaling of computation time with problem dimension. Numerical
results show that the GUDR is more accurate than UDR in estimating the standard
deviation of the output and has a performance comparable to the method of
moments using a third-order Taylor series expansion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-accelerated non-intrusive polynomial chaos expansion using
  partially tensor-structured quadrature rules 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingran Wang, Nicholas C. Orndorff, John T. Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the graph-accelerated non-intrusive polynomial chaos (NIPC) method
has been proposed for solving uncertainty quantification (UQ) problems. This
method leverages the full-grid integration-based NIPC method to address UQ
problems while employing the computational graph transformation approach, AMTC,
to accelerate the tensor-grid evaluations. This method exhibits remarkable
efficacy on a broad range of low-dimensional (three dimensions or less) UQ
problems featuring multidisciplinary models. However, it often does not scale
well with problem dimensions due to the exponential increase in the number of
quadrature points when using the full-grid quadrature rule. To expand the
applicability of this method to a broader range of UQ problems, this paper
introduces a new framework for generating a tailored, partially
tensor-structured quadrature rule to use with the graph-accelerated NIPC
method. This quadrature rule, generated through the designed quadrature
approach, possesses a tensor structure that is tailored for the computational
model. The selection of the tensor structure is guided by an analysis of the
computational graph, ensuring that the quadrature rule effectively capitalizes
on the sparsity within the computational graph when paired with the AMTC
method. This method has been tested on one 4D and one 6D UQ problem, both
originating from aircraft design scenarios and featuring multidisciplinary
models. Numerical results show that, when using with graph-accelerated NIPC
method, our approach generates a partially tensor-structured quadrature rule
that outperforms the full-grid Gauss quadrature and the designed quadrature
methods (more than 40% reduction in computational costs) in both of the test
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Linear Time-Delay Scheme to Propagate Reduced Electron Density
  Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harish S. Bhat, Hardeep Bassi, Karnamohit Ranka, Christine M. Isborn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For any linear system where the unreduced dynamics are governed by unitary
propagators, we derive a closed, time-delayed, linear system for a
reduced-dimensional quantity of interest. We apply this method to understand
the memory-dependence of reduced $1$-electron density matrices in
time-dependent configuration interaction (TDCI), a scheme to solve for the
correlated dynamics of electrons in molecules. Though time-dependent density
functional theory has established that the reduced $1$-electron density
possesses memory-dependence, the precise nature of this memory-dependence has
not been understood. We derive a self-contained, symmetry/constraint-preserving
method to propagate reduced TDCI electron density matrices. In numerical tests
on two model systems (H$_2$ and HeH$^+$), we show that with sufficiently large
time-delay (or memory-dependence), our method propagates reduced TDCI density
matrices with high quantitative accuracy. We study the dependence of our
results on time step and basis set. To derive our method, we calculate the
$4$-index tensor that relates reduced and full TDCI density matrices. Our
calculation applies to any TDCI system, regardless of basis set, number of
electrons, or choice of Slater determinants in the wave function. This
calculation enables a proof that the trace of the reduced TDCI density matrix
is constant and equals the number of electrons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital twin model of colon electromechanics for manometry <span class="highlight-title">prediction</span> of
  laser tissue soldering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        René Thierry Djoumessi, Pietro Lenarda, Alessio Gizzi, Simone Giusti, Pietro Alduini, Marco Paggi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present study introduces an advanced multi-physics and multi-scale
modeling approach to investigate in silico colon motility. We introduce a
generalized electromechanical framework, integrating cellular electrophysiology
and smooth muscle contractility, thus advancing a first-of-its-kind
computational model of laser tissue soldering after incision resection. The
proposed theoretical framework comprises three main elements: a microstructural
material model describing intestine wall geometry and composition of
reinforcing fibers, with four fiber families, two active-conductive and two
passive; an electrophysiological model describing the propagation of slow
waves, based on a fully-coupled nonlinear phenomenological approach; and a
thermodynamical consistent mechanical model describing the hyperelastic
energetic contributions ruling tissue equilibrium under diverse loading
conditions. The active strain approach was adopted to describe tissue
electromechanics by exploiting the multiplicative decomposition of the
deformation gradient for each active fiber family and solving the governing
equations via a staggered finite element scheme. The computational framework
was fine-tuned according to state-of-the-art experimental evidence, and
extensive numerical analyses allowed us to compare manometric traces computed
via numerical simulations with those obtained clinically in human patients. The
model proved capable of reproducing both qualitatively and quantitatively high
or low-amplitude propagation contractions. Colon motility after laser tissue
soldering demonstrates that material properties and couplings of the deposited
tissue are critical to reproducing a physiological muscular contraction, thus
restoring a proper peristaltic activity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating model evaluations in uncertainty propagation on tensor
  grids using computational graph transformations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingran Wang, Mark Sperry, Victor E. Gandarillas, John T. Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods such as non-intrusive polynomial chaos (NIPC), and stochastic
collocation are frequently used for uncertainty propagation problems.
Particularly for low-dimensional problems, these methods often use a
tensor-product grid for sampling the space of uncertain inputs. A limitation of
this approach is that it encounters a significant challenge: the number of
sample points grows exponentially with the increase of uncertain inputs.
Current strategies to mitigate computational costs abandon the tensor structure
of sampling points, with the aim of reducing their overall count.
Contrastingly, our investigation reveals that preserving the tensor structure
of sample points can offer distinct advantages in specific scenarios. Notably,
by manipulating the computational graph of the targeted model, it is feasible
to avoid redundant evaluations at the operation level to significantly reduce
the model evaluation cost on tensor-grid inputs. This paper presents a
pioneering method: Accelerated Model Evaluations on Tensor grids using
Computational graph transformations (AMTC). The core premise of AMTC lies in
the strategic modification of the computational graph of the target model to
algorithmically remove the repeated evaluations on the operation level. We
implemented the AMTC method within the compiler of a new modeling language
called the Computational System Design Language (CSDL). We demonstrate the
effectiveness of AMTC by using it with the full-grid NIPC method to solve four
low-dimensional UQ problems involving an analytical piston model, a
multidisciplinary unmanned aerial vehicle design model, a multi-point air taxi
mission analysis model, and a single-disciplinary rotor model, respectively.
For three of the four test problems, AMTC reduces the model evaluation cost by
between 50% and 90%, making the full-grid NIPC the most efficacious method to
use among the UQ methods implemented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonlinear dynamic analysis of shear- and torsion-free rods using
  isogeometric discretization and outlier removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10652v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10652v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi-Hoa Nguyen, Bruno A. Roccia, René R. Hiemstra, Cristian G. Gebhardt, Dominik Schillinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a discrete formulation of nonlinear shear- and
torsion-free rods introduced by Gebhardt and Romero in [20] that uses
isogeometric discretization and robust time integration. Omitting the director
as an independent variable field, we reduce the number of degrees of freedom
and obtain discrete solutions in multiple copies of the Euclidean space (R^3),
which is larger than the corresponding multiple copies of the manifold (R^3 x
S^2) obtained with standard Hermite finite elements. For implicit time
integration, we choose the same integration scheme as Gebhardt and Romero in
[20] that is a hybrid form of the midpoint and the trapezoidal rules. In
addition, we apply a recently introduced approach for outlier removal by
Hiemstra et al. [26] that reduces high-frequency content in the response
without affecting the accuracy, ensuring robustness of our nonlinear discrete
formulation. We illustrate the efficiency of our nonlinear discrete formulation
for static and transient rods under different loading conditions, demonstrating
good accuracy in space, time and the frequency domain. Our numerical example
coincides with a relevant application case, the simulation of mooring lines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-21T00:00:00Z">2024-03-21</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Deep Operator Networks to Predict Multiphysics Solution Fields
  in Materials Processing and Additive Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Kushwaha, Jaewan Park, Seid Koric, Junyan He, Iwona Jasiuk, Diab Abueidda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike classical artificial neural networks, which require retraining for
each new set of parametric inputs, the Deep Operator Network (DeepONet), a
lately introduced deep learning framework, approximates linear and nonlinear
solution operators by taking parametric functions (infinite-dimensional
objects) as inputs and mapping them to complete solution fields. In this paper,
two newly devised DeepONet formulations with sequential learning and Residual
U-Net (ResUNet) architectures are trained for the first time to simultaneously
predict complete thermal and mechanical solution fields under variable loading,
loading histories, process parameters, and even variable geometries. Two
real-world applications are demonstrated: 1- coupled thermo-mechanical analysis
of steel continuous casting with multiple visco-plastic constitutive laws and
2- sequentially coupled direct energy deposition for additive manufacturing.
Despite highly challenging spatially variable target stress distributions,
DeepONets can infer reasonably accurate full-field temperature and stress
solutions several orders of magnitude faster than traditional and highly
optimized finite-element analysis (FEA), even when FEA simulations are run on
the latest high-performance computing platforms. The proposed DeepONet model's
ability to provide field predictions almost instantly for unseen input
parameters opens the door for future preliminary evaluation and design
optimization of these vital industrial processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geom-DeepONet: A Point-cloud-based Deep Operator Network for Field
  <span class="highlight-title">Prediction</span>s on 3D Parameterized Geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyan He, Seid Koric, Diab Abueidda, Ali Najafi, Iwona Jasiuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern digital engineering design process commonly involves expensive
repeated simulations on varying three-dimensional (3D) geometries. The
efficient prediction capability of neural networks (NNs) makes them a suitable
surrogate to provide design insights. Nevertheless, few available NNs can
handle solution prediction on varying 3D shapes. We present a novel deep
operator network (DeepONet) variant called Geom-DeepONet, which encodes
parameterized 3D geometries and predicts full-field solutions on an arbitrary
number of nodes. To the best of the authors' knowledge, this is the first
attempt in the literature and is our primary novelty. In addition to expressing
shapes using mesh coordinates, the signed distance function for each node is
evaluated and used to augment the inputs to the trunk network of the
Geom-DeepONet, thereby capturing both explicit and implicit representations of
the 3D shapes. The powerful geometric encoding capability of a sinusoidal
representation network (SIREN) is also exploited by replacing the classical
feedforward neural networks in the trunk with SIREN. Additional data fusion
between the branch and trunk networks is introduced by an element-wise product.
A numerical benchmark was conducted to compare Geom-DeepONet to PointNet and
vanilla DeepONet, where results show that our architecture trains fast with a
small memory footprint and yields the most accurate results among the three
with less than 2 MPa stress error. Results show a much lower generalization
error of our architecture on unseen dissimilar designs than vanilla DeepONet.
Once trained, the model can predict vector solutions, and speed can be over
$10^5$ times faster than implicit finite element simulations for large meshes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universal Differential Equations as a Common Modeling Language for
  Neuroscience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed ElGazzar, Marcel van Gerven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unprecedented availability of large-scale datasets in neuroscience has
spurred the exploration of artificial deep neural networks (DNNs) both as
empirical tools and as models of natural neural systems. Their appeal lies in
their ability to approximate arbitrary functions directly from observations,
circumventing the need for cumbersome mechanistic modeling. However, without
appropriate constraints, DNNs risk producing implausible models, diminishing
their scientific value. Moreover, the interpretability of DNNs poses a
significant challenge, particularly with the adoption of more complex
expressive architectures. In this perspective, we argue for universal
differential equations (UDEs) as a unifying approach for model development and
validation in neuroscience. UDEs view differential equations as
parameterizable, differentiable mathematical objects that can be augmented and
trained with scalable deep learning techniques. This synergy facilitates the
integration of decades of extensive literature in calculus, numerical analysis,
and neural modeling with emerging advancements in AI into a potent framework.
We provide a primer on this burgeoning topic in scientific machine learning and
demonstrate how UDEs fill in a critical gap between mechanistic,
phenomenological, and data-driven models in neuroscience. We outline a flexible
recipe for modeling neural systems with UDEs and discuss how they can offer
principled solutions to inherent challenges across diverse neuroscience
applications such as understanding neural computation, controlling neural
systems, neural decoding, and normative modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Hendrik Bastek, WaiChing Sun, Dennis M. Kochmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models such as denoising diffusion models are quickly advancing
their ability to approximate highly complex data distributions. They are also
increasingly leveraged in scientific machine learning, where samples from the
implied data distribution are expected to adhere to specific governing
equations. We present a framework to inform denoising diffusion models on
underlying constraints on such generated samples during model training. Our
approach improves the alignment of the generated samples with the imposed
constraints and significantly outperforms existing methods without affecting
inference speed. Additionally, our findings suggest that incorporating such
constraints during training provides a natural regularization against
overfitting. Our framework is easy to implement and versatile in its
applicability for imposing equality and inequality constraints as well as
auxiliary optimization objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffSTOCK: Probabilistic relational Stock Market <span class="highlight-title">Prediction</span>s using
  Diffusion Models <span class="chip">SP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyanshu Daiya, Monika Yadav, Harshit Singh Rao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose an approach to generalize denoising diffusion
probabilistic models for stock market predictions and portfolio management.
Present works have demonstrated the efficacy of modeling interstock relations
for market time-series forecasting and utilized Graph-based learning models for
value prediction and portfolio management. Though convincing, these
deterministic approaches still fall short of handling uncertainties i.e., due
to the low signal-to-noise ratio of the financial data, it is quite challenging
to learn effective deterministic models. Since the probabilistic methods have
shown to effectively emulate higher uncertainties for time-series predictions.
To this end, we showcase effective utilisation of Denoising Diffusion
Probabilistic Models (DDPM), to develop an architecture for providing better
market predictions conditioned on the historical financial indicators and
inter-stock relations. Additionally, we also provide a novel deterministic
architecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit
inter-stock relations along with historical stock features. We demonstrate that
our model achieves SOTA performance for movement predication and Portfolio
management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation to the 2024 IEEE International Conference
  on Acoustics, Speech, and Signal Processing (ICASSP 2024), Seoul, Korea</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I-FENN for thermoelasticity based on physics-informed temporal
  convolutional network (PI-TCN) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17799v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17799v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diab W. Abueidda, Mostafa E. Mobasher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most currently available methods for modeling multiphysics, including
thermoelasticity, using machine learning approaches, are focused on solving
complete multiphysics problems using data-driven or physics-informed
multi-layer perceptron (MLP) networks. Such models rely on incremental
step-wise training of the MLPs, and lead to elevated computational expense;
they also lack the rigor of existing numerical methods like the finite element
method. We propose an integrated finite element neural network (I-FENN)
framework to expedite the solution of coupled transient thermoelasticity. A
novel physics-informed temporal convolutional network (PI-TCN) is developed and
embedded within the finite element framework to leverage the fast inference of
neural networks (NNs). The PI-TCN model captures some of the fields in the
multiphysics problem; then, the network output is used to compute the other
fields of interest using the finite element method. We establish a framework
that computationally decouples the energy equation from the linear momentum
equation. We first develop a PI-TCN model to predict the spatiotemporal
evolution of the temperature field across the simulation time based on the
energy equation and strain data. The PI-TCN model is integrated into the finite
element framework, where the PI-TCN output (temperature) is used to introduce
the temperature effect to the linear momentum equation. The finite element
problem is solved using the implicit Euler time discretization scheme,
resulting in a computational cost comparable to that of a weakly-coupled
thermoelasticity problem but with the ability to solve fully-coupled problems.
Finally, we demonstrate I-FENN's computational efficiency and generalization
capability in thermoelasticity through several numerical examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FloodGenome: Interpretable Machine Learning for Decoding Features
  Shaping Property Flood Risk Predisposition in Cities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenyue Liu, Ali Mostafavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the fundamental characteristics that shape the inherent flood
risk disposition of urban areas is critical for integrated urban design
strategies for flood risk reduction. Flood risk disposition specifies an
inherent and event-independent magnitude of property flood risk and measures
the extent to which urban areas are susceptible to property damage if exposed
to a weather hazard. This study presents FloodGenome as an interpretable
machine learning model for evaluation of the extent to which various
hydrological, topographic, and built-environment features and their
interactions shape flood risk disposition in urban areas. Using flood damage
claims data from the U.S. National Flood Insurance Program covering the period
2003 through 2023 across four metropolitan statistical areas (MSAs), the
analysis computes building damage ratios and flood claim counts by employing
k-means clustering for classifying census block groups (CBGs) into distinct
property flood risk disposition levels. Then a random forest model is created
to specify property flood risk levels of CBGs based on various intertwined
hydrological, topographic, and built-environment features. The model
transferability analysis results show consistent performance across MSAs,
revealing the universality of underlying features that shape city property
flood risks. The FloodGenome model is then used to:(1) evaluate the extent to
which future urban development would exacerbate flood risk disposition of urban
areas; and (2) specify property flood risk levels at finer spatial resolution
providing critical insights for flood risk management processes. The
FloodGenome model and the findings provide novel tools and insights for
improving the characterization and understanding of intertwined features that
shape flood risk profiles of cities.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-03-20T00:00:00Z">2024-03-20</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computational Engineering
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the <span class="highlight-title">Adaptive</span> Moment Estimation (ADAM) stochastic optimizer
  through an Implicit-Explicit (IMEX) time-stepping approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinab Bhattacharjee, Andrey A. Popov, Arash Sarshar, Adrian Sandu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Adam optimizer, often used in Machine Learning for neural network
training, corresponds to an underlying ordinary differential equation (ODE) in
the limit of very small learning rates. This work shows that the classical Adam
algorithm is a first order implicit-explicit (IMEX) Euler discretization of the
underlying ODE. Employing the time discretization point of view, we propose new
extensions of the Adam scheme obtained by using higher order IMEX methods to
solve the ODE. Based on this approach, we derive a new optimization algorithm
for neural network training that performs better than classical Adam on several
regression and classification problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A globalized and preconditioned Newton-CG solver for metric-aware curved
  high-order mesh optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Aparicio-Estrems, Abel Gargallo-Peiró, Xevi Roca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a specific-purpose globalized and preconditioned Newton-CG solver
to minimize a metric-aware curved high-order mesh distortion. The solver is
specially devised to optimize curved high-order meshes for high polynomial
degrees with a target metric featuring non-uniform sizing, high stretching
ratios, and curved alignment -- exactly the features that stiffen the
optimization problem. To this end, we consider two ingredients: a
specific-purpose globalization and a specific-purpose
Jacobi-$\text{iLDL}^{\text{T}}(0)$ preconditioning with varying accuracy and
curvature tolerances (dynamic forcing terms) for the CG method. These
improvements are critical in stiff problems because, without them, the large
number of non-linear and linear iterations makes curved optimization
impractical. Finally, to analyze the performance of our method, the results
compare the specific-purpose solver with standard optimization methods. For
this, we measure the matrix-vector products indicating the solver computational
cost and the line-search iterations indicating the total amount of objective
function evaluations. When we combine the globalization and the linear solver
ingredients, we conclude that the specific-purpose Newton-CG solver reduces the
total number of matrix-vector products by one order of magnitude. Moreover, the
number of non-linear and line-search iterations is mainly smaller but of
similar magnitude.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>64 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Defining metric-aware size-shape measures to validate and optimize
  curved high-order meshes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillermo Aparicio-Estrems, Abel Gargallo-Peiró, Xevi Roca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We define a regularized size-shape distortion (quality) measure for curved
high-order elements on a Riemannian space. To this end, we measure the
deviation of a given element, straight-sided or curved, from the stretching,
alignment, and sizing determined by a target metric. The defined distortion
(quality) is suitable to check the validity and the quality of straight-sided
and curved elements on Riemannian spaces determined by constant and point-wise
varying metrics. The examples illustrate that the distortion can be minimized
to curve (deform) the elements of a given high-order (linear) mesh and try to
match with curved (linear) elements the point-wise stretching, alignment, and
sizing of a discrete target metric tensor. In addition, the resulting meshes
simultaneously match the curved features of the target metric and boundary.
Finally, to verify if the minimization of the metric-aware size-shape
distortion leads to meshes approximating the target metric, we compute the
Riemannian measures for the element edges, faces, and cells. The results show
that, when compared to anisotropic straight-sided meshes, the Riemannian
measures of the curved high-order mesh entities are closer to unit.
Furthermore, the optimized meshes illustrate the potential of curved
$r$-adaptation to improve the accuracy of a function representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>49 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient numerical methods for the Maxey-Riley equations with Basset
  history term 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julio Urizarna-Carasa, Leon Schlegel, Daniel Ruprecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Maxey-Riley equations (MRE) describe the motion of a finite-sized,
spherical particle in a fluid. Because of wake effects, the force acting on a
particle depends on its past trajectory. This is modelled by an integral term
in the MRE, also called Basset force, that makes its numerical solution
challenging and memory intensive. A recent approach proposed by Prasath, Vasan
and Govindarajan exploits connections between the integral term and fractional
derivatives to reformulate the MRE as a time-dependent partial differential
equation on a semi-infinite pseudo-space. They also propose a numerical
algorithm based on polynomial expansions. This paper develops a numerical
approach based on finite difference instead, by adopting techniques by Koleva
and Fazio and Janelli to cope with the issues of having an unbounded spatial
domain. We compare convergence order and computational efficiency for particles
of varying size and density of the polynomial expansion by Prasath et al., our
finite difference schemes and a direct integrator for the MRE based on
multi-step methods proposed by Daitche.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Geometry Models for Texture Synthesis of Machined Metallic
  Surfaces: Sandblasting and Milling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Natascha Jeziorski, Claudia Redenbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training defect detection algorithms for visual surface inspection systems
requires a large and representative set of training data. Often there is not
enough real data available which additionally cannot cover the variety of
possible defects. Synthetic data generated by a synthetic visual surface
inspection environment can overcome this problem. Therefore, a digital twin of
the object is needed, whose micro-scale surface topography is modeled by
texture synthesis models. We develop stochastic texture models for sandblasted
and milled surfaces based on topography measurements of such surfaces. As the
surface patterns differ significantly, we use separate modeling approaches for
the two cases. Sandblasted surfaces are modeled by a combination of data-based
texture synthesis methods that rely entirely on the measurements. In contrast,
the model for milled surfaces is procedural and includes all process-related
parameters known from the machine settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting and Triaging Spoofing using Temporal Convolutional Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushalya Kularatnam, Tania Stathaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As algorithmic trading and electronic markets continue to transform the
landscape of financial markets, detecting and deterring rogue agents to
maintain a fair and efficient marketplace is crucial. The explosion of large
datasets and the continually changing tricks of the trade make it difficult to
adapt to new market conditions and detect bad actors. To that end, we propose a
framework that can be adapted easily to various problems in the space of
detecting market manipulation. Our approach entails initially employing a
labelling algorithm which we use to create a training set to learn a weakly
supervised model to identify potentially suspicious sequences of order book
states. The main goal here is to learn a representation of the order book that
can be used to easily compare future events. Subsequently, we posit the
incorporation of expert assessment to scrutinize specific flagged order book
states. In the event of an expert's unavailability, recourse is taken to the
application of a more complex algorithm on the identified suspicious order book
states. We then conduct a similarity search between any new representation of
the order book against the expert labelled representations to rank the results
of the weak learner. We show some preliminary results that are promising to
explore further in this direction
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the moments of Cox rate-and-state models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Z. Baki, M. N. M. van Lieshout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rate-and-state models are widely used physical models for the relation
between changes in pore pressure due to fluid injection or gas extraction and
the induced seismic hazard in a field. We consider the modification where the
pore pressure measurements are affected by noise and provide explicit
expressions for the first and second moments of the state variable. We show
that when the pressure increases, there is positive correlation. In the case of
decreasing pressure, both positive and negative correlation is possible. Using
the delta method, approximate first and second moments of the rate variable are
derived and compared to empirical moments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Influence of concentration-dependent material properties on the fracture
  and debonding of electrode particles with core-shell structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Y. Tu, B. Wu, W. Ai, E. Martínez-Pañeda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Core-shell electrode particle designs offer a route to improved lithium-ion
battery performance. However, they are susceptible to mechanical damage such as
fracture and debonding, which can significantly reduce their lifetime. Using a
coupled finite element model, we explore the impacts of diffusion-induced
stresses on the failure mechanisms of an exemplar system with an NMC811 core
and an NMC111 shell. In particular, we systematically compare the implications
of assuming constant material properties against using Li
concentration-dependent diffusion coefficient and partial molar volume. With
constant material properties, our results show that smaller cores with thinner
shells avoid debonding and fracture regimes. When factoring in a
concentration-dependent partial molar volume, the maximum values of tensile
hoop stress in the shell are found to be significantly lower than those
predicted with constant properties, reducing the likelihood of fracture.
Furthermore, with a concentration-dependent diffusion coefficient, significant
barriers to full electrode utilisation are observed due to reduced lithium
mobility at high states of lithiation. This provides a possible explanation for
the reduced accessible capacity observed in experiments. Shell thickness is
found to be the dominant factor in precluding structural integrity once the
concentration dependency is accounted for. These findings shed new light on the
performance and effective design of core-shell electrode particles.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A characteristics-based method for shock-ramp data analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13380v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13380v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingxiang Shen, Wei Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the data analysis problem of shock-ramp compression, i.e., ramp
compression after a relatively strong initial shock, a characteristics-based
method that strictly deals with the initial hydrodynamic shock is described in
detail. Validation of this analysis method using simulated shock-ramp data
generated by molecular dynamics and one-dimensional radiation hydrodynamic code
is also presented.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Application of advanced ultrasonic testing methods to Dissimilar Metal
  Welds -- Comparison of simulated and experimental results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Audrey Gardahaut, Hugues Lourme, Steve Mahaut, Masaki Nagai, Shan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Widely present in the primary circuit of Nuclear Power Plants (NPP),
Dissimilar Metal Welds (DMW) are inspected using Ultrasonic nondestructive
Testing (UT) techniques to ensure the integrity of the structure and detect
defects such as Stress Corrosion Cracking (SCC).In a previous collaborative
research, CRIEPI and CEA have worked on the understanding of the propagation of
ultrasonic waves in complex materials. Indeed, the ultrasonic propagation can
be disturbed due to the anisotropic and inhomogeneous properties of the medium
and the interpretation of inspection results can then be difficult. An
analytical model, based on a dynamic ray theory, developed by CEA-LIST and
implemented in the CIVA software had been used to predict the ultrasonic
propagation in a DMW. The model evaluates the ray trajectories, the travel-time
and the computation of the amplitude along the ray tube in a medium described
thanks to a continuously varying description of its physical properties. In
this study, the weld had been described by an analytical law of the
crystallographic orientation. The simulated results of the detection of
calibrated notches located in the buttering and the weld had been compared with
experimental data and had shown a good agreement.The new collaborative program
presented in this paper aims at detecting a real SCC defect located close to
the root of the DMW. Thus, simulations have been performed for a DMW described
with an analytical law and a smooth cartography of the crystallographic
orientation. Furthermore, advanced ultrasonic testing methods have been used to
inspect the specimen and detect the real SCC defect. Experimental and simulated
results of the mock-up inspection have been compared.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Machine Learning Approach for Multiscale Modeling of the Facet
  Capsular Ligament 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob S. Merson, Nishan Parvez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a new neural network architecture that strictly enforces
constitutive constraints such as polyconvexity, frame-indifference, zero strain
energy with zero deformations, and the symmetry of the stress and material
stiffness. Additionally, we show that for this neural network, the accuracy is
significantly improved by using a Sobolev minimization strategy that includes
derivative terms. Using our network and Sobolev minimization, we obtain a NMSE
of 0.15% for the energy, 0.815% averaged across the components of the stress,
and 5.4% averaged across the components of the stiffness. This machine learned
constitutive model was deployed in a finite element simulation of a facet
capsular ligament. The displacement fields and stress-strain curves where
compared to a multiscale simulation that required running on a GPU based
supercomputer. At 70% strain, the model using the neural network had less than
10% relative error in the mean stress value.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Canonical Descriptors for Periodic Lattice Truss Materials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Qi, Huai-Liang Zheng, Chen-xi Liu, Li MA, Kai-Uwe Schröder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For decades, aspects of the topological architecture, and of the mechanical
as well as other physical behaviors of periodic lattice truss materials (PLTMs)
have been massively studied. Their approximate infinite design space presents a
double-edged sword, implying on one hand dramatic designability in fulfilling
the requirement of various performance, but on the other hand unexpected
intractability in determining the best candidate with tailoring properties. In
recent years, the development of additive manufacturing and artificial
intelligence spurs an explosion in the methods exploring the design space and
searching its boundaries. However, regrettably, a normative description with
sufficient information of PLTMs applying to machine learning has not yet been
constructed, which confines the inverse design to some discrete and small
scrutinized space. In the current paper, we develop a system of canonical
descriptors for PLTMs, encoding not only the geometrical configurations but
also mechanical properties into matrix forms to establish good quantitative
correlations between structures and mechanical behaviors. The system mainly
consists of the geometry matrix for the lattice node configuration, density,
stretching and bending stiffness matrices for the lattice strut properties, as
well as packing matrix for the principal periodic orientation. All these
matrices are theoretically derived based on the intrinsic nature of PLTMs,
leading to concise descriptions and sufficient information. The
characteristics, including the completeness and uniqueness, of the descriptors
are analyzed. In addition, we discuss how the current system of descriptors can
be applied to the database construction and material discovery, and indicate
the possible open problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoQuest: Exploring Research Question Co-Creation with an LLM-based Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06155v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06155v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, Yun Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing novel research questions (RQs) often requires extensive literature
reviews, especially in interdisciplinary fields. To support RQ development
through human-AI co-creation, we leveraged Large Language Models (LLMs) to
build an LLM-based agent system named CoQuest. We conducted an experiment with
20 HCI researchers to examine the impact of two interaction designs:
breadth-first and depth-first RQ generation. The findings revealed that
participants perceived the breadth-first approach as more creative and
trustworthy upon task completion. Conversely, during the task, participants
considered the depth-first generated RQs as more creative. Additionally, we
discovered that AI processing delays allowed users to reflect on multiple RQs
simultaneously, leading to a higher quantity of generated RQs and an enhanced
sense of control. Our work makes both theoretical and practical contributions
by proposing and evaluating a mental model for human-AI co-creation of RQs. We
also address potential ethical issues, such as biases and over-reliance on AI,
advocating for using the system to improve human research creativity rather
than automating scientific inquiry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGCHI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-informed uncertainty quantification for laser-based powder bed
  fusion additive manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihaela Chiappetta, Chiara Piazzola, Lorenzo Tamellini, Alessandro Reali, Ferdinando Auricchio, Massimo Carraturo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an efficient approach to quantify the uncertainties associated
with the numerical simulations of the laser-based powder bed fusion of metals
processes. Our study focuses on a thermomechanical model of an Inconel 625
cantilever beam, based on the AMBench2018-01 benchmark proposed by the National
Institute of Standards and Technology (NIST). The proposed approach consists of
a forward uncertainty quantification analysis of the residual strains of the
cantilever beam given the uncertainty in some of the parameters of the
numerical simulation, namely the powder convection coefficient and the
activation temperature. The uncertainty on such parameters is modelled by a
data-informed probability density function obtained by a Bayesian inversion
procedure, based on the displacement experimental data provided by NIST. To
overcome the computational challenges of both the Bayesian inversion and the
forward uncertainty quantification analysis we employ a multi-fidelity
surrogate modelling technique, specifically the multi-index stochastic
collocation method. The proposed approach allows us to achieve a 33\% reduction
in the uncertainties on the prediction of residual strains compared with what
we would get basing the forward UQ analysis on a-priori ranges for the
uncertain parameters, and in particular the mode of the probability density
function of such quantities (i.e., its ``most likely value'', roughly speaking)
results to be in good agreement with the experimental data provided by NIST,
even though only displacement data were used for the Bayesian inversion
procedure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian estimation and uncertainty quantification of a
  temperature-dependent thermal conductivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo L. S. Silva, Clemens Verhoosel, Erik Quaeghebeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of estimating a temperature-dependent thermal
conductivity model (curve) from temperature measurements. We apply a Bayesian
estimation approach that takes into account measurement errors and limited
prior information of system properties. The approach intertwines system
simulation and Markov chain Monte Carlo (MCMC) sampling. We investigate the
impact of assuming different model classes - cubic polynomials and piecewise
linear functions - their parametrization, and different types of prior
information - ranging from uninformative to informative. Piecewise linear
functions require more parameters (conductivity values) to be estimated than
the four parameters (coefficients or conductivity values) needed for cubic
polynomials. The former model class is more flexible, but the latter requires
less MCMC samples. While parametrizing polynomials with coefficients may feel
more natural, it turns out that parametrizing them using conductivity values is
far more natural for the specification of prior information. Robust estimation
is possible for all model classes and parametrizations, as long as the prior
information is accurate or not too informative. Gaussian Markov random field
priors are especially well-suited for piecewise linear functions.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-03-28T05:26:39.314445010Z">
            2024-03-28 05:26:39 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
