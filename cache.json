{"2024-03-27T00:00:00Z":{"Systems and Control":[{"id":"http://arxiv.org/abs/2302.13483v4","updated":"2024-03-27T17:38:27Z","published":"2023-02-27T02:42:27Z","title":"CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems","summary":"  We present CrystalBox, a novel, model-agnostic, posthoc explainability\nframework for Deep Reinforcement Learning (DRL) controllers in the large family\nof input-driven environments which includes computer systems. We combine the\nnatural decomposability of reward functions in input-driven environments with\nthe explanatory power of decomposed returns. We propose an efficient algorithm\nto generate future-based explanations across both discrete and continuous\ncontrol environments. Using applications such as adaptive bitrate streaming and\ncongestion control, we demonstrate CrystalBox's capability to generate\nhigh-fidelity explanations. We further illustrate its higher utility across\nthree practical use cases: contrastive explanations, network observability, and\nguided reward design, as opposed to prior explainability techniques that\nidentify salient features.\n","authors":["Sagar Patel","Sangeetha Abdu Jyothi","Nina Narodytska"],"pdf_url":"https://arxiv.org/pdf/2302.13483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13967v3","updated":"2024-03-27T17:28:46Z","published":"2023-11-23T12:31:28Z","title":"Unconstrained learning of networked nonlinear systems via free\n  parametrization of stable interconnected operators","summary":"  This paper characterizes a new parametrization of nonlinear networked\nincrementally $L_2$-bounded operators in discrete time. The distinctive novelty\nis that our parametrization is \\emph{free} -- that is, a sparse large-scale\noperator with bounded incremental $L_2$ gain is obtained for any choice of the\nreal values of our parameters. This property allows one to freely search over\noptimal parameters via unconstrained gradient descent, enabling direct\napplications in large-scale optimal control and system identification. Further,\nwe can embed prior knowledge about the interconnection topology and stability\nproperties of the system directly into the large-scale distributed operator we\ndesign. Our approach is extremely general in that it can seamlessly encapsulate\nand interconnect state-of-the-art Neural Network (NN) parametrizations of\nstable dynamical systems. To demonstrate the effectiveness of this approach, we\nprovide a simulation example showcasing the identification of a networked\nnonlinear system. The results underscore the superiority of our free\nparametrizations over standard NN-based identification methods where a prior\nover the system topology and local stability properties are not enforced.\n","authors":["Leonardo Massai","Danilo Saccani","Luca Furieri","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2311.13967v3.pdf","comment":"Full version of the paper to appear at ECC 2024"},{"id":"http://arxiv.org/abs/2201.06180v2","updated":"2024-03-27T16:45:26Z","published":"2022-01-17T02:30:25Z","title":"Nonlinear Control Allocation: A Learning Based Approach","summary":"  Modern aircraft are designed with redundant control effectors to cater for\nfault tolerance and maneuverability requirements. This leads to aircraft being\nover-actuated and requires control allocation schemes to distribute the control\ncommands among control effectors. Traditionally, optimization-based control\nallocation schemes are used; however, for nonlinear allocation problems, these\nmethods require large computational resources. In this work, an artificial\nneural network (ANN) based nonlinear control allocation scheme is proposed. The\nproposed scheme is composed of learning the inverse of the control\neffectiveness map through ANN, and then implementing it as an allocator instead\nof solving an online optimization problem. Stability conditions are presented\nfor closed-loop systems incorporating the allocator, and computational\nchallenges are explored with piece-wise linear effectiveness functions and\nANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is\ncompared with a standard quadratic programming-based method for control\nallocation.\n","authors":["Hafiz Zeeshan Iqbal Khan","Surrayya Mobeen","Jahanzeb Rajput","Jamshed Riaz"],"pdf_url":"https://arxiv.org/pdf/2201.06180v2.pdf","comment":"submitted to IEEE Conference on Decision and Control (CDC), 2024"},{"id":"http://arxiv.org/abs/2403.18739v1","updated":"2024-03-27T16:32:32Z","published":"2024-03-27T16:32:32Z","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural\n  Networks","summary":"  Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.\n","authors":["Olov Holmer","Mattias Krysander","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.18739v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18707v1","updated":"2024-03-27T15:56:35Z","published":"2024-03-27T15:56:35Z","title":"Connections between Reachability and Time Optimality","summary":"  This paper presents the concept of an equivalence relation between the set of\noptimal control problems. By leveraging this concept, we show that the boundary\nof the reachability set can be constructed by the solutions of time optimal\nproblems. Alongside, a more generalized equivalence theorem is presented\ntogether. The findings facilitate the use of solution structures from a certain\nclass of optimal control problems to address problems in corresponding\nequivalent classes. As a byproduct, we state and prove the construction methods\nof the reachability sets of three-dimensional curves with prescribed curvature\nbound. The findings are twofold: Firstly, we prove that any boundary point of\nthe reachability set, with the terminal direction taken into account, can be\naccessed via curves of H, CSC, CCC, or their respective subsegments, where H\ndenotes a helicoidal arc, C a circular arc with maximum curvature, and S a\nstraight segment. Secondly, we show that any boundary point of the reachability\nset, without considering the terminal direction, can be accessed by curves of\nCC, CS, or their respective subsegments. These findings extend the developments\npresented in literature regarding planar curves, or Dubins car dynamics, into\nspatial curves in $\\mathbb{R}^3$. For higher dimensions, we confirm that the\nproblem of identifying the reachability set of curvature bounded paths subsumes\nthe well-known Markov-Dubins problem. These advancements in understanding the\nreachability of curvature bounded paths in $\\mathbb{R}^3$ hold significant\npractical implications, particularly in the contexts of mission planning\nproblems and time optimal guidance.\n","authors":["Juho Bae","Ji Hoon Bai","Byung-Yoon Lee","Jun-Yong Lee","Chang-Hun Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18707v1.pdf","comment":"Submitted to Automatica"},{"id":"http://arxiv.org/abs/2403.18703v1","updated":"2024-03-27T15:52:54Z","published":"2024-03-27T15:52:54Z","title":"Fpga-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koepp"],"pdf_url":"https://arxiv.org/pdf/2403.18703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18695v1","updated":"2024-03-27T15:44:25Z","published":"2024-03-27T15:44:25Z","title":"An Efficient Risk-aware Branch MPC for Automated Driving that is Robust\n  to Uncertain Vehicle Behaviors","summary":"  One of the critical challenges in automated driving is ensuring safety of\nautomated vehicles despite the unknown behavior of the other vehicles. Although\nmotion prediction modules are able to generate a probability distribution\nassociated with various behavior modes, their probabilistic estimates are often\ninaccurate, thus leading to a possibly unsafe trajectory. To overcome this\nchallenge, we propose a risk-aware motion planning framework that appropriately\naccounts for the ambiguity in the estimated probability distribution. We\nformulate the risk-aware motion planning problem as a min-max optimization\nproblem and develop an efficient iterative method by incorporating a\nregularization term in the probability update step. Via extensive numerical\nstudies, we validate the convergence of our method and demonstrate its\nadvantages compared to the state-of-the-art approaches.\n","authors":["Luyao Zhang","George Pantazis","Shaohang Han","Sergio Grammatico"],"pdf_url":"https://arxiv.org/pdf/2403.18695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18664v1","updated":"2024-03-27T15:08:00Z","published":"2024-03-27T15:08:00Z","title":"Neural Network-Based Piecewise Survival Models","summary":"  In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.\n","authors":["Olov Holmer","Erik Frisk","Mattias Krysander"],"pdf_url":"https://arxiv.org/pdf/2403.18664v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18650v1","updated":"2024-03-27T14:58:11Z","published":"2024-03-27T14:58:11Z","title":"MPC-CBF with Adaptive Safety Margins for Safety-critical Teleoperation\n  over Imperfect Network Connections","summary":"  The paper focuses on the design of a control strategy for safety-critical\nremote teleoperation. The main goal is to make the controlled system track the\ndesired velocity specified by an operator while avoiding obstacles despite\ncommunication delays. Control Barrier Functions (CBFs) are used to define the\nsafety constraints that the system has to respect to avoid obstacles, while\nModel Predictive Control (MPC) provides the framework for adjusting the desired\ninput, taking the constraints into account. The resulting input is sent to the\nremote system, where appropriate low-level velocity controllers translate it\ninto system-specific commands. The main novelty of the paper is a method to\nmake the CBFs robust against the uncertainties caused by the network delays\naffecting the system's state and do so in a less conservative manner. The\nresults show how the proposed method successfully solves the safety-critical\nteleoperation problem, making the controlled systems avoid obstacles with\ndifferent types of network delay. The controller has also been tested in\nsimulation and on a real manipulator, demonstrating its general applicability\nwhen reliable low-level velocity controllers are available.\n","authors":["Riccardo Periotto","Mina Ferizbegovic","Fernando S. Barbosa","Roberto C. Sundin"],"pdf_url":"https://arxiv.org/pdf/2403.18650v1.pdf","comment":"Accepted for publication in the 2024 European Control Conference\n  (ECC)"},{"id":"http://arxiv.org/abs/2403.18649v1","updated":"2024-03-27T14:56:44Z","published":"2024-03-27T14:56:44Z","title":"Addressing Data Annotation Challenges in Multiple Sensors: A Solution\n  for Scania Collected Datasets","summary":"  Data annotation in autonomous vehicles is a critical step in the development\nof Deep Neural Network (DNN) based models or the performance evaluation of the\nperception system. This often takes the form of adding 3D bounding boxes on\ntime-sequential and registered series of point-sets captured from active\nsensors like Light Detection and Ranging (LiDAR) and Radio Detection and\nRanging (RADAR). When annotating multiple active sensors, there is a need to\nmotion compensate and translate the points to a consistent coordinate frame and\ntimestamp respectively. However, highly dynamic objects pose a unique\nchallenge, as they can appear at different timestamps in each sensor's data.\nWithout knowing the speed of the objects, their position appears to be\ndifferent in different sensor outputs. Thus, even after motion compensation,\nhighly dynamic objects are not matched from multiple sensors in the same frame,\nand human annotators struggle to add unique bounding boxes that capture all\nobjects. This article focuses on addressing this challenge, primarily within\nthe context of Scania collected datasets. The proposed solution takes a track\nof an annotated object as input and uses the Moving Horizon Estimation (MHE) to\nrobustly estimate its speed. The estimated speed profile is utilized to correct\nthe position of the annotated box and add boxes to object clusters missed by\nthe original annotation.\n","authors":["Ajinkya Khoche","Aron Asefaw","Alejandro Gonzalez","Bogdan Timus","Sina Sharif Mansouri","Patric Jensfelt"],"pdf_url":"https://arxiv.org/pdf/2403.18649v1.pdf","comment":"Accepted to European Control Conference 2024"},{"id":"http://arxiv.org/abs/2403.18632v1","updated":"2024-03-27T14:38:22Z","published":"2024-03-27T14:38:22Z","title":"Optimal Control Synthesis of Markov Decision Processes for Efficiency\n  with Surveillance Tasks","summary":"  We investigate the problem of optimal control synthesis for Markov Decision\nProcesses (MDPs), addressing both qualitative and quantitative objectives.\nSpecifically, we require the system to fulfill a qualitative surveillance task\nin the sense that a specific region of interest can be visited infinitely often\nwith probability one. Furthermore, to quantify the performance of the system,\nwe consider the concept of efficiency, which is defined as the ratio between\nrewards and costs. This measure is more general than the standard long-run\naverage reward metric as it aims to maximize the reward obtained per unit cost.\nOur objective is to synthesize a control policy that ensures the surveillance\ntask while maximizes the efficiency. We provide an effective approach to\nsynthesize a stationary control policy achieving $\\epsilon$-optimality by\nintegrating state classifications of MDPs and perturbation analysis in a novel\nmanner. Our results generalize existing works on efficiency-optimal control\nsynthesis for MDP by incorporating qualitative surveillance tasks. A robot\nmotion planning case study is provided to illustrate the proposed algorithm.\n","authors":["Yu Chen","Xuanyuan Yin","Shaoyuan Li","Xiang Yin"],"pdf_url":"https://arxiv.org/pdf/2403.18632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.16075v4","updated":"2024-03-27T14:29:24Z","published":"2023-07-29T20:57:34Z","title":"Redesigning Large-Scale Multimodal Transit Networks with Shared\n  Autonomous Mobility Services","summary":"  This study addresses a large-scale multimodal transit network design problem,\nwith Shared Autonomous Mobility Services (SAMS) as both transit feeders and an\norigin-to-destination mode. The framework captures spatial demand and modal\ncharacteristics, considers intermodal transfers and express services,\ndetermines transit infrastructure investment and path flows, and generates\ntransit routes. A system-optimal multimodal transit network is designed with\nminimum total door-to-door generalized costs of users and operators, satisfying\ntransit origin-destination demand within a pre-set infrastructure budget.\nFirstly, the geography, demand, and modes in each zone are characterized with\ncontinuous approximation. The decisions of network link investment and\nmultimodal path flows in zonal connection optimization are formulated as a\nminimum-cost multi-commodity network flow (MCNF) problem and solved efficiently\nwith a mixed-integer linear programming (MILP) solver. Subsequently, the route\ngeneration problem is solved by expanding the MCNF formulation to minimize\nintramodal transfers. The model is illustrated through a set of experiments\nwith the Chicago network comprised of 50 zones and seven modes, under three\nscenarios. The computational results present savings in traveler journey time\nand operator cost demonstrating the potential benefits of collaboration between\nmultimodal transit systems and SAMS.\n","authors":["Max T. M. Ng","Hani S. Mahmassani","Ömer Verbas","Taner Cokyasar","Roman Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2307.16075v4.pdf","comment":"48 pages, 18 figures, accepted for publication in Transportation\n  Research Part C: Emerging Technologies, and presentation in the 25th\n  International Symposium on Transportation and Traffic Theory (ISTTT25)"},{"id":"http://arxiv.org/abs/2403.18588v1","updated":"2024-03-27T14:11:32Z","published":"2024-03-27T14:11:32Z","title":"From Virtual Reality to the Emerging Discipline of Perception\n  Engineering","summary":"  This paper makes the case that a powerful new discipline, which we term\nperception engineering, is steadily emerging. It follows from a progression of\nideas that involve creating illusions, from historical paintings and film, to\nvideo games and virtual reality in modern times. Rather than creating physical\nartifacts such as bridges, airplanes, or computers, perception engineers create\nillusory perceptual experiences. The scope is defined over any agent that\ninteracts with the physical world, including both biological organisms (humans,\nanimals) and engineered systems (robots, autonomous systems). The key idea is\nthat an agent, called a producer, alters the environment with the intent to\nalter the perceptual experience of another agent, called a receiver. Most\nimportantly, the paper introduces a precise mathematical formulation of this\nprocess, based on the von Neumann-Morgenstern notion of information, to help\nscope and define the discipline. It is then applied to the cases of engineered\nand biological agents with discussion of its implications on existing fields\nsuch as virtual reality, robotics, and even social media. Finally, open\nchallenges and opportunities for involvement are identified.\n","authors":["Steven M. LaValle","Evan G. Center","Timo Ojala","Matti Pouke","Nicoletta Prencipe","Basak Sakcak","Markku Suomalainen","Kalle G. Timperi","Vadim K. Weinstein"],"pdf_url":"https://arxiv.org/pdf/2403.18588v1.pdf","comment":"30 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.18571v1","updated":"2024-03-27T13:52:41Z","published":"2024-03-27T13:52:41Z","title":"Bootstrapping Guarantees: Stability and Performance Analysis for Dynamic\n  Encrypted Control","summary":"  Encrypted dynamic controllers that operate for an unlimited time have been a\nchallenging subject of research. The fundamental difficulty is the accumulation\nof errors and scaling factors in the internal state during operation.\nBootstrapping, a technique commonly employed in fully homomorphic\ncryptosystems, can be used to avoid overflows in the controller state but can\npotentially introduce significant numerical errors. In this paper, we analyze\ndynamic encrypted control with explicit consideration of bootstrapping. By\nrecognizing the bootstrapping errors occurring in the controller's state as an\nuncertainty in the robust control framework, we can provide stability and\nperformance guarantees for the whole encrypted control system. Further, the\nconservatism of the stability and performance test is reduced by using a lifted\nversion of the control system.\n","authors":["Sebastian Schlor","Frank Allgöwer"],"pdf_url":"https://arxiv.org/pdf/2403.18571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18564v1","updated":"2024-03-27T13:45:45Z","published":"2024-03-27T13:45:45Z","title":"Formal Verification with Constrained Polynomial Logical Zonotope","summary":"  In this paper, we propose using constrained polynomial logical zonotopes for\nformal verification of logical systems. We perform reachability analysis to\ncompute the set of states that could be reached. To do this, we utilize a\nrecently introduced set representation called polynomial logical zonotopes for\nperforming computationally efficient and exact reachability analysis on logical\nsystems. Notably, polynomial logical zonotopes address the \"curse of\ndimensionality\" when analyzing the reachability of logical systems since the\nset representation can represent 2^n binary vectors using n generators. After\nfinishing the reachability analysis, the formal verification involves verifying\nwhether the intersection of the calculated reachable set and the unsafe set is\nempty or not. However, polynomial logical zonotopes are not closed under\nintersections. To address this, we formulate constrained polynomial logical\nzonotopes, which maintain the computational efficiency and exactness of\npolynomial logical zonotopes for reachability analysis while supporting exact\nintersections. Furthermore, we present an extensive empirical study\nillustrating and verifying the benefits of using constrained polynomial logical\nzonotopes for the formal verification of logical systems.\n","authors":["Ahmad Hafez","Frank J. Jiang","Karl H. Johansson","Amr Alanwar"],"pdf_url":"https://arxiv.org/pdf/2403.18564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18561v1","updated":"2024-03-27T13:42:19Z","published":"2024-03-27T13:42:19Z","title":"A Dynamic Programming Approach for Road Traffic Estimation","summary":"  We consider a road network represented by a directed graph. We assume to\ncollect many measurements of traffic flows on all the network arcs, or on a\nsubset of them. We assume that the users are divided into different groups.\nEach group follows a different path. The flows of all user groups are modeled\nas a set of independent Poisson processes. Our focus is estimating the paths\nfollowed by each user group, and the means of the associated Poisson processes.\nWe present a possible solution based on a Dynamic Programming algorithm. The\nmethod relies on the knowledge of high order cumulants. We discuss the\ntheoretical properties of the introduced method. Finally, we present some\nnumerical tests on well-known benchmark networks, using synthetic data.\n","authors":["Mattia Laurini","Irene Saccani","Stefano Ardizzoni","Luca Consolini","Marco Locatelli"],"pdf_url":"https://arxiv.org/pdf/2403.18561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18557v1","updated":"2024-03-27T13:39:06Z","published":"2024-03-27T13:39:06Z","title":"Stability Properties of the Impulsive Goodwin's Oscillator in 1-cycle","summary":"  The Impulsive Goodwin's Oscillator (IGO) is a mathematical model of a hybrid\nclosed-loop system. It arises by closing a special kind of continuous linear\npositive time-invariant system with impulsive feedback, which employs both\namplitude and frequency pulse modulation. The structure of IGO precludes the\nexistence of equilibria, and all its solutions are oscillatory. With its origin\nin mathematical biology, the IGO also presents a control paradigm useful in a\nwide range of applications, in particular dosing of chemicals and medicines.\nSince the pulse modulation feedback mechanism introduces significant\nnonlinearity and non-smoothness in the closedloop dynamics, conventional\ncontroller design methods fail to apply. However, the hybrid dynamics of IGO\nreduce to a nonlinear, time-invariant discrete-time system, exhibiting a\none-to-one correspondence between periodic solutions of the original IGO and\nthose of the discrete-time system. The paper proposes a design approach that\nleverages the linearization of the equivalent discrete-time dynamics in the\nvicinity of a fixed point. A simple and efficient local stability condition of\nthe 1-cycle in terms of the characteristics of the amplitude and frequency\nmodulation functions is obtained.\n","authors":["Anton V. Proskurnikov","Alexander Medvedev"],"pdf_url":"https://arxiv.org/pdf/2403.18557v1.pdf","comment":"submitted to IEEE CDC 2024"},{"id":"http://arxiv.org/abs/2403.18539v1","updated":"2024-03-27T13:14:29Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10483v2","updated":"2024-03-27T13:11:53Z","published":"2023-08-21T05:54:57Z","title":"Aggregate Model of District Heating Network for Integrated Energy\n  Dispatch: A Physically Informed Data-Driven Approach","summary":"  The district heating network (DHN) is essential in enhancing the operational\nflexibility of integrated energy systems (IES). Yet, it is hard to obtain an\naccurate and concise DHN model for the operation owing to complicated network\nfeatures and imperfect measurements. Considering this, this paper proposes a\nphysical-ly informed data-driven aggregate model (AGM) for the DHN, providing a\nconcise description of the source-load relationship of DHN without exposing\nnetwork details. First, we derive the analytical relationship between the state\nvariables of the source and load nodes of the DHN, offering a physical\nfundament for the AGM. Second, we propose a physics-informed estimator for the\nAGM that is robust to low-quality measurements, in which the physical\nconstraints associated with the parameter normalization and sparsity are\nembedded to improve the accuracy and robustness. Finally, we propose a\nphysics-enhanced algorithm to solve the nonlinear estimator with non-closed\nconstraints efficiently. Simulation results verify the effectiveness of the\nproposed method.\n","authors":["Shuai Lu","Zihang Gao","Yong Sun","Suhan Zhang","Baoju Li","Chengliang Hao","Yijun Xu","Wei Gu"],"pdf_url":"https://arxiv.org/pdf/2308.10483v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17392v2","updated":"2024-03-27T12:10:00Z","published":"2024-03-26T05:23:12Z","title":"Natural-artificial hybrid swarm: Cyborg-insect group navigation in\n  unknown obstructed soft terrain","summary":"  Navigating multi-robot systems in complex terrains has always been a\nchallenging task. This is due to the inherent limitations of traditional robots\nin collision avoidance, adaptation to unknown environments, and sustained\nenergy efficiency. In order to overcome these limitations, this research\nproposes a solution by integrating living insects with miniature electronic\ncontrollers to enable robotic-like programmable control, and proposing a novel\ncontrol algorithm for swarming. Although these creatures, called cyborg\ninsects, have the ability to instinctively avoid collisions with neighbors and\nobstacles while adapting to complex terrains, there is a lack of literature on\nthe control of multi-cyborg systems. This research gap is due to the difficulty\nin coordinating the movements of a cyborg system under the presence of insects'\ninherent individual variability in their reactions to control input. In\nresponse to this issue, we propose a novel swarm navigation algorithm\naddressing these challenges. The effectiveness of the algorithm is demonstrated\nthrough an experimental validation in which a cyborg swarm was successfully\nnavigated through an unknown sandy field with obstacles and hills. This\nresearch contributes to the domain of swarm robotics and showcases the\npotential of integrating biological organisms with robotics and control theory\nto create more intelligent autonomous systems with real-world applications.\n","authors":["Yang Bai","Phuoc Thanh Tran Ngoc","Huu Duoc Nguyen","Duc Long Le","Quang Huy Ha","Kazuki Kai","Yu Xiang See To","Yaosheng Deng","Jie Song","Naoki Wakamiya","Hirotaka Sato","Masaki Ogura"],"pdf_url":"https://arxiv.org/pdf/2403.17392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08787v2","updated":"2024-03-27T11:06:49Z","published":"2023-11-15T08:59:05Z","title":"Polygonal Cone Control Barrier Functions (PolyC2BF) for safe navigation\n  in cluttered environments","summary":"  In fields such as mining, search and rescue, and archaeological exploration,\nensuring real-time, collision-free navigation of robots in confined, cluttered\nenvironments is imperative. Despite the value of established path planning\nalgorithms, they often face challenges in convergence rates and handling\ndynamic infeasibilities. Alternative techniques like collision cones struggle\nto accurately represent complex obstacle geometries. This paper introduces a\nnovel category of control barrier functions, known as Polygonal Cone Control\nBarrier Function (PolyC2BF), which addresses overestimation and computational\ncomplexity issues. The proposed PolyC2BF, formulated as a Quadratic Programming\n(QP) problem, proves effective in facilitating collision-free movement of\nmultiple robots in complex environments. The efficacy of this approach is\nfurther demonstrated through PyBullet simulations on quadruped (unicycle\nmodel), and crazyflie 2.1 (quadrotor model) in cluttered environments.\n","authors":["Manan Tayal","Shishir Kolathaya"],"pdf_url":"https://arxiv.org/pdf/2311.08787v2.pdf","comment":"6 Pages, 6 Figures. Accepted at European Control Conference (ECC)\n  2024. arXiv admin note: text overlap with arXiv:2303.15871"},{"id":"http://arxiv.org/abs/2402.01216v2","updated":"2024-03-27T10:25:07Z","published":"2024-02-02T08:39:39Z","title":"Robust Commutation Design: Applied to Switched Reluctance Motors","summary":"  Switched Reluctance Motors (SRMs) are cost-effective electric actuators that\nutilize magnetic reluctance to generate torque, with torque ripple arising from\nunaccounted manufacturing defects in the rotor tooth geometry. This paper aims\nto design a versatile, resource-efficient commutation function for accurate\ncontrol of a range of SRMs, mitigating torque ripple despite manufacturing\nvariations across SRMs and individual rotor teeth. The developed commutation\nfunction optimally distributes current between coils by leveraging the variance\nin the torque-current-angle model and is designed with few parameters for easy\nintegration on affordable hardware. Monte Carlo simulations and experimental\nresults show a tracking error reduction of up to 31% and 11%, respectively. The\ndeveloped approach is beneficial for applications using a single driver for\nmultiple systems and those constrained by memory or modeling effort, providing\nan economical solution for improved tracking performance and reduced acoustic\nnoise.\n","authors":["Max van Meer","Gert Witvoet","Tom Oomen"],"pdf_url":"https://arxiv.org/pdf/2402.01216v2.pdf","comment":"6 pages, 7 figures. Final version"},{"id":"http://arxiv.org/abs/2309.07798v2","updated":"2024-03-27T10:23:01Z","published":"2023-09-14T15:36:59Z","title":"Enhancing Performance, Calibration Time and Efficiency in Brain-Machine\n  Interfaces through Transfer Learning and Wearable EEG Technology","summary":"  Brain-machine interfaces (BMIs) have emerged as a transformative force in\nassistive technologies, empowering individuals with motor impairments by\nenabling device control and facilitating functional recovery. However, the\npersistent challenge of inter-session variability poses a significant hurdle,\nrequiring time-consuming calibration at every new use. Compounding this issue,\nthe low comfort level of current devices further restricts their usage. To\naddress these challenges, we propose a comprehensive solution that combines a\ntiny CNN-based Transfer Learning (TL) approach with a comfortable, wearable EEG\nheadband. The novel wearable EEG device features soft dry electrodes placed on\nthe headband and is capable of on-board processing. We acquire multiple\nsessions of motor-movement EEG data and achieve up to 96% inter-session\naccuracy using TL, greatly reducing the calibration time and improving\nusability. By executing the inference on the edge every 100ms, the system is\nestimated to achieve 30h of battery life. The comfortable BMI setup with tiny\nCNN and TL paves the way to future on-device continual learning, essential for\ntackling inter-session variability and improving usability.\n","authors":["Xiaying Wang","Lan Mei","Victor Kartsch","Andrea Cossettini","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2309.07798v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18422v1","updated":"2024-03-27T10:21:07Z","published":"2024-03-27T10:21:07Z","title":"Feedback Linearizable Discretizations of Second Order Mechanical Systems\n  using Retraction Maps","summary":"  Mechanical systems, in nature, are often described by a set of\ncontinuous-time, nonlinear, second-order differential equations (SODEs). This\nhas motivated designs of various control laws implemented on digital\ncontrollers, consequently requiring numerical discretization schemes. Feedback\nlinearizability of such sampled systems depends on the discretization scheme or\nmap choice. In this article, we utilize retraction maps and their lifts to\nconstruct feedback linearizable discretizations for SODEs, which can be applied\nto various mechanical systems.\n","authors":["Shreyas N. B.","David Martin Diego","Ravi Banavar"],"pdf_url":"https://arxiv.org/pdf/2403.18422v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18413v1","updated":"2024-03-27T10:01:14Z","published":"2024-03-27T10:01:14Z","title":"HyRRT-Connect: A Bidirectional Rapidly-Exploring Random Trees Motion\n  Planning Algorithm for Hybrid Systems","summary":"  This paper proposes a bidirectional rapidly-exploring random trees (RRT)\nalgorithm to solve the motion planning problem for hybrid systems. The proposed\nalgorithm, called HyRRT-Connect, propagates in both forward and backward\ndirections in hybrid time until an overlap between the forward and backward\npropagation results is detected. Then, HyRRT-Connect constructs a motion plan\nthrough the reversal and concatenation of functions defined on hybrid time\ndomains, ensuring the motion plan thoroughly satisfies the given hybrid\ndynamics. To address the potential discontinuity along the flow caused by\ntolerating some distance between the forward and backward partial motion plans,\nwe reconstruct the backward partial motion plan by a forward-in-hybrid-time\nsimulation from the final state of the forward partial motion plan. By applying\nthe reversed input of the backward partial motion plan, the reconstruction\nprocess effectively eliminates the discontinuity and ensures that as the\ntolerance distance decreases to zero, the distance between the endpoint of the\nreconstructed motion plan and the final state set approaches zero. The proposed\nalgorithm is applied to an actuated bouncing ball example and a walking robot\nexample so as to highlight its generality and computational improvement.\n","authors":["Nan Wang","Ricardo G. Sanfelice"],"pdf_url":"https://arxiv.org/pdf/2403.18413v1.pdf","comment":"Accepted by the 8th IFAC International Conference on Analysis and\n  Design of Hybrid Systems (ADHS 2024)"},{"id":"http://arxiv.org/abs/2403.18398v1","updated":"2024-03-27T09:37:12Z","published":"2024-03-27T09:37:12Z","title":"Adaptive Economic Model Predictive Control for linear systems with\n  performance guarantees","summary":"  We present a model predictive control (MPC) formulation to directly optimize\neconomic criteria for linear constrained systems subject to disturbances and\nuncertain model parameters. The proposed formulation combines a certainty\nequivalent economic MPC with a simple least-squares parameter adaptation. For\nthe resulting adaptive economic MPC scheme, we derive strong asymptotic and\ntransient performance guarantees. We provide a numerical example involving\nbuilding temperature control and demonstrate performance benefits of online\nparameter adaptation.\n","authors":["Maximilian Degner","Raffaele Soloperto","Melanie N. Zeilinger","John Lygeros","Johannes Köhler"],"pdf_url":"https://arxiv.org/pdf/2403.18398v1.pdf","comment":"8 pages, 3 figures, submitted to IEEE CDC 2024"},{"id":"http://arxiv.org/abs/2403.18371v1","updated":"2024-03-27T09:08:06Z","published":"2024-03-27T09:08:06Z","title":"Multivariable control of modular multilevel converters with convergence\n  and safety guarantees","summary":"  Well-designed current control is a key factor in ensuring the efficient and\nsafe operation of modular multilevel converters (MMCs). Even though this\ncontrol problem involves multiple control objectives, conventional current\ncontrol schemes are comprised of independently designed decoupled controllers,\ne.g., proportional-integral (PI) or proportional-resonant (PR). Due to the\nbilinearity of the MMC dynamics, tuning PI and PR controllers so that good\nperformance and constraint satisfaction are guaranteed is quite challenging.\nThis challenge becomes more relevant in an AC/AC MMC configuration due to the\ncomplexity of tracking the single-phase sinusoidal components of the MMC\noutput. In this paper, we propose a method to design a multivariable\ncontroller, i.e., a static feedback gain, to regulate the MMC currents. We use\na physics-informed transformation to model the MMC dynamics linearly and\nsynthesise the proposed controller. We use this linear model to formulate a\nlinear matrix inequality that computes a feedback gain that guarantees safe and\neffective operation, including (i) limited tracking error, (ii) stability, and\n(iii) meeting all constraints. To test the efficacy of our method, we examine\nits performance in a direct AC/AC MMC simulated in Simulink/PLECS and in a\nscaled-down AC/AC MMC prototype to investigate the ultra-fast charging of\nelectric vehicles.\n","authors":["Victor Daniel Reyes Dreke","Ygor Pereira Marca","Maurice Roes","Mircea Lazar"],"pdf_url":"https://arxiv.org/pdf/2403.18371v1.pdf","comment":"Submitted to IEEE Open Journal of the Industrial Electronics"},{"id":"http://arxiv.org/abs/2403.18275v1","updated":"2024-03-27T06:02:55Z","published":"2024-03-27T06:02:55Z","title":"Differentially Private Dual Gradient Tracking for Distributed Resource\n  Allocation","summary":"  This paper investigates privacy issues in distributed resource allocation\nover directed networks, where each agent holds a private cost function and\noptimizes its decision subject to a global coupling constraint through local\ninteraction with other agents. Conventional methods for resource allocation\nover directed networks require all agents to transmit their original data to\nneighbors, which poses the risk of disclosing sensitive and private\ninformation. To address this issue, we propose an algorithm called\ndifferentially private dual gradient tracking (DP-DGT) for distributed resource\nallocation, which obfuscates the exchanged messages using independent Laplacian\nnoise. Our algorithm ensures that the agents' decisions converge to a\nneighborhood of the optimal solution almost surely. Furthermore, without the\nassumption of bounded gradients, we prove that the cumulative differential\nprivacy loss under the proposed algorithm is finite even when the number of\niterations goes to infinity. To the best of our knowledge, we are the first to\nsimultaneously achieve these two goals in distributed resource allocation\nproblems over directed networks. Finally, numerical simulations on economic\ndispatch problems within the IEEE 14-bus system illustrate the effectiveness of\nour proposed algorithm.\n","authors":["Wei Huo","Xiaomeng Chen","Lingying Huang","Karl Henrik Johansson","Ling Shi"],"pdf_url":"https://arxiv.org/pdf/2403.18275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18254v1","updated":"2024-03-27T04:54:23Z","published":"2024-03-27T04:54:23Z","title":"Differentially Private Distributed Nonconvex Stochastic Optimization\n  with Quantized Communications","summary":"  This paper proposes a new distributed nonconvex stochastic optimization\nalgorithm that can achieve privacy protection, communication efficiency and\nconvergence simultaneously. Specifically, each node adds time-varying privacy\nnoises to its local state to avoid information leakage, and then quantizes its\nnoise-perturbed state before transmitting to improve communication efficiency.\nBy employing the subsampling method controlled through the sample-size\nparameter, the proposed algorithm reduces the impact of privacy noises, and\nenhances the differential privacy level. When the global cost function\nsatisfies the Polyak-Lojasiewicz condition, the mean and high-probability\nconvergence rate and the oracle complexity of the proposed algorithm are given.\nImportantly, the proposed algorithm achieves both the mean convergence and a\nfinite cumulative differential privacy budget over infinite iterations as the\nsample-size goes to infinity. A numerical example of the distributed training\non the \"MNIST\" dataset is given to show the effectiveness of the algorithm.\n","authors":["Jialong Chen","Jimin Wang","Ji-Feng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18254v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18250v1","updated":"2024-03-27T04:46:33Z","published":"2024-03-27T04:46:33Z","title":"Linear Hybrid Asymmetrical Load-Modulated Balanced Amplifier with\n  Multi-Band Reconfigurability and Antenna-VSWR Resilience","summary":"  This paper presents the first-ever highly linear and load-insensitive\nthree-way load-modulation power amplifier (PA) based on reconfigurable hybrid\nasymmetrical load modulated balanced amplifier (H-ALMBA). Through proper\namplitude and phase controls, the carrier, control amplifier (CA), and two\npeaking balanced amplifiers (BA1 and BA2) can form a linear high-order load\nmodulation over wide bandwidth. Moreover, it is theoretically unveiled that the\nload modulation behavior of H-ALMBA can be insensitive to load mismatch by\nleveraging bias reconfiguration and the intrinsic load-insensitivity of\nbalanced topology. Specifically, the PA's linearity and efficiency profiles can\nbe maintained against arbitrary load mismatch through $Z_\\mathrm{L}$-dependent\nreconfiguration of CA supply voltage ($V_\\mathrm{DD,CA}$) and turning-on\nsequence of BA1 and BA2. Based on the proposed theory, an RF-input linear\nH-ALMBA is developed with GaN transistors and wideband quadrature hybrids. Over\nthe design bandwidth from $1.7$-$2.9$ GHz, an efficiency of $56.8\\%$$-$$72.9\\%$\nat peak power and $49.8\\%$$-$$61.2\\%$ at $10$-dB PBO are measured together with\nlinear AMAM and AMPM responses. In modulated evaluation with 4G LTE signal, an\nEVM of $3.1\\%$, ACPR of $-39$ dB, and average efficiency of up to $52\\%$ are\nmeasured. Moreover, the reconfigurable H-ALMBA experimentally maintains an\nexcellent average efficiency and linearity against arbitrary load mismatch at\n$2:1$ VSWR, and this mismatch-resilient operation can be achieved at any\nin-band frequencies. The overall measured performance favorably outperforms the\nstate-of-the-art.\n","authors":["Jiachen Guo","Yuchen Cao","Kenle Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18250v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2401.11542v2","updated":"2024-03-27T04:39:26Z","published":"2024-01-21T16:51:07Z","title":"Nigel -- Mechatronic Design and Robust Sim2Real Control of an\n  Over-Actuated Autonomous Vehicle","summary":"  Simulation to reality (sim2real) transfer from a dynamics and controls\nperspective usually involves re-tuning or adapting the designed algorithms to\nsuit real-world operating conditions, which often violates the performance\nguarantees established originally. This work presents a generalizable framework\nfor achieving reliable sim2real transfer of autonomy-oriented control systems\nusing multi-model multi-objective robust optimal control synthesis, which lends\nwell to uncertainty handling and disturbance rejection with theoretical\nguarantees. Particularly, this work is centered around a novel\nactuation-redundant scaled autonomous vehicle called Nigel, with independent\nall-wheel drive and independent all-wheel steering architecture, whose enhanced\nconfiguration space bodes well for robust control applications. To this end, we\npresent the mechatronic design, dynamics modeling, parameter identification,\nand robust stabilizing as well as tracking control of Nigel using the proposed\nframework, with exhaustive experimentation and benchmarking in simulation as\nwell as real-world settings.\n","authors":["Chinmay Vilas Samak","Tanmay Vilas Samak","Javad Mohammadpour Velni","Venkat Narayan Krovi"],"pdf_url":"https://arxiv.org/pdf/2401.11542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18235v1","updated":"2024-03-27T03:50:14Z","published":"2024-03-27T03:50:14Z","title":"An Execution-time-certified QP Algorithm for $\\ell_1$ penalty-based\n  Soft-constrained MPC","summary":"  Providing an execution time certificate and handling possible infeasibility\nin closed-loop are two pressing requirements of Model Predictive Control (MPC).\nTo simultaneously meet these two requirements, this paper uses $\\ell_1$\npenalty-based soft-constrained MPC formulation and innovatively transforms the\nresulting non-smooth QP into a box-constrained QP, which is solved by our\npreviously proposed direct and execution-time certified algorithm with only\ndimension-dependent (data-independent) and exact number of iterations [1]. This\napproach not only overcomes the limitation of our previously proposed algorithm\n[1], only applicable to input-constrained MPC, but also enjoys exact recovery\nfeature (exactly recover the same solution when the original problem is\nfeasible) of $\\ell_1$ penalty-based soft-constrained MPC formulation without\nsuffering numerical difficulty of the resulting non-smoothness. Other various\nreal-time QP applications, not limited to MPC, will also benefit from our QP\nalgorithm with execution-time certificate and global feasibility.\n","authors":["Liang Wu","Richard D. Braatz"],"pdf_url":"https://arxiv.org/pdf/2403.18235v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.18200v1","updated":"2024-03-27T02:21:20Z","published":"2024-03-27T02:21:20Z","title":"Fault-tolerant properties of scale-free linear protocols for\n  synchronization of homogeneous multi-agent systems","summary":"  Originally, protocols were designed for multi-agent systems (MAS) using\ninformation about the network. However, in many cases there is no or only\nlimited information available about the network. Recently, there has been a\nfocus on scale-free synchronization of multi-agent systems (MAS). In this case,\nthe protocol is designed without any prior information about the network. As\nlong as the network contains a directed spanning tree, the scale-free protocol\nguarantees that the network achieves synchronization.\n  If there is no directed spanning tree for the network then synchronization\ncannot be achieved. But what happens when these scale-free protocols are\napplied to such a network where the directed spanning tree no longer exists?\nThe latter might arise if, for instance, a fault occurs in one of more crucial\nlinks. This paper establishes that the network decomposes into a number of\nbasic bicomponents which achieves synchronization among all nodes in this basic\nbicomponent. On the other hand, nodes which are not part of any basic\nbicomponent converge to a weighted average of the synchronized trajectories of\nthe basic bicomponents. The weights are independent of the initial conditions\nand are independent of the designed protocol.\n","authors":["Anton A. Stoorvogel","Ali Saberi","Zhenwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18200v1.pdf","comment":"The article was submitted to IEEE Transactions on Automatic Control\n  for review at March 27th, 2024"},{"id":"http://arxiv.org/abs/2403.16488v3","updated":"2024-03-27T01:30:16Z","published":"2024-03-25T07:16:08Z","title":"Ensuring Disturbance Rejection Performance by Synthesizing\n  Grid-Following and Grid-Forming Inverters in Power Systems","summary":"  To satisfy dynamic requirements of power systems, it is imperative for\ngrid-tied inverters to ensure good disturbance rejection performance (DRP)\nunder variable grid conditions. This letter discovers and theoretically proves\nthat for general networks, synthesizing grid-following (GFL) inverters and\ngrid-forming (GFM) inverters can always more effectively ensure the DRP of\nmultiple inverters, as compared to homogeneous inverter-based systems that\nsolely utilize either GFL or GFM inverters. The synthesis of GFL inverters and\nGFM inverters can concurrently increase the smallest eigenvalue and decrease\nthe largest eigenvalue of the network grounded Laplacian matrix. This can be\nequivalent to rematching the proper short-circuit ratio (SCR) for GFL and GFM\ninverters, thereby ensuring the DRP of inverters both in weak and strong grids.\nThe results reveal the necessity of synthesizing diverse inverter control\nschemes from the network-based perspective. Sensitivity function-based tests\nand real-time simulations validate our results.\n","authors":["Fuyilong Ma","Huanhai Xin","Zhiyi Li","Linbin Huang"],"pdf_url":"https://arxiv.org/pdf/2403.16488v3.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.18166v1","updated":"2024-03-27T00:21:49Z","published":"2024-03-27T00:21:49Z","title":"Incentive-Compatible Vertiport Reservation in Advanced Air Mobility: An\n  Auction-Based Approach","summary":"  The rise of advanced air mobility (AAM) is expected to become a\nmultibillion-dollar industry in the near future. Market-based mechanisms are\ntouted to be an integral part of AAM operations, which comprise heterogeneous\noperators with private valuations. In this work, we study the problem of\ndesigning a mechanism to coordinate the movement of electric vertical take-off\nand landing (eVTOL) aircraft, operated by multiple operators each having\nheterogeneous valuations associated with their fleet, between vertiports, while\nenforcing the arrival, departure, and parking constraints at vertiports.\nParticularly, we propose an incentive-compatible and individually rational\nvertiport reservation mechanism that maximizes a social welfare metric, which\nencapsulates the objective of maximizing the overall valuations of all\noperators while minimizing the congestion at vertiports. Additionally, we\nimprove the computational tractability of designing the reservation mechanism\nby proposing a mixed binary linear programming approach that is based on\nconstructing network flow graph corresponding to the underlying problem.\n","authors":["Pan-Yang Su","Chinmay Maheshwari","Victoria Tuck","Shankar Sastry"],"pdf_url":"https://arxiv.org/pdf/2403.18166v1.pdf","comment":"26 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2403.18164v1","updated":"2024-03-27T00:16:17Z","published":"2024-03-27T00:16:17Z","title":"Incentive Designs for Learning Agents to Stabilize Coupled Exogenous\n  Systems","summary":"  We consider a large population of learning agents noncooperatively selecting\nstrategies from a common set, influencing the dynamics of an exogenous system\n(ES) we seek to stabilize at a desired equilibrium. Our approach is to design a\ndynamic payoff mechanism capable of shaping the population's strategy profile,\nthus affecting the ES's state, by offering incentives for specific strategies\nwithin budget limits. Employing system-theoretic passivity concepts, we\nestablish conditions under which a payoff mechanism can be systematically\nconstructed to ensure the global asymptotic stabilization of the ES's\nequilibrium. In comparison to previous approaches originally studied in the\ncontext of the so-called epidemic population games, the method proposed here\nallows for more realistic epidemic models and other types of ES, such as\npredator-prey dynamics. Stabilization is established with the support of a\nLyapunov function, which provides useful bounds on the transients.\n","authors":["Jair Certório","Nuno C. Martins","Richard J. La","Murat Arcak"],"pdf_url":"https://arxiv.org/pdf/2403.18164v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.18163v1","updated":"2024-03-27T00:12:51Z","published":"2024-03-27T00:12:51Z","title":"A Study of Three Influencer Archetypes for the Control of Opinion Spread\n  in Time-Varying Social Networks","summary":"  In this work we consider the impact of information spread in time-varying\nsocial networks, where agents request to follow other agents with aligned\nopinions while dropping ties to neighbors whose posts are too dissimilar to\ntheir own views. Opinion control and rhetorical influence has a very long\nhistory, employing various methods including education, persuasion, propaganda,\nmarketing, and manipulation through mis-, dis-, and mal-information. The\nautomation of opinion controllers, however, has only recently become easily\ndeployable at a wide scale, with the advent of large language models (LLMs) and\ngenerative AI that can translate the quantified commands from opinion\ncontrollers into actual content with the appropriate nuance. Automated agents\nin social networks can be deployed for various purposes, such as breaking up\necho chambers, bridging valuable new connections between agents, or shaping the\nopinions of a target population -- and all of these raise important ethical\nconcerns that deserve serious attention and thoughtful discussion and debate.\nThis paper attempts to contribute to this discussion by considering three\narchetypal influencing styles observed by human drivers in these settings,\ncomparing and contrasting the impact of these different control methods on the\nopinions of agents in the network. We will demonstrate the efficacy of current\ngenerative AI for generating nuanced content consistent with the command signal\nfrom automatic opinion controllers like these, and we will report on frameworks\nfor approaching the relevant ethical considerations.\n","authors":["Michael DeBuse","Sean Warnick"],"pdf_url":"https://arxiv.org/pdf/2403.18163v1.pdf","comment":"Submission to IEEE 2024 Conference on Decision and Control. 8 pages,\n  7 figures, 1 table"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13525v2","updated":"2024-03-27T17:47:56Z","published":"2023-05-22T22:41:49Z","title":"A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs","summary":"  Large communication costs are a critical bottleneck in training\nstate-of-the-art neural networks on distributed systems. This paper introduces\nAxoNN, a novel four-dimensional (4D) parallelization approach, inspired by\nAgarwal's algorithm for matrix multiplication, for parallelizing tensor\ncomputations in deep learning, AxoNN employs two key strategies to minimize\ncommunication overhead. First, we optimize communication by overlapping\nexpensive collective operations (reduce-scatter, all-gather, all-reduce) with\ncomputations. Our experiments with a 20-billion parameter transformer model\ndemonstrate that these optimizations deliver nearly 53\\% improvement. Second,\nwe present an analytical model to assist users in identifying\ncommunication-minimizing configurations within the vast search space defined by\nour 4D algorithm. This model empowers practitioners by simplifying the tuning\nprocess for their specific training workloads. When training an 80-billion\nparameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a\nstate-of-the-art framework, by a significant 26%. Additionally, it achieves 57%\nof the theoretical peak FLOP/s.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya K. Ranjan","Zack Sating","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2305.13525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13483v4","updated":"2024-03-27T17:38:27Z","published":"2023-02-27T02:42:27Z","title":"CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems","summary":"  We present CrystalBox, a novel, model-agnostic, posthoc explainability\nframework for Deep Reinforcement Learning (DRL) controllers in the large family\nof input-driven environments which includes computer systems. We combine the\nnatural decomposability of reward functions in input-driven environments with\nthe explanatory power of decomposed returns. We propose an efficient algorithm\nto generate future-based explanations across both discrete and continuous\ncontrol environments. Using applications such as adaptive bitrate streaming and\ncongestion control, we demonstrate CrystalBox's capability to generate\nhigh-fidelity explanations. We further illustrate its higher utility across\nthree practical use cases: contrastive explanations, network observability, and\nguided reward design, as opposed to prior explainability techniques that\nidentify salient features.\n","authors":["Sagar Patel","Sangeetha Abdu Jyothi","Nina Narodytska"],"pdf_url":"https://arxiv.org/pdf/2302.13483v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04381v2","updated":"2024-03-27T17:07:47Z","published":"2023-09-08T15:23:40Z","title":"Generalization Bounds: Perspectives from Information Theory and\n  PAC-Bayes","summary":"  A fundamental question in theoretical machine learning is generalization.\nOver the past decades, the PAC-Bayesian approach has been established as a\nflexible framework to address the generalization capabilities of machine\nlearning algorithms, and design new ones. Recently, it has garnered increased\ninterest due to its potential applicability for a variety of learning\nalgorithms, including deep neural networks. In parallel, an\ninformation-theoretic view of generalization has developed, wherein the\nrelation between generalization and various information measures has been\nestablished. This framework is intimately connected to the PAC-Bayesian\napproach, and a number of results have been independently discovered in both\nstrands. In this monograph, we highlight this strong connection and present a\nunified treatment of PAC-Bayesian and information-theoretic generalization\nbounds. We present techniques and results that the two perspectives have in\ncommon, and discuss the approaches and interpretations that differ. In\nparticular, we demonstrate how many proofs in the area share a modular\nstructure, through which the underlying ideas can be intuited. We pay special\nattention to the conditional mutual information (CMI) framework; analytical\nstudies of the information complexity of learning algorithms; and the\napplication of the proposed methods to deep learning. This monograph is\nintended to provide a comprehensive introduction to information-theoretic\ngeneralization bounds and their connection to PAC-Bayes, serving as a\nfoundation from which the most recent developments are accessible. It is aimed\nbroadly towards researchers with an interest in generalization and theoretical\nmachine learning.\n","authors":["Fredrik Hellström","Giuseppe Durisi","Benjamin Guedj","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2309.04381v2.pdf","comment":"228 pages"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18765v1","updated":"2024-03-27T17:03:31Z","published":"2024-03-27T17:03:31Z","title":"CaT: Constraints as Terminations for Legged Locomotion Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) has demonstrated impressive results in\nsolving complex robotic tasks such as quadruped locomotion. Yet, current\nsolvers fail to produce efficient policies respecting hard constraints. In this\nwork, we advocate for integrating constraints into robot learning and present\nConstraints as Terminations (CaT), a novel constrained RL algorithm. Departing\nfrom classical constrained RL formulations, we reformulate constraints through\nstochastic terminations during policy learning: any violation of a constraint\ntriggers a probability of terminating potential future rewards the RL agent\ncould attain. We propose an algorithmic approach to this formulation, by\nminimally modifying widely used off-the-shelf RL algorithms in robot learning\n(such as Proximal Policy Optimization). Our approach leads to excellent\nconstraint adherence without introducing undue complexity and computational\noverhead, thus mitigating barriers to broader adoption. Through empirical\nevaluation on the real quadruped robot Solo crossing challenging obstacles, we\ndemonstrate that CaT provides a compelling solution for incorporating\nconstraints into RL frameworks. Videos and code are available at\nhttps://constraints-as-terminations.github.io.\n","authors":["Elliot Chane-Sane","Pierre-Alexandre Leziart","Thomas Flayols","Olivier Stasse","Philippe Souères","Nicolas Mansard"],"pdf_url":"https://arxiv.org/pdf/2403.18765v1.pdf","comment":"Project webpage: https://constraints-as-terminations.github.io"},{"id":"http://arxiv.org/abs/2311.01483v3","updated":"2024-03-27T16:56:23Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Novel Federated Learning Framework over LEO Satellite Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2403.14623v2","updated":"2024-03-27T16:49:35Z","published":"2024-03-21T17:59:41Z","title":"Simplified Diffusion Schrödinger Bridge","summary":"  This paper introduces a novel theoretical simplification of the Diffusion\nSchr\\\"odinger Bridge (DSB) that facilitates its unification with Score-based\nGenerative Models (SGMs), addressing the limitations of DSB in complex data\ngeneration and enabling faster convergence and enhanced performance. By\nemploying SGMs as an initial solution for DSB, our approach capitalizes on the\nstrengths of both frameworks, ensuring a more efficient training process and\nimproving the performance of SGM. We also propose a reparameterization\ntechnique that, despite theoretical approximations, practically improves the\nnetwork's fitting capabilities. Our extensive experimental evaluations confirm\nthe effectiveness of the simplified DSB, demonstrating its significant\nimprovements. We believe the contributions of this work pave the way for\nadvanced generative modeling. The code is available at\nhttps://github.com/checkcrab/SDSB.\n","authors":["Zhicong Tang","Tiankai Hang","Shuyang Gu","Dong Chen","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2403.14623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03683v2","updated":"2024-03-27T16:44:22Z","published":"2023-11-07T03:19:16Z","title":"Preventing Arbitrarily High Confidence on Far-Away Data in\n  Point-Estimated Discriminative Neural Networks","summary":"  Discriminatively trained, deterministic neural networks are the de facto\nchoice for classification problems. However, even though they achieve\nstate-of-the-art results on in-domain test sets, they tend to be overconfident\non out-of-distribution (OOD) data. For instance, ReLU networks - a popular\nclass of neural network architectures - have been shown to almost always yield\nhigh confidence predictions when the test data are far away from the training\nset, even when they are trained with OOD data. We overcome this problem by\nadding a term to the output of the neural network that corresponds to the logit\nof an extra class, that we design to dominate the logits of the original\nclasses as we move away from the training data.This technique provably prevents\narbitrarily high confidence on far-away test data while maintaining a simple\ndiscriminative point-estimate training. Evaluation on various benchmarks\ndemonstrates strong performance against competitive baselines on both far-away\nand realistic OOD data.\n","authors":["Ahmad Rashid","Serena Hacker","Guojun Zhang","Agustinus Kristiadi","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2311.03683v2.pdf","comment":"Accepted at AISTATS 2024"},{"id":"http://arxiv.org/abs/2403.18742v1","updated":"2024-03-27T16:39:28Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18739v1","updated":"2024-03-27T16:32:32Z","published":"2024-03-27T16:32:32Z","title":"Usage-Specific Survival Modeling Based on Operational Data and Neural\n  Networks","summary":"  Accurate predictions of when a component will fail are crucial when planning\nmaintenance, and by modeling the distribution of these failure times, survival\nmodels have shown to be particularly useful in this context. The presented\nmethodology is based on conventional neural network-based survival models that\nare trained using data that is continuously gathered and stored at specific\ntimes, called snapshots. An important property of this type of training data is\nthat it can contain more than one snapshot from a specific individual which\nresults in that standard maximum likelihood training can not be directly\napplied since the data is not independent. However, the papers show that if the\ndata is in a specific format where all snapshot times are the same for all\nindividuals, called homogeneously sampled, maximum likelihood training can be\napplied and produce desirable results. In many cases, the data is not\nhomogeneously sampled and in this case, it is proposed to resample the data to\nmake it homogeneously sampled. How densely the dataset is sampled turns out to\nbe an important parameter; it should be chosen large enough to produce good\nresults, but this also increases the size of the dataset which makes training\nslow. To reduce the number of samples needed during training, the paper also\nproposes a technique to, instead of resampling the dataset once before the\ntraining starts, randomly resample the dataset at the start of each epoch\nduring the training. The proposed methodology is evaluated on both a simulated\ndataset and an experimental dataset of starter battery failures. The results\nshow that if the data is homogeneously sampled the methodology works as\nintended and produces accurate survival models. The results also show that\nrandomly resampling the dataset on each epoch is an effective way to reduce the\nsize of the training data.\n","authors":["Olov Holmer","Mattias Krysander","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2403.18739v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.18735v1","updated":"2024-03-27T16:24:26Z","published":"2024-03-27T16:24:26Z","title":"Nonlinear model reduction for operator learning","summary":"  Operator learning provides methods to approximate mappings between\ninfinite-dimensional function spaces. Deep operator networks (DeepONets) are a\nnotable architecture in this field. Recently, an extension of DeepONet based on\nmodel reduction and neural networks, proper orthogonal decomposition\n(POD)-DeepONet, has been able to outperform other architectures in terms of\naccuracy for several benchmark tests. We extend this idea towards nonlinear\nmodel order reduction by proposing an efficient framework that combines neural\nnetworks with kernel principal component analysis (KPCA) for operator learning.\nOur results demonstrate the superior performance of KPCA-DeepONet over\nPOD-DeepONet.\n","authors":["Hamidreza Eivazi","Stefan Wittek","Andreas Rausch"],"pdf_url":"https://arxiv.org/pdf/2403.18735v1.pdf","comment":"Published as a Tiny Paper at ICLR 2024 (Notable)"},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2402.07868v2","updated":"2024-03-27T16:12:43Z","published":"2024-02-12T18:29:17Z","title":"Nesting Particle Filters for Experimental Design in Dynamical Systems","summary":"  In this paper, we propose a novel approach to Bayesian experimental design\nfor non-exchangeable data that formulates it as risk-sensitive policy\noptimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential\nMonte Carlo technique to infer optimal designs, and embed it into a particle\nMarkov chain Monte Carlo framework to perform gradient-based policy\namortization. Our approach is distinct from other amortized experimental design\ntechniques, as it does not rely on contrastive estimators. Numerical validation\non a set of dynamical systems showcases the efficacy of our method in\ncomparison to other state-of-the-art strategies.\n","authors":["Sahel Iqbal","Adrien Corenflos","Simo Särkkä","Hany Abdulsamad"],"pdf_url":"https://arxiv.org/pdf/2402.07868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11798v3","updated":"2024-03-27T16:12:18Z","published":"2023-09-21T06:04:06Z","title":"A Comprehensive Review of Community Detection in Graphs","summary":"  The study of complex networks has significantly advanced our understanding of\ncommunity structures which serves as a crucial feature of real-world graphs.\nDetecting communities in graphs is a challenging problem with applications in\nsociology, biology, and computer science. Despite the efforts of an\ninterdisciplinary community of scientists, a satisfactory solution to this\nproblem has not yet been achieved. This review article delves into the topic of\ncommunity detection in graphs, which serves as a thorough exposition of various\ncommunity detection methods from perspectives of modularity-based method,\nspectral clustering, probabilistic modelling, and deep learning. Along with the\nmethods, a new community detection method designed by us is also presented.\nAdditionally, the performance of these methods on the datasets with and without\nground truth is compared. In conclusion, this comprehensive review provides a\ndeep understanding of community detection in graphs.\n","authors":["Jiakang Li","Songning Lai","Zhihao Shuai","Yuan Tan","Yifan Jia","Mianyang Yu","Zichen Song","Xiaokang Peng","Ziyang Xu","Yongxin Ni","Haifeng Qiu","Jiayu Yang","Yutong Liu","Yonggang Lu"],"pdf_url":"https://arxiv.org/pdf/2309.11798v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.17878v2","updated":"2024-03-27T16:01:00Z","published":"2024-03-26T17:10:15Z","title":"Empowering Data Mesh with Federated Learning","summary":"  The evolution of data architecture has seen the rise of data lakes, aiming to\nsolve the bottlenecks of data management and promote intelligent\ndecision-making. However, this centralized architecture is limited by the\nproliferation of data sources and the growing demand for timely analysis and\nprocessing. A new data paradigm, Data Mesh, is proposed to overcome these\nchallenges. Data Mesh treats domains as a first-class concern by distributing\nthe data ownership from the central team to each data domain, while keeping the\nfederated governance to monitor domains and their data products. Many\nmulti-million dollar organizations like Paypal, Netflix, and Zalando have\nalready transformed their data analysis pipelines based on this new\narchitecture. In this decentralized architecture where data is locally\npreserved by each domain team, traditional centralized machine learning is\nincapable of conducting effective analysis across multiple domains, especially\nfor security-sensitive organizations. To this end, we introduce a pioneering\napproach that incorporates Federated Learning into Data Mesh. To the best of\nour knowledge, this is the first open-source applied work that represents a\ncritical advancement toward the integration of federated learning methods into\nthe Data Mesh paradigm, underscoring the promising prospects for\nprivacy-preserving and decentralized data analysis strategies within Data Mesh\narchitecture.\n","authors":["Haoyuan Li","Salman Toor"],"pdf_url":"https://arxiv.org/pdf/2403.17878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18710v1","updated":"2024-03-27T15:57:42Z","published":"2024-03-27T15:57:42Z","title":"Deep Learning for Traffic Flow Prediction using Cellular Automata-based\n  Model and CNN-LSTM architecture","summary":"  Recent works have attempted to use deep learning to predict future states of\ntraffic flow, but have met with mixed results. These approaches face two key\nchallenges. First, training deep learning neural networks requires large\namounts of training data which are not yet easily available for traffic flow\nsystems. Second, even when data is available, the neural networks require\naccess to historical data that covers most possible traffic flow dynamics to\nsuccessfully predict future traffic states. Specifically, these deep learning\napproaches do not fully leverage domain-knowledge about traffic flow dynamics,\ndespite a significant existing knowledge-base. In this work, we propose to\nsolve both issues using a Convolutional Neural Network (CNNs) with Long Short\nTerm Memory (LSTM) deep learning architecture to successfully predict traffic\nflow, while leveraging a cellular automata-based statistical mechanics model of\ntraffic flow to generate training and test data. Another major contribution of\nthis paper is the insight that training data for a large traffic system can\nactually be sampled from the simulations of a much smaller traffic system. This\nis achieved through observing that the normalized energy distribution of the\nstatistical mechanics model is scale invariant, which significantly eases the\nburden of data generation for large scale traffic systems. The resulting\nsimulations indicate good agreement between the predicted and the true traffic\nflow dynamics.\n","authors":["Zhaohui Yang","Kshitij Jerath"],"pdf_url":"https://arxiv.org/pdf/2403.18710v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18705v1","updated":"2024-03-27T15:54:55Z","published":"2024-03-27T15:54:55Z","title":"Conditional Wasserstein Distances with Applications in Bayesian OT Flow\n  Matching","summary":"  In inverse problems, many conditional generative models approximate the\nposterior measure by minimizing a distance between the joint measure and its\nlearned approximation. While this approach also controls the distance between\nthe posterior measures in the case of the Kullback--Leibler divergence, this is\nin general not hold true for the Wasserstein distance. In this paper, we\nintroduce a conditional Wasserstein distance via a set of restricted couplings\nthat equals the expected Wasserstein distance of the posteriors. Interestingly,\nthe dual formulation of the conditional Wasserstein-1 flow resembles losses in\nthe conditional Wasserstein GAN literature in a quite natural way. We derive\ntheoretical properties of the conditional Wasserstein distance, characterize\nthe corresponding geodesics and velocity fields as well as the flow ODEs.\nSubsequently, we propose to approximate the velocity fields by relaxing the\nconditional Wasserstein distance. Based on this, we propose an extension of OT\nFlow Matching for solving Bayesian inverse problems and demonstrate its\nnumerical advantages on an inverse problem and class-conditional image\ngeneration.\n","authors":["Jannis Chemseddine","Paul Hagemann","Christian Wald","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2403.18705v1.pdf","comment":"This paper supersedes arXiv:2310.13433"},{"id":"http://arxiv.org/abs/2403.18703v1","updated":"2024-03-27T15:52:54Z","published":"2024-03-27T15:52:54Z","title":"Fpga-Based Neural Thrust Controller for UAVs","summary":"  The advent of unmanned aerial vehicles (UAVs) has improved a variety of\nfields by providing a versatile, cost-effective and accessible platform for\nimplementing state-of-the-art algorithms. To accomplish a broader range of\ntasks, there is a growing need for enhanced on-board computing to cope with\nincreasing complexity and dynamic environmental conditions. Recent advances\nhave seen the application of Deep Neural Networks (DNNs), particularly in\ncombination with Reinforcement Learning (RL), to improve the adaptability and\nperformance of UAVs, especially in unknown environments. However, the\ncomputational requirements of DNNs pose a challenge to the limited computing\nresources available on many UAVs. This work explores the use of Field\nProgrammable Gate Arrays (FPGAs) as a viable solution to this challenge,\noffering flexibility, high performance, energy and time efficiency. We propose\na novel hardware board equipped with an Artix-7 FPGA for a popular open-source\nmicro-UAV platform. We successfully validate its functionality by implementing\nan RL-based low-level controller using real-world experiments.\n","authors":["Sharif Azem","David Scheunert","Mengguang Li","Jonas Gehrunger","Kai Cui","Christian Hochberger","Heinz Koepp"],"pdf_url":"https://arxiv.org/pdf/2403.18703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18699v1","updated":"2024-03-27T15:48:16Z","published":"2024-03-27T15:48:16Z","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","summary":"  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18699v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18687v1","updated":"2024-03-27T15:34:27Z","published":"2024-03-27T15:34:27Z","title":"InceptionTime vs. Wavelet -- A comparison for time series classification","summary":"  Neural networks were used to classify infrasound data. Two different\napproaches were compared. One based on the direct classification of time series\ndata, using a custom implementation of the InceptionTime network. For the other\napproach, we generated 2D images of the wavelet transformation of the signals,\nwhich were subsequently classified using a ResNet implementation. Choosing\nappropriate hyperparameter settings, both achieve a classification accuracy of\nabove 90 %, with the direct approach reaching 95.2 %.\n","authors":["Daniel Klenkert","Daniel Schaeffer","Julian Stauch"],"pdf_url":"https://arxiv.org/pdf/2403.18687v1.pdf","comment":"4 pages, 1 figure"},{"id":"http://arxiv.org/abs/2403.18685v1","updated":"2024-03-27T15:29:08Z","published":"2024-03-27T15:29:08Z","title":"Representatividad Muestral en la Incertidumbre Simétrica Multivariada\n  para la Selección de Atributos","summary":"  In this work, we analyze the behavior of the multivariate symmetric\nuncertainty (MSU) measure through the use of statistical simulation techniques\nunder various mixes of informative and non-informative randomly generated\nfeatures. Experiments show how the number of attributes, their cardinalities,\nand the sample size affect the MSU. In this thesis, through observation of\nresults, it is proposed an heuristic condition that preserves good quality in\nthe MSU under different combinations of these three factors, providing a new\nuseful criterion to help drive the process of dimension reduction.\n  --\n  En el presente trabajo hemos analizado el comportamiento de una versi\\'on\nmultivariada de la incertidumbre sim\\'etrica a trav\\'es de t\\'ecnicas de\nsimulaci\\'on estad\\'isticas sobre varias combinaciones de atributos\ninformativos y no-informativos generados de forma aleatoria. Los experimentos\nmuestran como el n\\'umero de atributos, sus cardinalidades y el tama\\~no\nmuestral afectan al MSU como medida. En esta tesis, mediante la observaci\\'on\nde resultados hemos propuesto una condici\\'on que preserva una buena calidad en\nel MSU bajo diferentes combinaciones de los tres factores mencionados, lo cual\nprovee un nuevo y valioso criterio para llevar a cabo el proceso de reducci\\'on\nde dimensionalidad.\n","authors":["Gustavo Sosa-Cabrera"],"pdf_url":"https://arxiv.org/pdf/2403.18685v1.pdf","comment":"52 pages, in Spanish. Advisors: Miguel Garc\\'ia-Torres, Santiago\n  G\\'omez-Guerrero, Christian E. Schaerer Serra"},{"id":"http://arxiv.org/abs/2403.18681v1","updated":"2024-03-27T15:24:54Z","published":"2024-03-27T15:24:54Z","title":"TransFusion: Contrastive Learning with Transformers","summary":"  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18681v1.pdf","comment":"17 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2403.18680v1","updated":"2024-03-27T15:22:16Z","published":"2024-03-27T15:22:16Z","title":"NL-ITI: Optimizing Probing and Intervention for Improvement of ITI\n  Method","summary":"  Large Language Models (LLM) are prone to returning false information. It\nconstitutes one of major challenges in the AI field. In our work, we explore\nparadigm introduced by Inference-Time-Intervention (ITI). In first stage, it\nidentifies attention heads, which contain the highest amount of desired type of\nknowledge (e.g., truthful). Afterwards, during inference, LLM activations are\nshifted for chosen subset of attention heads. We further improved the ITI\nframework by introducing a nonlinear probing and multi-token intervention -\nNon-Linear ITI (NL-ITI). NL-ITI is tested on diverse multiple-choice\nbenchmarks, including TruthfulQA, on which we report around 14% MC1 metric\nimprovement with respect to the baseline ITI results. NL-ITI achieves also\nencouraging results on other testsets - on Business Ethics subdomain of MMLU,\naround 18% MC1 improvement over baseline LLaMA2-7B. Additionally, NL-ITI\nperforms better while being less invasive in the behavior of LLM at the same\ntime (as measured by Kullback-Leibler divergence).\n","authors":["Jakub Hoscilowicz","Adam Wiacek","Jan Chojnacki","Adam Cieslak","Leszek Michon","Vitalii Urbanevych","Artur Janicki"],"pdf_url":"https://arxiv.org/pdf/2403.18680v1.pdf","comment":"Code is available at https://github.com/Samsung/NL-ITI"},{"id":"http://arxiv.org/abs/2403.17143v2","updated":"2024-03-27T15:15:16Z","published":"2024-03-25T19:40:26Z","title":"Guided Distant Supervision for Multilingual Relation Extraction Data:\n  Adapting to a New Language","summary":"  Relation extraction is essential for extracting and understanding\nbiographical information in the context of digital humanities and related\nsubjects. There is a growing interest in the community to build datasets\ncapable of training machine learning models to extract relationships. However,\nannotating such datasets can be expensive and time-consuming, in addition to\nbeing limited to English. This paper applies guided distant supervision to\ncreate a large biographical relationship extraction dataset for German. Our\ndataset, composed of more than 80,000 instances for nine relationship types, is\nthe largest biographical German relationship extraction dataset. We also create\na manually annotated dataset with 2000 instances to evaluate the models and\nrelease it together with the dataset compiled using guided distant supervision.\nWe train several state-of-the-art machine learning models on the automatically\ncreated dataset and release them as well. Furthermore, we experiment with\nmultilingual and cross-lingual experiments that could benefit many low-resource\nlanguages.\n","authors":["Alistair Plum","Tharindu Ranasinghe","Christoph Purschke"],"pdf_url":"https://arxiv.org/pdf/2403.17143v2.pdf","comment":"Accepted to LREC-COLING 2024 (The 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation)"},{"id":"http://arxiv.org/abs/2403.18671v1","updated":"2024-03-27T15:15:14Z","published":"2024-03-27T15:15:14Z","title":"Fact Checking Beyond Training Set","summary":"  Evaluating the veracity of everyday claims is time consuming and in some\ncases requires domain expertise. We empirically demonstrate that the commonly\nused fact checking pipeline, known as the retriever-reader, suffers from\nperformance deterioration when it is trained on the labeled data from one\ndomain and used in another domain. Afterwards, we delve into each component of\nthe pipeline and propose novel algorithms to address this problem. We propose\nan adversarial algorithm to make the retriever component robust against\ndistribution shift. Our core idea is to initially train a bi-encoder on the\nlabeled source data, and then, to adversarially train two separate document and\nclaim encoders using unlabeled target data. We then focus on the reader\ncomponent and propose to train it such that it is insensitive towards the order\nof claims and evidence documents. Our empirical evaluations support the\nhypothesis that such a reader shows a higher robustness against distribution\nshift. To our knowledge, there is no publicly available multi-topic fact\nchecking dataset. Thus, we propose a simple automatic method to re-purpose two\nwell-known fact checking datasets. We then construct eight fact checking\nscenarios from these datasets, and compare our model to a set of strong\nbaseline models, including recent domain adaptation models that use GPT4 for\ngenerating synthetic data.\n","authors":["Payam Karisani","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18671v1.pdf","comment":"NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18668v1","updated":"2024-03-27T15:11:07Z","published":"2024-03-27T15:11:07Z","title":"Aiming for Relevance","summary":"  Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.\n","authors":["Bar Eini Porat","Danny Eytan","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2403.18668v1.pdf","comment":"10 pages, 9 figures, AMIA Informatics 2024"},{"id":"http://arxiv.org/abs/2403.18664v1","updated":"2024-03-27T15:08:00Z","published":"2024-03-27T15:08:00Z","title":"Neural Network-Based Piecewise Survival Models","summary":"  In this paper, a family of neural network-based survival models is presented.\nThe models are specified based on piecewise definitions of the hazard function\nand the density function on a partitioning of the time; both constant and\nlinear piecewise definitions are presented, resulting in a family of four\nmodels. The models can be seen as an extension of the commonly used\ndiscrete-time and piecewise exponential models and thereby add flexibility to\nthis set of standard models. Using a simulated dataset the models are shown to\nperform well compared to the highly expressive, state-of-the-art energy-based\nmodel, while only requiring a fraction of the computation time.\n","authors":["Olov Holmer","Erik Frisk","Mattias Krysander"],"pdf_url":"https://arxiv.org/pdf/2403.18664v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2403.13374v3","updated":"2024-03-27T14:57:54Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17251v2","updated":"2024-03-27T14:48:48Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. In this contribution, we review some recent results in social bots\nresearch, highlighting and revising factual errors as well as methodological\nand conceptual biases. More importantly, we demystify common misconceptions,\naddressing fundamental points on how social bots research is discussed. Our\nanalysis surfaces the need to discuss research about online disinformation and\nmanipulation in a rigorous, unbiased, and responsible way. This article\nbolsters such effort by identifying and refuting common fallacious arguments\nused by both proponents and opponents of social bots research, as well as\nproviding directions toward sound methodologies for future research in the\nfield.\n","authors":["Stefano Cresci","Kai-Cheng Yang","Angelo Spognardi","Roberto Di Pietro","Filippo Menczer","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12882v2","updated":"2024-03-27T14:47:41Z","published":"2023-08-23T17:42:00Z","title":"LCANets++: Robust Audio Classification using Multi-layer Neural Networks\n  with Lateral Competition","summary":"  Audio classification aims at recognizing audio signals, including speech\ncommands or sound events. However, current audio classifiers are susceptible to\nperturbations and adversarial attacks. In addition, real-world audio\nclassification tasks often suffer from limited labeled data. To help bridge\nthese gaps, previous work developed neuro-inspired convolutional neural\nnetworks (CNNs) with sparse coding via the Locally Competitive Algorithm (LCA)\nin the first layer (i.e., LCANets) for computer vision. LCANets learn in a\ncombination of supervised and unsupervised learning, reducing dependency on\nlabeled samples. Motivated by the fact that auditory cortex is also sparse, we\nextend LCANets to audio recognition tasks and introduce LCANets++, which are\nCNNs that perform sparse coding in multiple layers via LCA. We demonstrate that\nLCANets++ are more robust than standard CNNs and LCANets against perturbations,\ne.g., background noise, as well as black-box and white-box attacks, e.g.,\nevasion and fast gradient sign (FGSM) attacks.\n","authors":["Sayanton V. Dibbo","Juston S. Moore","Garrett T. Kenyon","Michael A. Teti"],"pdf_url":"https://arxiv.org/pdf/2308.12882v2.pdf","comment":"Accepted at 2024 IEEE International Conference on Acoustics, Speech\n  and Signal Processing Workshops (ICASSPW)"},{"id":"http://arxiv.org/abs/2403.18637v1","updated":"2024-03-27T14:42:08Z","published":"2024-03-27T14:42:08Z","title":"Transformers-based architectures for stroke segmentation: A review","summary":"  Stroke remains a significant global health concern, necessitating precise and\nefficient diagnostic tools for timely intervention and improved patient\noutcomes. The emergence of deep learning methodologies has transformed the\nlandscape of medical image analysis. Recently, Transformers, initially designed\nfor natural language processing, have exhibited remarkable capabilities in\nvarious computer vision applications, including medical image analysis. This\ncomprehensive review aims to provide an in-depth exploration of the\ncutting-edge Transformer-based architectures applied in the context of stroke\nsegmentation. It commences with an exploration of stroke pathology, imaging\nmodalities, and the challenges associated with accurate diagnosis and\nsegmentation. Subsequently, the review delves into the fundamental ideas of\nTransformers, offering detailed insights into their architectural intricacies\nand the underlying mechanisms that empower them to effectively capture complex\nspatial information within medical images. The existing literature is\nsystematically categorized and analyzed, discussing various approaches that\nleverage Transformers for stroke segmentation. A critical assessment is\nprovided, highlighting the strengths and limitations of these methods,\nincluding considerations of performance and computational efficiency.\nAdditionally, this review explores potential avenues for future research and\ndevelopment\n","authors":["Yalda Zafari-Ghadim","Essam A. Rashed","Mohamed Mabrok"],"pdf_url":"https://arxiv.org/pdf/2403.18637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18635v1","updated":"2024-03-27T14:40:25Z","published":"2024-03-27T14:40:25Z","title":"Fusion approaches for emotion recognition from speech using acoustic and\n  text-based features","summary":"  In this paper, we study different approaches for classifying emotions from\nspeech using acoustic and text-based features. We propose to obtain\ncontextualized word embeddings with BERT to represent the information contained\nin speech transcriptions and show that this results in better performance than\nusing Glove embeddings. We also propose and compare different strategies to\ncombine the audio and text modalities, evaluating them on IEMOCAP and\nMSP-PODCAST datasets. We find that fusing acoustic and text-based systems is\nbeneficial on both datasets, though only subtle differences are observed across\nthe evaluated fusion approaches. Finally, for IEMOCAP, we show the large effect\nthat the criteria used to define the cross-validation folds have on results. In\nparticular, the standard way of creating folds for this dataset results in a\nhighly optimistic estimation of performance for the text-based system,\nsuggesting that some previous works may overestimate the advantage of\nincorporating transcriptions.\n","authors":["Leonardo Pepino","Pablo Riera","Luciana Ferrer","Agustin Gravano"],"pdf_url":"https://arxiv.org/pdf/2403.18635v1.pdf","comment":"5 pages. Accepted in ICASSP 2020"},{"id":"http://arxiv.org/abs/2403.18631v1","updated":"2024-03-27T14:38:02Z","published":"2024-03-27T14:38:02Z","title":"First Experiences with the Identification of People at Risk for Diabetes\n  in Argentina using Machine Learning Techniques","summary":"  Detecting Type 2 Diabetes (T2D) and Prediabetes (PD) is a real challenge for\nmedicine due to the absence of pathogenic symptoms and the lack of known\nassociated risk factors. Even though some proposals for machine learning models\nenable the identification of people at risk, the nature of the condition makes\nit so that a model suitable for one population may not necessarily be suitable\nfor another. In this article, the development and assessment of predictive\nmodels to identify people at risk for T2D and PD specifically in Argentina are\ndiscussed. First, the database was thoroughly preprocessed and three specific\ndatasets were generated considering a compromise between the number of records\nand the amount of available variables. After applying 5 different\nclassification models, the results obtained show that a very good performance\nwas observed for two datasets with some of these models. In particular, RF, DT,\nand ANN demonstrated great classification power, with good values for the\nmetrics under consideration. Given the lack of this type of tool in Argentina,\nthis work represents the first step towards the development of more\nsophisticated models.\n","authors":["Enzo Rucci","Gonzalo Tittarelli","Franco Ronchetti","Jorge F. Elgart","Laura Lanzarini","Juan José Gagliardino"],"pdf_url":"https://arxiv.org/pdf/2403.18631v1.pdf","comment":"Accepted for publication in Computer Science - CACIC 2023"},{"id":"http://arxiv.org/abs/2403.16451v3","updated":"2024-03-27T14:36:21Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v1","updated":"2024-03-27T14:28:44Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18597v1","updated":"2024-03-27T14:20:11Z","published":"2024-03-27T14:20:11Z","title":"Heterogeneous Peridynamic Neural Operators: Discover Biotissue\n  Constitutive Law and Microstructure From Digital Image Correlation\n  Measurements","summary":"  Human tissues are highly organized structures with specific collagen fiber\narrangements varying from point to point. The effects of such heterogeneity\nplay an important role for tissue function, and hence it is of critical to\ndiscover and understand the distribution of such fiber orientations from\nexperimental measurements, such as the digital image correlation data. To this\nend, we introduce the heterogeneous peridynamic neural operator (HeteroPNO)\napproach, for data-driven constitutive modeling of heterogeneous anisotropic\nmaterials. The goal is to learn both a nonlocal constitutive law together with\nthe material microstructure, in the form of a heterogeneous fiber orientation\nfield, from loading field-displacement field measurements. To this end, we\npropose a two-phase learning approach. Firstly, we learn a homogeneous\nconstitutive law in the form of a neural network-based kernel function and a\nnonlocal bond force, to capture complex homogeneous material responses from\ndata. Then, in the second phase we reinitialize the learnt bond force and the\nkernel function, and training them together with a fiber orientation field for\neach material point. Owing to the state-based peridynamic skeleton, our\nHeteroPNO-learned material models are objective and have the balance of linear\nand angular momentum guaranteed. Moreover, the effects from heterogeneity and\nnonlinear constitutive relationship are captured by the kernel function and the\nbond force respectively, enabling physical interpretability. As a result, our\nHeteroPNO architecture can learn a constitutive model for a biological tissue\nwith anisotropic heterogeneous response undergoing large deformation regime.\nMoreover, the framework is capable to provide displacement and stress field\npredictions for new and unseen loading instances.\n","authors":["Siavash Jafarzadeh","Stewart Silling","Lu Zhang","Colton Ross","Chung-Hao Lee","S. M. Rakibur Rahman","Shuodao Wang","Yue Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18597v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18587v1","updated":"2024-03-27T14:11:23Z","published":"2024-03-27T14:11:23Z","title":"The Impact of Uniform Inputs on Activation Sparsity and Energy-Latency\n  Attacks in Computer Vision","summary":"  Resource efficiency plays an important role for machine learning nowadays.\nThe energy and decision latency are two critical aspects to ensure a\nsustainable and practical application. Unfortunately, the energy consumption\nand decision latency are not robust against adversaries. Researchers have\nrecently demonstrated that attackers can compute and submit so-called sponge\nexamples at inference time to increase the energy consumption and decision\nlatency of neural networks. In computer vision, the proposed strategy crafts\ninputs with less activation sparsity which could otherwise be used to\naccelerate the computation. In this paper, we analyze the mechanism how these\nenergy-latency attacks reduce activation sparsity. In particular, we find that\ninput uniformity is a key enabler. A uniform image, that is, an image with\nmostly flat, uniformly colored surfaces, triggers more activations due to a\nspecific interplay of convolution, batch normalization, and ReLU activation.\nBased on these insights, we propose two new simple, yet effective strategies\nfor crafting sponge examples: sampling images from a probability distribution\nand identifying dense, yet inconspicuous inputs in natural datasets. We\nempirically examine our findings in a comprehensive evaluation with multiple\nimage classification models and show that our attack achieves the same sparsity\neffect as prior sponge-example methods, but at a fraction of computation\neffort. We also show that our sponge examples transfer between different neural\nnetworks. Finally, we discuss applications of our findings for the good by\nimproving efficiency by increasing sparsity.\n","authors":["Andreas Müller","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2403.18587v1.pdf","comment":"Accepted at the DLSP 2024"},{"id":"http://arxiv.org/abs/2403.18582v1","updated":"2024-03-27T14:03:41Z","published":"2024-03-27T14:03:41Z","title":"One flow to correct them all: improving simulations in high-energy\n  physics with a single normalising flow and a switch","summary":"  Simulated events are key ingredients in almost all high-energy physics\nanalyses. However, imperfections in the simulation can lead to sizeable\ndifferences between the observed data and simulated events. The effects of such\nmismodelling on relevant observables must be corrected either effectively via\nscale factors, with weights or by modifying the distributions of the\nobservables and their correlations. We introduce a correction method that\ntransforms one multidimensional distribution (simulation) into another one\n(data) using a simple architecture based on a single normalising flow with a\nboolean condition. We demonstrate the effectiveness of the method on a\nphysics-inspired toy dataset with non-trivial mismodelling of several\nobservables and their correlations.\n","authors":["Caio Cesar Daumann","Mauro Donega","Johannes Erdmann","Massimiliano Galli","Jan Lukas Späh","Davide Valsecchi"],"pdf_url":"https://arxiv.org/pdf/2403.18582v1.pdf","comment":"19 pages, 12 figures"},{"id":"http://arxiv.org/abs/2306.09459v3","updated":"2024-03-27T14:02:58Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events, it is essential to capture both the event\nitself and the decision point in the context of the model. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. In this paper, we propose the Recurrent Action Transformer\nwith Memory (RATE) - a model that incorporates recurrent memory. To evaluate\nour model, we conducted extensive experiments on both memory-intensive\nenvironments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo\ncontrol environments. The results show that the use of memory can significantly\nimprove performance in memory-intensive environments while maintaining or\nimproving results in classic environments. We hope that our findings will\nstimulate research on memory mechanisms for transformers applicable to offline\nreinforcement learning.\n","authors":["Alexey Staroverov","Egor Cherepanov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11427v2","updated":"2024-03-27T14:02:57Z","published":"2023-09-20T16:01:45Z","title":"Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing","summary":"  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n","authors":["Sewoong Lee","JinKyou Choi","Min Su Kim"],"pdf_url":"https://arxiv.org/pdf/2309.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18579v1","updated":"2024-03-27T13:59:09Z","published":"2024-03-27T13:59:09Z","title":"On Optimizing Hyperparameters for Quantum Neural Networks","summary":"  The increasing capabilities of Machine Learning (ML) models go hand in hand\nwith an immense amount of data and computational power required for training.\nTherefore, training is usually outsourced into HPC facilities, where we have\nstarted to experience limits in scaling conventional HPC hardware, as theorized\nby Moore's law. Despite heavy parallelization and optimization efforts, current\nstate-of-the-art ML models require weeks for training, which is associated with\nan enormous $CO_2$ footprint. Quantum Computing, and specifically Quantum\nMachine Learning (QML), can offer significant theoretical speed-ups and\nenhanced expressive power. However, training QML models requires tuning various\nhyperparameters, which is a nontrivial task and suboptimal choices can highly\naffect the trainability and performance of the models. In this study, we\nidentify the most impactful hyperparameters and collect data about the\nperformance of QML models. We compare different configurations and provide\nresearchers with performance data and concrete suggestions for hyperparameter\nselection.\n","authors":["Sabrina Herbst","Vincenzo De Maio","Ivona Brandic"],"pdf_url":"https://arxiv.org/pdf/2403.18579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18578v1","updated":"2024-03-27T13:59:05Z","published":"2024-03-27T13:59:05Z","title":"SteinGen: Generating Fidelitous and Diverse Graph Samples","summary":"  Generating graphs that preserve characteristic structures while promoting\nsample diversity can be challenging, especially when the number of graph\nobservations is small. Here, we tackle the problem of graph generation from\nonly one observed graph. The classical approach of graph generation from\nparametric models relies on the estimation of parameters, which can be\ninconsistent or expensive to compute due to intractable normalisation\nconstants. Generative modelling based on machine learning techniques to\ngenerate high-quality graph samples avoids parameter estimation but usually\nrequires abundant training samples. Our proposed generating procedure,\nSteinGen, which is phrased in the setting of graphs as realisations of\nexponential random graph models, combines ideas from Stein's method and MCMC by\nemploying Markovian dynamics which are based on a Stein operator for the target\nmodel. SteinGen uses the Glauber dynamics associated with an estimated Stein\noperator to generate a sample, and re-estimates the Stein operator from the\nsample after every sampling step. We show that on a class of exponential random\ngraph models this novel \"estimation and re-estimation\" generation strategy\nyields high distributional similarity (high fidelity) to the original data,\ncombined with high sample diversity.\n","authors":["Gesine Reinert","Wenkai Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18570v1","updated":"2024-03-27T13:51:26Z","published":"2024-03-27T13:51:26Z","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","summary":"  Water distribution systems (WDS) are an integral part of critical\ninfrastructure which is pivotal to urban development. As 70% of the world's\npopulation will likely live in urban environments in 2050, efficient simulation\nand planning tools for WDS play a crucial role in reaching UN's sustainable\ndevelopmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this\nrealm, we propose a novel and efficient machine learning emulator, more\nprecisely, a physics-informed deep learning (DL) model, for hydraulic state\nestimation in WDS. Using a recursive approach, our model only needs a few graph\nconvolutional neural network (GCN) layers and employs an innovative algorithm\nbased on message passing. Unlike conventional machine learning tasks, the model\nuses hydraulic principles to infer two additional hydraulic state features in\nthe process of reconstructing the available ground truth feature in an\nunsupervised manner. To the best of our knowledge, this is the first DL\napproach to emulate the popular hydraulic simulator EPANET, utilizing no\nadditional information. Like most DL models and unlike the hydraulic simulator,\nour model demonstrates vastly faster emulation times that do not increase\ndrastically with the size of the WDS. Moreover, we achieve high accuracy on the\nground truth and very similar results compared to the hydraulic simulator as\ndemonstrated through experiments on five real-world WDS datasets.\n","authors":["Inaam Ashraf","Janine Strotherm","Luca Hermes","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18570v1.pdf","comment":"Extended version of the paper with the same title published at\n  Proceedings of the AAAI Conference on Artificial Intelligence 2024"},{"id":"http://arxiv.org/abs/2403.18569v1","updated":"2024-03-27T13:50:13Z","published":"2024-03-27T13:50:13Z","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction","summary":"  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n","authors":["Yuxiang Zhao","Zhuomin Chai","Xun Jiang","Yibo Lin","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12370v2","updated":"2024-03-27T13:44:21Z","published":"2023-10-18T22:34:32Z","title":"No-Regret Learning in Bilateral Trade via Global Budget Balance","summary":"  Bilateral trade models the problem of intermediating between two rational\nagents -- a seller and a buyer -- both characterized by a private valuation for\nan item they want to trade. We study the online learning version of the\nproblem, in which at each time step a new seller and buyer arrive and the\nlearner has to set prices for them without any knowledge about their\n(adversarially generated) valuations.\n  In this setting, known impossibility results rule out the existence of\nno-regret algorithms when budget balanced has to be enforced at each time step.\nIn this paper, we introduce the notion of \\emph{global budget balance}, which\nonly requires the learner to fulfill budget balance over the entire time\nhorizon. Under this natural relaxation, we provide the first no-regret\nalgorithms for adversarial bilateral trade under various feedback models.\nFirst, we show that in the full-feedback model, the learner can guarantee\n$\\tilde O(\\sqrt{T})$ regret against the best fixed prices in hindsight, and\nthat this bound is optimal up to poly-logarithmic terms. Second, we provide a\nlearning algorithm guaranteeing a $\\tilde O(T^{3/4})$ regret upper bound with\none-bit feedback, which we complement with a $\\Omega(T^{5/7})$ lower bound that\nholds even in the two-bit feedback model. Finally, we introduce and analyze an\nalternative benchmark that is provably stronger than the best fixed prices in\nhindsight and is inspired by the literature on bandits with knapsacks.\n","authors":["Martino Bernasconi","Matteo Castiglioni","Andrea Celli","Federico Fusco"],"pdf_url":"https://arxiv.org/pdf/2310.12370v2.pdf","comment":"Accepted at STOC 2024"},{"id":"http://arxiv.org/abs/2403.18560v1","updated":"2024-03-27T13:42:14Z","published":"2024-03-27T13:42:14Z","title":"Noise-Robust Keyword Spotting through Self-supervised Pretraining","summary":"  Voice assistants are now widely available, and to activate them a keyword\nspotting (KWS) algorithm is used. Modern KWS systems are mainly trained using\nsupervised learning methods and require a large amount of labelled data to\nachieve a good performance. Leveraging unlabelled data through self-supervised\nlearning (SSL) has been shown to increase the accuracy in clean conditions.\nThis paper explores how SSL pretraining such as Data2Vec can be used to enhance\nthe robustness of KWS models in noisy conditions, which is under-explored.\n  Models of three different sizes are pretrained using different pretraining\napproaches and then fine-tuned for KWS. These models are then tested and\ncompared to models trained using two baseline supervised learning methods, one\nbeing standard training using clean data and the other one being multi-style\ntraining (MTR). The results show that pretraining and fine-tuning on clean data\nis superior to supervised learning on clean data across all testing conditions,\nand superior to supervised MTR for testing conditions of SNR above 5 dB. This\nindicates that pretraining alone can increase the model's robustness. Finally,\nit is found that using noisy data for pretraining models, especially with the\nData2Vec-denoising approach, significantly enhances the robustness of KWS\nmodels in noisy conditions.\n","authors":["Jacob Mørk","Holger Severin Bovbjerg","Gergely Kiss","Zheng-Hua Tan"],"pdf_url":"https://arxiv.org/pdf/2403.18560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06712v2","updated":"2024-03-27T13:38:35Z","published":"2024-01-12T17:26:51Z","title":"Few-Shot Detection of Machine-Generated Text using Style Representations","summary":"  The advent of instruction-tuned language models that convincingly mimic human\nwriting poses a significant risk of abuse. However, such abuse may be\ncounteracted with the ability to detect whether a piece of text was composed by\na language model rather than a human author. Some previous approaches to this\nproblem have relied on supervised methods by training on corpora of confirmed\nhuman- and machine- written documents. Unfortunately, model under-specification\nposes an unavoidable challenge for neural network-based detectors, making them\nbrittle in the face of data shifts, such as the release of newer language\nmodels producing still more fluent text than the models used to train the\ndetectors. Other approaches require access to the models that may have\ngenerated a document in question, which is often impractical. In light of these\nchallenges, we pursue a fundamentally different approach not relying on samples\nfrom language models of concern at training time. Instead, we propose to\nleverage representations of writing style estimated from human-authored text.\nIndeed, we find that features effective at distinguishing among human authors\nare also effective at distinguishing human from machine authors, including\nstate-of-the-art large language models like Llama-2, ChatGPT, and GPT-4.\nFurthermore, given a handful of examples composed by each of several specific\nlanguage models of interest, our approach affords the ability to predict which\nmodel generated a given document. The code and data to reproduce our\nexperiments are available at\nhttps://github.com/LLNL/LUAR/tree/main/fewshot_iclr2024.\n","authors":["Rafael Rivera Soto","Kailin Koch","Aleem Khan","Barry Chen","Marcus Bishop","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2401.06712v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00117v4","updated":"2024-03-27T13:38:00Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v4.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.18542v1","updated":"2024-03-27T13:22:38Z","published":"2024-03-27T13:22:38Z","title":"Attention-aware semantic relevance predicting Chinese sentence reading","summary":"  In recent years, several influential computational models and metrics have\nbeen proposed to predict how humans comprehend and process sentence. One\nparticularly promising approach is contextual semantic similarity. Inspired by\nthe attention algorithm in Transformer and human memory mechanisms, this study\nproposes an ``attention-aware'' approach for computing contextual semantic\nrelevance. This new approach takes into account the different contributions of\ncontextual parts and the expectation effect, allowing it to incorporate\ncontextual information fully. The attention-aware approach also facilitates the\nsimulation of existing reading models and evaluate them. The resulting\n``attention-aware'' metrics of semantic relevance can more accurately predict\nfixation durations in Chinese reading tasks recorded in an eye-tracking corpus\nthan those calculated by existing approaches. The study's findings further\nprovide strong support for the presence of semantic preview benefits in Chinese\nnaturalistic reading. Furthermore, the attention-aware metrics of semantic\nrelevance, being memory-based, possess high interpretability from both\nlinguistic and cognitive standpoints, making them a valuable computational tool\nfor modeling eye-movements in reading and further gaining insight into the\nprocess of language comprehension. Our approach underscores the potential of\nthese metrics to advance our comprehension of how humans understand and process\nlanguage, ultimately leading to a better understanding of language\ncomprehension and processing.\n","authors":["Kun Sun"],"pdf_url":"https://arxiv.org/pdf/2403.18542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18540v1","updated":"2024-03-27T13:17:15Z","published":"2024-03-27T13:17:15Z","title":"skscope: Fast Sparsity-Constrained Optimization in Python","summary":"  Applying iterative solvers on sparsity-constrained optimization (SCO)\nrequires tedious mathematical deduction and careful programming/debugging that\nhinders these solvers' broad impact. In the paper, the library skscope is\nintroduced to overcome such an obstacle. With skscope, users can solve the SCO\nby just programming the objective function. The convenience of skscope is\ndemonstrated through two examples in the paper, where sparse linear regression\nand trend filtering are addressed with just four lines of code. More\nimportantly, skscope's efficient implementation allows state-of-the-art solvers\nto quickly attain the sparse solution regardless of the high dimensionality of\nparameter space. Numerical experiments reveal the available solvers in skscope\ncan achieve up to 80x speedup on the competing relaxation solutions obtained\nvia the benchmarked convex solver. skscope is published on the Python Package\nIndex (PyPI) and Conda, and its source code is available at:\nhttps://github.com/abess-team/skscope.\n","authors":["Zezhi Wang","Jin Zhu","Peng Chen","Huiyang Peng","Xiaoke Zhang","Anran Wang","Yu Zheng","Junxian Zhu","Xueqin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18540v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2403.18539v1","updated":"2024-03-27T13:14:29Z","published":"2024-03-27T13:14:29Z","title":"Safe and Robust Reinforcement-Learning: Principles and Practice","summary":"  Reinforcement Learning (RL) has shown remarkable success in solving\nrelatively complex tasks, yet the deployment of RL systems in real-world\nscenarios poses significant challenges related to safety and robustness. This\npaper aims to identify and further understand those challenges thorough the\nexploration of the main dimensions of the safe and robust RL landscape,\nencompassing algorithmic, ethical, and practical considerations. We conduct a\ncomprehensive review of methodologies and open problems that summarizes the\nefforts in recent years to address the inherent risks associated with RL\napplications.\n  After discussing and proposing definitions for both safe and robust RL, the\npaper categorizes existing research works into different algorithmic approaches\nthat enhance the safety and robustness of RL agents. We examine techniques such\nas uncertainty estimation, optimisation methodologies, exploration-exploitation\ntrade-offs, and adversarial training. Environmental factors, including\nsim-to-real transfer and domain adaptation, are also scrutinized to understand\nhow RL systems can adapt to diverse and dynamic surroundings. Moreover, human\ninvolvement is an integral ingredient of the analysis, acknowledging the broad\nset of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and\nrobust RL implementation, this paper introduces a practical checklist derived\nfrom the synthesized literature. The checklist encompasses critical aspects of\nalgorithm design, training environment considerations, and ethical guidelines.\nIt will serve as a resource for developers and policymakers alike to ensure the\nresponsible deployment of RL systems in many application domains.\n","authors":["Taku Yamagata","Raul Santos-Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2403.18539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18535v1","updated":"2024-03-27T13:11:34Z","published":"2024-03-27T13:11:34Z","title":"Theoretical Bound-Guided Hierarchical VAE for Neural Image Codecs","summary":"  Recent studies reveal a significant theoretical link between variational\nautoencoders (VAEs) and rate-distortion theory, notably in utilizing VAEs to\nestimate the theoretical upper bound of the information rate-distortion\nfunction of images. Such estimated theoretical bounds substantially exceed the\nperformance of existing neural image codecs (NICs). To narrow this gap, we\npropose a theoretical bound-guided hierarchical VAE (BG-VAE) for NIC. The\nproposed BG-VAE leverages the theoretical bound to guide the NIC model towards\nenhanced performance. We implement the BG-VAE using Hierarchical VAEs and\ndemonstrate its effectiveness through extensive experiments. Along with\nadvanced neural network blocks, we provide a versatile, variable-rate NIC that\noutperforms existing methods when considering both rate-distortion performance\nand computational complexity. The code is available at BG-VAE.\n","authors":["Yichi Zhang","Zhihao Duan","Yuning Huang","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.18535v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME2024)"},{"id":"http://arxiv.org/abs/2403.18525v1","updated":"2024-03-27T12:59:44Z","published":"2024-03-27T12:59:44Z","title":"Language Plays a Pivotal Role in the Object-Attribute Compositional\n  Generalization of CLIP","summary":"  Vision-language models, such as CLIP, have shown promising\nOut-of-Distribution (OoD) generalization under various types of distribution\nshifts. Recent studies attempted to investigate the leading cause of this\ncapability. In this work, we follow the same path, but focus on a specific type\nof OoD data - images with novel compositions of attribute-object pairs - and\nstudy whether such models can successfully classify those images into\ncomposition classes. We carefully designed an authentic image test dataset\ncalled ImageNet-AO, consisting of attributes for objects that are unlikely\nencountered in the CLIP training sets. We found that CLIPs trained with large\ndatasets such as OpenAI CLIP, LAION-400M, and LAION-2B show orders-of-magnitude\nimprovement in effective compositional OoD generalization compared to both\nsupervised models and CLIPs trained with smaller datasets, such as CC-12M and\nYFCC-15M. Our results provide evidence that the scale and diversity of training\ndata and language supervision play a key role in unlocking the compositional\ngeneralization abilities of vision-language models.\n","authors":["Reza Abbasi","Mohammad Samiei","Mohammad Hossein Rohban","Mahdieh Soleymani Baghshah"],"pdf_url":"https://arxiv.org/pdf/2403.18525v1.pdf","comment":"Oral accepted at OODCV 2023(http://www.ood-cv.org)"},{"id":"http://arxiv.org/abs/2303.10365v3","updated":"2024-03-27T12:53:12Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18519v1","updated":"2024-03-27T12:50:27Z","published":"2024-03-27T12:50:27Z","title":"Improving Line Search Methods for Large Scale Neural Network Training","summary":"  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n","authors":["Philip Kenneweg","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18517v1","updated":"2024-03-27T12:49:14Z","published":"2024-03-27T12:49:14Z","title":"Efficient Algorithms for Regularized Nonnegative Scale-invariant\n  Low-rank Approximation Models","summary":"  Regularized nonnegative low-rank approximations such as sparse Nonnegative\nMatrix Factorization or sparse Nonnegative Tucker Decomposition are an\nimportant branch of dimensionality reduction models with enhanced\ninterpretability. However, from a practical perspective, the choice of\nregularizers and regularization coefficients, as well as the design of\nefficient algorithms, is challenging because of the multifactor nature of these\nmodels and the lack of theory to back these choices. This paper aims at\nimproving upon these issues. By studying a more general model called the\nHomogeneous Regularized Scale-Invariant, we prove that the scale-invariance\ninherent to low-rank approximation models causes an implicit regularization\nwith both unexpected beneficial and detrimental effects. This observation\nallows to better understand the effect of regularization functions in low-rank\napproximation models, to guide the choice of the regularization\nhyperparameters, and to design balancing strategies to enhance the convergence\nspeed of dedicated optimization algorithms. Some of these results were already\nknown but restricted to specific instances of regularized low-rank\napproximations. We also derive a generic Majorization Minimization algorithm\nthat handles many regularized nonnegative low-rank approximations, with\nconvergence guarantees. We showcase our contributions on sparse Nonnegative\nMatrix Factorization, ridge-regularized Canonical Polyadic decomposition and\nsparse Nonnegative Tucker Decomposition.\n","authors":["Jeremy E. Cohen","Valentin Leplat"],"pdf_url":"https://arxiv.org/pdf/2403.18517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18514v1","updated":"2024-03-27T12:44:57Z","published":"2024-03-27T12:44:57Z","title":"CT-3DFlow : Leveraging 3D Normalizing Flows for Unsupervised Detection\n  of Pathological Pulmonary CT scans","summary":"  Unsupervised pathology detection can be implemented by training a model on\nhealthy data only and measuring the deviation from the training set upon\ninference, for example with CNN-based feature extraction and one-class\nclassifiers, or reconstruction-score-based methods such as AEs, GANs and\nDiffusion models. Normalizing Flows (NF) have the ability to directly learn the\nprobability distribution of training examples through an invertible\narchitecture. We leverage this property in a novel 3D NF-based model named\nCT-3DFlow, specifically tailored for patient-level pulmonary pathology\ndetection in chest CT data. Our model is trained unsupervised on healthy 3D\npulmonary CT patches, and detects deviations from its log-likelihood\ndistribution as anomalies. We aggregate patches-level likelihood values from a\npatient's CT scan to provide a patient-level 'normal'/'abnormal' prediction.\nOut-of-distribution detection performance is evaluated using expert annotations\non a separate chest CT test dataset, outperforming other state-of-the-art\nmethods.\n","authors":["Aissam Djahnine","Alexandre Popoff","Emilien Jupin-Delevaux","Vincent Cottin","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2403.18514v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18509v1","updated":"2024-03-27T12:39:16Z","published":"2024-03-27T12:39:16Z","title":"Distributed Maximum Consensus over Noisy Links","summary":"  We introduce a distributed algorithm, termed noise-robust distributed maximum\nconsensus (RD-MC), for estimating the maximum value within a multi-agent\nnetwork in the presence of noisy communication links. Our approach entails\nredefining the maximum consensus problem as a distributed optimization problem,\nallowing a solution using the alternating direction method of multipliers.\nUnlike existing algorithms that rely on multiple sets of noise-corrupted\nestimates, RD-MC employs a single set, enhancing both robustness and\nefficiency. To further mitigate the effects of link noise and improve\nrobustness, we apply moving averaging to the local estimates. Through extensive\nsimulations, we demonstrate that RD-MC is significantly more robust to\ncommunication link noise compared to existing maximum-consensus algorithms.\n","authors":["Ehsan Lari","Reza Arablouei","Naveen K. D. Venkategowda","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18509v1.pdf","comment":"5 pages, 7 figures, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18506v1","updated":"2024-03-27T12:35:23Z","published":"2024-03-27T12:35:23Z","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","summary":"  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n","authors":["Philip Kenneweg","Leonardo Galli","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08579v2","updated":"2024-03-27T12:28:02Z","published":"2024-03-13T14:34:34Z","title":"Machine Learning Optimized Orthogonal Basis Piecewise Polynomial\n  Approximation","summary":"  Piecewise Polynomials (PPs) are utilized in several engineering disciplines,\nlike trajectory planning, to approximate position profiles given in the form of\na set of points. While the approximation target along with domain-specific\nrequirements, like Ck -continuity, can be formulated as a system of equations\nand a result can be computed directly, such closed-form solutions posses\nlimited flexibility with respect to polynomial degrees, polynomial bases or\nadding further domain-specific requirements. Sufficiently complex optimization\ngoals soon call for the use of numerical methods, like gradient descent. Since\ngradient descent lies at the heart of training Artificial Neural Networks\n(ANNs), modern Machine Learning (ML) frameworks like TensorFlow come with a set\nof gradient-based optimizers potentially suitable for a wide range of\noptimization problems beyond the training task for ANNs. Our approach is to\nutilize the versatility of PP models and combine it with the potential of\nmodern ML optimizers for the use in function approximation in 1D trajectory\nplanning in the context of electronic cam design. We utilize available\noptimizers of the ML framework TensorFlow directly, outside of the scope of\nANNs, to optimize model parameters of our PP model. In this paper, we show how\nan orthogonal polynomial basis contributes to improving approximation and\ncontinuity optimization performance. Utilizing Chebyshev polynomials of the\nfirst kind, we develop a novel regularization approach enabling clearly\nimproved convergence behavior. We show that, using this regularization\napproach, Chebyshev basis performs better than power basis for all relevant\noptimizers in the combined approximation and continuity optimization setting\nand demonstrate usability of the presented approach within the electronic cam\ndomain.\n","authors":["Hannes Waclawek","Stefan Huber"],"pdf_url":"https://arxiv.org/pdf/2403.08579v2.pdf","comment":"Submitted to LION18"},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.18495v1","updated":"2024-03-27T12:15:22Z","published":"2024-03-27T12:15:22Z","title":"Direct mineral content prediction from drill core images via transfer\n  learning","summary":"  Deep subsurface exploration is important for mining, oil and gas industries,\nas well as in the assessment of geological units for the disposal of chemical\nor nuclear waste, or the viability of geothermal energy systems. Typically,\ndetailed examinations of subsurface formations or units are performed on\ncuttings or core materials extracted during drilling campaigns, as well as on\ngeophysical borehole data, which provide detailed information about the\npetrophysical properties of the rocks. Depending on the volume of rock samples\nand the analytical program, the laboratory analysis and diagnostics can be very\ntime-consuming. This study investigates the potential of utilizing machine\nlearning, specifically convolutional neural networks (CNN), to assess the\nlithology and mineral content solely from analysis of drill core images, aiming\nto support and expedite the subsurface geological exploration. The paper\noutlines a comprehensive methodology, encompassing data preprocessing, machine\nlearning methods, and transfer learning techniques. The outcome reveals a\nremarkable 96.7% accuracy in the classification of drill core segments into\ndistinct formation classes. Furthermore, a CNN model was trained for the\nevaluation of mineral content using a learning data set from multidimensional\nlog analysis data (silicate, total clay, carbonate). When benchmarked against\nlaboratory XRD measurements on samples from the cores, both the advanced\nmultidimensional log analysis model and the neural network approach developed\nhere provide equally good performance. This work demonstrates that deep\nlearning and particularly transfer learning can support extracting\npetrophysical properties, including mineral content and formation\nclassification, from drill core images, thus offering a road map for enhancing\nmodel performance and data set quality in image-based analysis of drill cores.\n","authors":["Romana Boiger","Sergey V. Churakov","Ignacio Ballester Llagaria","Georg Kosakowski","Raphael Wüst","Nikolaos I. Prasianakis"],"pdf_url":"https://arxiv.org/pdf/2403.18495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18494v1","updated":"2024-03-27T12:10:30Z","published":"2024-03-27T12:10:30Z","title":"Learning in PINNs: Phase transition, total diffusion, and generalization","summary":"  We investigate the learning dynamics of fully-connected neural networks\nthrough the lens of gradient signal-to-noise ratio (SNR), examining the\nbehavior of first-order optimizers like Adam in non-convex objectives. By\ninterpreting the drift/diffusion phases in the information bottleneck theory,\nfocusing on gradient homogeneity, we identify a third phase termed ``total\ndiffusion\", characterized by equilibrium in the learning rates and homogeneous\ngradients. This phase is marked by an abrupt SNR increase, uniform residuals\nacross the sample space and the most rapid training convergence. We propose a\nresidual-based re-weighting scheme to accelerate this diffusion in quadratic\nloss functions, enhancing generalization. We also explore the information\ncompression phenomenon, pinpointing a significant saturation-induced\ncompression of activations at the total diffusion phase, with deeper layers\nexperiencing negligible information loss. Supported by experimental data on\nphysics-informed neural networks (PINNs), which underscore the importance of\ngradient homogeneity due to their PDE-based sample inter-dependence, our\nfindings suggest that recognizing phase transitions could refine ML\noptimization strategies for improved generalization.\n","authors":["Sokratis J. Anagnostopoulos","Juan Diego Toscano","Nikolaos Stergiopulos","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2403.18494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18489v1","updated":"2024-03-27T12:01:51Z","published":"2024-03-27T12:01:51Z","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of\n  Evapotranspiration by Deep Neural Network Models","summary":"  Reference Evapotranspiration (ET0) is a key parameter for designing smart\nirrigation scheduling, since it is related by a coefficient to the water needs\nof a crop. The United Nations Food and Agriculture Organization, proposed a\nstandard method for ET0 computation (FAO56PM), based on the parameterization of\nthe Penman-Monteith equation, that is widely adopted in the literature. To\ncompute ET0 using the FAO56-PM method, four main weather parameters are needed:\ntemperature, humidity, wind, and solar radiation (SR). One way to make daily\nET0 estimations for future days is to use freely available weather forecast\nservices (WFSs), where many meteorological parameters are estimated up to the\nnext 15 days. A problem with this method is that currently, SR is not provided\nas a free forecast parameter on most of those online services or, normally,\nsuch forecasts present a financial cost penalty. For this reason, several ET0\nestimation models using machine and deep learning were developed and presented\nin the literature, that use as input features a reduced set of carefully\nselected weather parameters, that are compatible with common freely available\nWFSs. However, most studies on this topic have only evaluated model performance\nusing data from weather stations (WSs), without considering the effect of using\nweather forecast data. In this study, the performance of authors' previous\nmodels is evaluated when using weather forecast data from two online WFSs, in\nthe following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)\nestimate SR by ANN model, and then use that estimation for ET0 computation,\nusing the FAO56-PM method. Employing data collected from two WFSs and a WS\nlocated in Vale do Lobo, Portugal, the latter approach achieved the best\nresult, with a coefficient of determination (R2) ranging between 0.893 and\n0.667, when considering forecasts up to 15 days.\n","authors":["Pedro J. Vaz","Gabriela Schütz","Carlos Guerrero","Pedro J. S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2403.18489v1.pdf","comment":"A partial version of the work submitted to ESRE/INTERNATIONAL\n  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY"},{"id":"http://arxiv.org/abs/2403.18486v1","updated":"2024-03-27T11:58:45Z","published":"2024-03-27T11:58:45Z","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with\n  Conditional Diffusion Models","summary":"  Data scarcity in the brain-computer interface field can be alleviated through\nthe use of generative models, specifically diffusion models. While diffusion\nmodels have previously been successfully applied to electroencephalogram (EEG)\ndata, existing models lack flexibility w.r.t.~sampling or require alternative\nrepresentations of the EEG data. To overcome these limitations, we introduce a\nnovel approach to conditional diffusion models that utilizes classifier-free\nguidance to directly generate subject-, session-, and class-specific EEG data.\nIn addition to commonly used metrics, domain-specific metrics are employed to\nevaluate the specificity of the generated samples. The results indicate that\nthe proposed model can generate EEG data that resembles real data for each\nsubject, session, and class.\n","authors":["Guido Klein","Pierre Guetschel","Gianluigi Silvestri","Michael Tangermann"],"pdf_url":"https://arxiv.org/pdf/2403.18486v1.pdf","comment":"submitted to 9th Graz BCI conference, 6 pages, 3 figures, first\n  figure is split into two subfigures, 1 table"},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.18452v1","updated":"2024-03-27T11:11:08Z","published":"2024-03-27T11:11:08Z","title":"SingularTrajectory: Universal Trajectory Predictor Using Diffusion Model","summary":"  There are five types of trajectory prediction tasks: deterministic,\nstochastic, domain adaptation, momentary observation, and few-shot. These\nassociated tasks are defined by various factors, such as the length of input\npaths, data split and pre-processing methods. Interestingly, even though they\ncommonly take sequential coordinates of observations as input and infer future\npaths in the same coordinates as output, designing specialized architectures\nfor each task is still necessary. For the other task, generality issues can\nlead to sub-optimal performances. In this paper, we propose SingularTrajectory,\na diffusion-based universal trajectory prediction framework to reduce the\nperformance gap across the five tasks. The core of SingularTrajectory is to\nunify a variety of human dynamics representations on the associated tasks. To\ndo this, we first build a Singular space to project all types of motion\npatterns from each task into one embedding space. We next propose an adaptive\nanchor working in the Singular space. Unlike traditional fixed anchor methods\nthat sometimes yield unacceptable paths, our adaptive anchor enables correct\nanchors, which are put into a wrong location, based on a traversability map.\nFinally, we adopt a diffusion-based predictor to further enhance the prototype\npaths using a cascaded denoising process. Our unified framework ensures the\ngenerality across various benchmark settings such as input modality, and\ntrajectory lengths. Extensive experiments on five public benchmarks demonstrate\nthat SingularTrajectory substantially outperforms existing models, highlighting\nits effectiveness in estimating general dynamics of human movements. Code is\npublicly available at https://github.com/inhwanbae/SingularTrajectory .\n","authors":["Inhwan Bae","Young-Jae Park","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18452v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18451v1","updated":"2024-03-27T11:11:06Z","published":"2024-03-27T11:11:06Z","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT","summary":"  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n","authors":["Yi Hu","Jinhang Zuo","Alanis Zhao","Bob Iannucci","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.18451v1.pdf","comment":"accepted and to be published in 2024 IEEE International Workshop on\n  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)"},{"id":"http://arxiv.org/abs/2403.09267v3","updated":"2024-03-27T11:11:02Z","published":"2024-03-14T10:44:10Z","title":"Deep Limit Order Book Forecasting","summary":"  We exploit cutting-edge deep learning methodologies to explore the\npredictability of high-frequency Limit Order Book mid-price changes for a\nheterogeneous set of stocks traded on the NASDAQ exchange. In so doing, we\nrelease `LOBFrame', an open-source code base to efficiently process large-scale\nLimit Order Book data and quantitatively assess state-of-the-art deep learning\nmodels' forecasting capabilities. Our results are twofold. We demonstrate that\nthe stocks' microstructural characteristics influence the efficacy of deep\nlearning methods and that their high forecasting power does not necessarily\ncorrespond to actionable trading signals. We argue that traditional machine\nlearning metrics fail to adequately assess the quality of forecasts in the\nLimit Order Book context. As an alternative, we propose an innovative\noperational framework that evaluates predictions' practicality by focusing on\nthe probability of accurately forecasting complete transactions. This work\noffers academics and practitioners an avenue to make informed and robust\ndecisions on the application of deep learning techniques, their scope and\nlimitations, effectively exploiting emergent statistical properties of the\nLimit Order Book.\n","authors":["Antonio Briola","Silvia Bartolucci","Tomaso Aste"],"pdf_url":"https://arxiv.org/pdf/2403.09267v3.pdf","comment":"43 pages, 14 figures, 12 Tables"},{"id":"http://arxiv.org/abs/2403.18447v1","updated":"2024-03-27T11:06:44Z","published":"2024-03-27T11:06:44Z","title":"Can Language Beat Numerical Regression? Language-Based Multimodal\n  Trajectory Prediction","summary":"  Language models have demonstrated impressive ability in context understanding\nand generative performance. Inspired by the recent success of language\nfoundation models, in this paper, we propose LMTraj (Language-based Multimodal\nTrajectory predictor), which recasts the trajectory prediction task into a sort\nof question-answering problem. Departing from traditional numerical regression\nmodels, which treat the trajectory coordinate sequence as continuous signals,\nwe consider them as discrete signals like text prompts. Specially, we first\ntransform an input space for the trajectory coordinate into the natural\nlanguage space. Here, the entire time-series trajectories of pedestrians are\nconverted into a text prompt, and scene images are described as text\ninformation through image captioning. The transformed numerical and image data\nare then wrapped into the question-answering template for use in a language\nmodel. Next, to guide the language model in understanding and reasoning\nhigh-level knowledge, such as scene context and social relationships between\npedestrians, we introduce an auxiliary multi-task question and answering. We\nthen train a numerical tokenizer with the prompt data. We encourage the\ntokenizer to separate the integer and decimal parts well, and leverage it to\ncapture correlations between the consecutive numbers in the language model.\nLastly, we train the language model using the numerical tokenizer and all of\nthe question-answer prompts. Here, we propose a beam-search-based most-likely\nprediction and a temperature-based multimodal prediction to implement both\ndeterministic and stochastic inferences. Applying our LMTraj, we show that the\nlanguage-based model can be a powerful pedestrian trajectory predictor, and\noutperforms existing numerical-based predictor methods. Code is publicly\navailable at https://github.com/inhwanbae/LMTrajectory .\n","authors":["Inhwan Bae","Junoh Lee","Hae-Gon Jeon"],"pdf_url":"https://arxiv.org/pdf/2403.18447v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18444v1","updated":"2024-03-27T11:00:53Z","published":"2024-03-27T11:00:53Z","title":"FRESCO: Federated Reinforcement Energy System for Cooperative\n  Optimization","summary":"  The rise in renewable energy is creating new dynamics in the energy grid that\npromise to create a cleaner and more participative energy grid, where\ntechnology plays a crucial part in making the required flexibility to achieve\nthe vision of the next-generation grid. This work presents FRESCO, a framework\nthat aims to ease the implementation of energy markets using a hierarchical\ncontrol architecture of reinforcement learning agents trained using federated\nlearning. The core concept we are proving is that having greedy agents subject\nto changing conditions from a higher level agent creates a cooperative setup\nthat will allow for fulfilling all the individual objectives. This paper\npresents a general overview of the framework, the current progress, and some\ninsights we obtained from the recent results.\n","authors":["Nicolas Mauricio Cuadrado","Roberto Alejandro Gutierrez","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18444v1.pdf","comment":"Tiny Paper at ICLR 2023"},{"id":"http://arxiv.org/abs/2403.18439v1","updated":"2024-03-27T10:47:06Z","published":"2024-03-27T10:47:06Z","title":"Generalized Policy Learning for Smart Grids: FL TRPO Approach","summary":"  The smart grid domain requires bolstering the capabilities of existing energy\nmanagement systems; Federated Learning (FL) aligns with this goal as it\ndemonstrates a remarkable ability to train models on heterogeneous datasets\nwhile maintaining data privacy, making it suitable for smart grid applications,\nwhich often involve disparate data distributions and interdependencies among\nfeatures that hinder the suitability of linear models. This paper introduces a\nframework that combines FL with a Trust Region Policy Optimization (FL TRPO)\naiming to reduce energy-associated emissions and costs. Our approach reveals\nlatent interconnections and employs personalized encoding methods to capture\nunique insights, understanding the relationships between features and optimal\nstrategies, allowing our model to generalize to previously unseen data.\nExperimental results validate the robustness of our approach, affirming its\nproficiency in effectively learning policy models for smart grid challenges.\n","authors":["Yunxiang Li","Nicolas Mauricio Cuadrado","Samuel Horváth","Martin Takáč"],"pdf_url":"https://arxiv.org/pdf/2403.18439v1.pdf","comment":"ICLR 2024 Workshop: Tackling Climate Change with Machine Learning"},{"id":"http://arxiv.org/abs/2403.18438v1","updated":"2024-03-27T10:45:16Z","published":"2024-03-27T10:45:16Z","title":"Global Vegetation Modeling with Pre-Trained Weather Transformers","summary":"  Accurate vegetation models can produce further insights into the complex\ninteraction between vegetation activity and ecosystem processes. Previous\nresearch has established that long-term trends and short-term variability of\ntemperature and precipitation affect vegetation activity. Motivated by the\nrecent success of Transformer-based Deep Learning models for medium-range\nweather forecasting, we adapt the publicly available pre-trained FourCastNet to\nmodel vegetation activity while accounting for the short-term dynamics of\nclimate variability. We investigate how the learned global representation of\nthe atmosphere's state can be transferred to model the normalized difference\nvegetation index (NDVI). Our model globally estimates vegetation activity at a\nresolution of \\SI{0.25}{\\degree} while relying only on meteorological data. We\ndemonstrate that leveraging pre-trained weather models improves the NDVI\nestimates compared to learning an NDVI model from scratch. Additionally, we\ncompare our results to other recent data-driven NDVI modeling approaches from\nmachine learning and ecology literature. We further provide experimental\nevidence on how much data and training time is necessary to turn FourCastNet\ninto an effective vegetation model. Code and models will be made available upon\npublication.\n","authors":["Pascal Janetzky","Florian Gallusser","Simon Hentschel","Andreas Hotho","Anna Krause"],"pdf_url":"https://arxiv.org/pdf/2403.18438v1.pdf","comment":"Tackling Climate Change with Machine Learning Workshop @ ICLR 2024"},{"id":"http://arxiv.org/abs/2403.18436v1","updated":"2024-03-27T10:40:27Z","published":"2024-03-27T10:40:27Z","title":"Collaborative Active Learning in Conditional Trust Environment","summary":"  In this paper, we investigate collaborative active learning, a paradigm in\nwhich multiple collaborators explore a new domain by leveraging their combined\nmachine learning capabilities without disclosing their existing data and\nmodels. Instead, the collaborators share prediction results from the new domain\nand newly acquired labels. This collaboration offers several advantages: (a) it\naddresses privacy and security concerns by eliminating the need for direct\nmodel and data disclosure; (b) it enables the use of different data sources and\ninsights without direct data exchange; and (c) it promotes cost-effectiveness\nand resource efficiency through shared labeling costs. To realize these\nbenefits, we introduce a collaborative active learning framework designed to\nfulfill the aforementioned objectives. We validate the effectiveness of the\nproposed framework through simulations. The results demonstrate that\ncollaboration leads to higher AUC scores compared to independent efforts,\nhighlighting the framework's ability to overcome the limitations of individual\nmodels. These findings support the use of collaborative approaches in active\nlearning, emphasizing their potential to enhance outcomes through collective\nexpertise and shared resources. Our work provides a foundation for further\nresearch on collaborative active learning and its practical applications in\nvarious domains where data privacy, cost efficiency, and model performance are\ncritical considerations.\n","authors":["Zan-Kai Chong","Hiroyuki Ohsaki","Bryan Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18436v1.pdf","comment":"5 pages, 9 figures, conference"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18423v1","updated":"2024-03-27T10:24:25Z","published":"2024-03-27T10:24:25Z","title":"SemRoDe: Macro Adversarial Training to Learn Representations That are\n  Robust to Word-Level Attacks","summary":"  Language models (LMs) are indispensable tools for natural language processing\ntasks, but their vulnerability to adversarial attacks remains a concern. While\ncurrent research has explored adversarial training techniques, their\nimprovements to defend against word-level attacks have been limited. In this\nwork, we propose a novel approach called Semantic Robust Defence (SemRoDe), a\nMacro Adversarial Training strategy to enhance the robustness of LMs. Drawing\ninspiration from recent studies in the image domain, we investigate and later\nconfirm that in a discrete data setting such as language, adversarial samples\ngenerated via word substitutions do indeed belong to an adversarial domain\nexhibiting a high Wasserstein distance from the base domain. Our method learns\na robust representation that bridges these two domains. We hypothesize that if\nsamples were not projected into an adversarial domain, but instead to a domain\nwith minimal shift, it would improve attack robustness. We align the domains by\nincorporating a new distance-based objective. With this, our model is able to\nlearn more generalized representations by aligning the model's high-level\noutput features and therefore better handling unseen adversarial samples. This\nmethod can be generalized across word embeddings, even when they share minimal\noverlap at both vocabulary and word-substitution levels. To evaluate the\neffectiveness of our approach, we conduct experiments on BERT and RoBERTa\nmodels on three datasets. The results demonstrate promising state-of-the-art\nrobustness.\n","authors":["Brian Formento","Wenjie Feng","Chuan Sheng Foo","Luu Anh Tuan","See-Kiong Ng"],"pdf_url":"https://arxiv.org/pdf/2403.18423v1.pdf","comment":"Published in NAACL 2024 (Main Track)"},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01191v2","updated":"2024-03-27T10:12:31Z","published":"2023-11-02T12:36:19Z","title":"VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification","summary":"  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.01191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18415v1","updated":"2024-03-27T10:06:33Z","published":"2024-03-27T10:06:33Z","title":"The Topos of Transformer Networks","summary":"  The transformer neural network has significantly out-shined all other neural\nnetwork architectures as the engine behind large language models. We provide a\ntheoretical analysis of the expressivity of the transformer architecture\nthrough the lens of topos theory. From this viewpoint, we show that many common\nneural network architectures, such as the convolutional, recurrent and graph\nconvolutional networks, can be embedded in a pretopos of piecewise-linear\nfunctions, but that the transformer necessarily lives in its topos completion.\nIn particular, this suggests that the two network families instantiate\ndifferent fragments of logic: the former are first order, whereas transformers\nare higher-order reasoners. Furthermore, we draw parallels with architecture\nsearch and gradient descent, integrating our analysis in the framework of\ncybernetic agents.\n","authors":["Mattia Jacopo Villani","Peter McBurney"],"pdf_url":"https://arxiv.org/pdf/2403.18415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.06075v2","updated":"2024-03-27T09:51:15Z","published":"2023-09-12T09:12:37Z","title":"A2V: A Semi-Supervised Domain Adaptation Framework for Brain Vessel\n  Segmentation via Two-Phase Training Angiography-to-Venography Translation","summary":"  We present a semi-supervised domain adaptation framework for brain vessel\nsegmentation from different image modalities. Existing state-of-the-art methods\nfocus on a single modality, despite the wide range of available cerebrovascular\nimaging techniques. This can lead to significant distribution shifts that\nnegatively impact the generalization across modalities. By relying on annotated\nangiographies and a limited number of annotated venographies, our framework\naccomplishes image-to-image translation and semantic segmentation, leveraging a\ndisentangled and semantically rich latent space to represent heterogeneous data\nand perform image-level adaptation from source to target domains. Moreover, we\nreduce the typical complexity of cycle-based architectures and minimize the use\nof adversarial training, which allows us to build an efficient and intuitive\nmodel with stable training. We evaluate our method on magnetic resonance\nangiographies and venographies. While achieving state-of-the-art performance in\nthe source domain, our method attains a Dice score coefficient in the target\ndomain that is only 8.9% lower, highlighting its promising potential for robust\ncerebrovascular image segmentation across different modalities.\n","authors":["Francesco Galati","Daniele Falcetta","Rosa Cortese","Barbara Casolla","Ferran Prados","Ninon Burgos","Maria A. Zuluaga"],"pdf_url":"https://arxiv.org/pdf/2309.06075v2.pdf","comment":"Accepted at the 34th British Machine Vision Conference (BMVC)"},{"id":"http://arxiv.org/abs/2310.05723v2","updated":"2024-03-27T09:48:34Z","published":"2023-10-09T13:47:05Z","title":"Planning to Go Out-of-Distribution in Offline-to-Online Reinforcement\n  Learning","summary":"  Offline pretraining with a static dataset followed by online fine-tuning\n(offline-to-online, or OtO) is a paradigm well matched to a real-world RL\ndeployment process. In this scenario, we aim to find the best-performing policy\nwithin a limited budget of online interactions. Previous work in the OtO\nsetting has focused on correcting for bias introduced by the policy-constraint\nmechanisms of offline RL algorithms. Such constraints keep the learned policy\nclose to the behavior policy that collected the dataset, but we show this can\nunnecessarily limit policy performance if the behavior policy is far from\noptimal. Instead, we forgo constraints and frame OtO RL as an exploration\nproblem that aims to maximize the benefit of online data-collection. We first\nstudy the major online RL exploration methods based on intrinsic rewards and\nUCB in the OtO setting, showing that intrinsic rewards add training instability\nthrough reward-function modification, and UCB methods are myopic and it is\nunclear which learned-component's ensemble to use for action selection. We then\nintroduce an algorithm for planning to go out-of-distribution (PTGOOD) that\navoids these issues. PTGOOD uses a non-myopic planning procedure that targets\nexploration in relatively high-reward regions of the state-action space\nunlikely to be visited by the behavior policy. By leveraging concepts from the\nConditional Entropy Bottleneck, PTGOOD encourages data collected online to\nprovide new information relevant to improving the final deployment policy\nwithout altering rewards. We show empirically in several continuous control\ntasks that PTGOOD significantly improves agent returns during online\nfine-tuning and avoids the suboptimal policy convergence that many of our\nbaselines exhibit in several environments.\n","authors":["Trevor McInroe","Adam Jelley","Stefano V. Albrecht","Amos Storkey"],"pdf_url":"https://arxiv.org/pdf/2310.05723v2.pdf","comment":"10 pages, 17 figures, preprint"},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18402v1","updated":"2024-03-27T09:44:50Z","published":"2024-03-27T09:44:50Z","title":"On Spectrogram Analysis in a Multiple Classifier Fusion Framework for\n  Power Grid Classification Using Electric Network Frequency","summary":"  The Electric Network Frequency (ENF) serves as a unique signature inherent to\npower distribution systems. Here, a novel approach for power grid\nclassification is developed, leveraging ENF. Spectrograms are generated from\naudio and power recordings across different grids, revealing distinctive ENF\npatterns that aid in grid classification through a fusion of classifiers. Four\ntraditional machine learning classifiers plus a Convolutional Neural Network\n(CNN), optimized using Neural Architecture Search, are developed for One-vs-All\nclassification. This process generates numerous predictions per sample, which\nare then compiled and used to train a shallow multi-label neural network\nspecifically designed to model the fusion process, ultimately leading to the\nconclusive class prediction for each sample. Experimental findings reveal that\nboth validation and testing accuracy outperform those of current\nstate-of-the-art classifiers, underlining the effectiveness and robustness of\nthe proposed methodology.\n","authors":["Georgios Tzolopoulos","Christos Korgialas","Constantine Kotropoulos"],"pdf_url":"https://arxiv.org/pdf/2403.18402v1.pdf","comment":"13th International Conference on Pattern Recognition Applications and\n  Methods (ICPRAM)"},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.18393v1","updated":"2024-03-27T09:30:50Z","published":"2024-03-27T09:30:50Z","title":"Tensor-based Graph Learning with Consistency and Specificity for\n  Multi-view Clustering","summary":"  Graph learning is widely recognized as a crucial technique in multi-view\nclustering. Existing graph learning methods typically involve constructing an\nadaptive neighbor graph based on probabilistic neighbors and then learning a\nconsensus graph to for clustering, however, they are confronted with two\nlimitations. Firstly, they often rely on Euclidean distance to measure\nsimilarity when constructing the adaptive neighbor graph, which proves\ninadequate in capturing the intrinsic structure among data points in many\nreal-world scenarios. Secondly, most of these methods focus solely on consensus\ngraph, ignoring view-specific graph information. In response to the\naforementioned drawbacks, we in this paper propose a novel tensor-based graph\nlearning framework that simultaneously considers consistency and specificity\nfor multi-view clustering. Specifically, we calculate the similarity distance\non the Stiefel manifold to preserve the intrinsic structure among data points.\nBy making an assumption that the learned neighbor graph of each view comprises\nboth a consistent graph and a view-specific graph, we formulate a new\ntensor-based target graph learning paradigm. Owing to the benefits of tensor\nsingular value decomposition (t-SVD) in uncovering high-order correlations,\nthis model is capable of achieving a complete understanding of the target\ngraph. Furthermore, we develop an iterative algorithm to solve the proposed\nobjective optimization problem. Experiments conducted on real-world datasets\nhave demonstrated the superior performance of the proposed method over some\nstate-of-the-art multi-view clustering methods. The source code has been\nreleased on https://github.com/lshi91/CSTGL-Code.\n","authors":["Long Shi","Lei Cao","Yunshan Ye","Yu Zhao","Badong Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18379v1","updated":"2024-03-27T09:17:50Z","published":"2024-03-27T09:17:50Z","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining\n  Useful Life Prediction","summary":"  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion\nbatteries is crucial for maintaining the safe and stable operation of\nrechargeable battery management systems. However, this task is often\nchallenging due to the complex temporal dynamics involved. Recently,\nattention-based networks, such as Transformers and Informer, have been the\npopular architecture in time series forecasting. Despite their effectiveness,\nthese models with abundant parameters necessitate substantial training time to\nunravel temporal patterns. To tackle these challenges, we propose a simple\nMLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which\nis an architecture based exclusively on multi-layer perceptrons (MLPs),\nextracting information by mixing operations along both intra-patch and\ninter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer\ncomprises parallel dual-head mixer layers: the intra-patch mixing MLP,\ncapturing local temporal patterns in the short-term period, and the inter-patch\nmixing MLP, capturing global temporal patterns in the long-term period.\nNotably, to address the varying importance of features in RUL prediction, we\nintroduce a weighted loss function in the MLP-Mixer-based architecture, marking\nthe first time such an approach has been employed. Our experiments demonstrate\nthat IIP-Mixer achieves competitive performance in battery RUL prediction,\noutperforming other popular time-series frameworks\n","authors":["Guangzai Ye","Li Feng","Jianlan Guo","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18375v1","updated":"2024-03-27T09:14:36Z","published":"2024-03-27T09:14:36Z","title":"Stragglers-Aware Low-Latency Synchronous Federated Learning via\n  Layer-Wise Model Updates","summary":"  Synchronous federated learning (FL) is a popular paradigm for collaborative\nedge learning. It typically involves a set of heterogeneous devices locally\ntraining neural network (NN) models in parallel with periodic centralized\naggregations. As some of the devices may have limited computational resources\nand varying availability, FL latency is highly sensitive to stragglers.\nConventional approaches discard incomplete intra-model updates done by\nstragglers, alter the amount of local workload and architecture, or resort to\nasynchronous settings; which all affect the trained model performance under\ntight training latency constraints. In this work, we propose straggler-aware\nlayer-wise federated learning (SALF) that leverages the optimization procedure\nof NNs via backpropagation to update the global model in a layer-wise fashion.\nSALF allows stragglers to synchronously convey partial gradients, having each\nlayer of the global model be updated independently with a different\ncontributing set of users. We provide a theoretical analysis, establishing\nconvergence guarantees for the global model under mild assumptions on the\ndistribution of the participating devices, revealing that SALF converges at the\nsame asymptotic rate as FL with no timing limitations. This insight is matched\nwith empirical observations, demonstrating the performance gains of SALF\ncompared to alternative mechanisms mitigating the device heterogeneity gap in\nFL.\n","authors":["Natalie Lang","Alejandro Cohen","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2403.18375v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08533v4","updated":"2024-03-27T09:11:48Z","published":"2023-12-13T21:46:09Z","title":"World Models via Policy-Guided Trajectory Diffusion","summary":"  World models are a powerful tool for developing intelligent agents. By\npredicting the outcome of a sequence of actions, world models enable policies\nto be optimised via on-policy reinforcement learning (RL) using synthetic data,\ni.e. in \"in imagination\". Existing world models are autoregressive in that they\ninterleave predicting the next state with sampling the next action from the\npolicy. Prediction error inevitably compounds as the trajectory length grows.\nIn this work, we propose a novel world modelling approach that is not\nautoregressive and generates entire on-policy trajectories in a single pass\nthrough a diffusion model. Our approach, Policy-Guided Trajectory Diffusion\n(PolyGRAD), leverages a denoising model in addition to the gradient of the\naction distribution of the policy to diffuse a trajectory of initially random\nstates and actions into an on-policy synthetic trajectory. We analyse the\nconnections between PolyGRAD, score-based generative models, and\nclassifier-guided diffusion models. Our results demonstrate that PolyGRAD\noutperforms state-of-the-art baselines in terms of trajectory prediction error\nfor short trajectories, with the exception of autoregressive diffusion. For\nshort trajectories, PolyGRAD obtains similar errors to autoregressive\ndiffusion, but with lower computational requirements. For long trajectories,\nPolyGRAD obtains comparable performance to baselines. Our experiments\ndemonstrate that PolyGRAD enables performant policies to be trained via\non-policy RL in imagination for MuJoCo continuous control domains. Thus,\nPolyGRAD introduces a new paradigm for accurate on-policy world modelling\nwithout autoregressive sampling.\n","authors":["Marc Rigter","Jun Yamada","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2312.08533v4.pdf","comment":"Published in TMLR, March 2024"},{"id":"http://arxiv.org/abs/2102.12920v5","updated":"2024-03-27T09:07:29Z","published":"2021-02-25T15:18:13Z","title":"Emerging Trends in Federated Learning: From Model Fusion to Federated X\n  Learning","summary":"  Federated learning is a new learning paradigm that decouples data collection\nand model training via multi-party computation and model aggregation. As a\nflexible learning setting, federated learning has the potential to integrate\nwith other learning frameworks. We conduct a focused survey of federated\nlearning in conjunction with other learning algorithms. Specifically, we\nexplore various learning algorithms to improve the vanilla federated averaging\nalgorithm and review model fusion methods such as adaptive aggregation,\nregularization, clustered methods, and Bayesian methods. Following the emerging\ntrends, we also discuss federated learning in the intersection with other\nlearning paradigms, termed federated X learning, where X includes multitask\nlearning, meta-learning, transfer learning, unsupervised learning, and\nreinforcement learning. In addition to reviewing state-of-the-art studies, this\npaper also identifies key challenges and applications in this field, while also\nhighlighting promising future directions.\n","authors":["Shaoxiong Ji","Yue Tan","Teemu Saravirta","Zhiqin Yang","Yixin Liu","Lauri Vasankari","Shirui Pan","Guodong Long","Anwar Walid"],"pdf_url":"https://arxiv.org/pdf/2102.12920v5.pdf","comment":"To appear in the International Journal of Machine Learning and\n  Cybernetics"},{"id":"http://arxiv.org/abs/2403.17905v2","updated":"2024-03-27T09:07:02Z","published":"2024-03-26T17:45:06Z","title":"Scalable Non-Cartesian Magnetic Resonance Imaging with R2D2","summary":"  We propose a new approach for non-Cartesian magnetic resonance image\nreconstruction. While unrolled architectures provide robustness via\ndata-consistency layers, embedding measurement operators in Deep Neural Network\n(DNN) can become impractical at large scale. Alternative Plug-and-Play (PnP)\napproaches, where the denoising DNNs are blind to the measurement setting, are\nnot affected by this limitation and have also proven effective, but their\nhighly iterative nature also affects scalability. To address this scalability\nchallenge, we leverage the \"Residual-to-Residual DNN series for high-Dynamic\nrange imaging (R2D2)\" approach recently introduced in astronomical imaging.\nR2D2's reconstruction is formed as a series of residual images, iteratively\nestimated as outputs of DNNs taking the previous iteration's image estimate and\nassociated data residual as inputs. The method can be interpreted as a learned\nversion of the Matching Pursuit algorithm. We demonstrate R2D2 in simulation,\nconsidering radial k-space sampling acquisition sequences. Our preliminary\nresults suggest that R2D2 achieves: (i) suboptimal performance compared to its\nunrolled incarnation R2D2-Net, which is however non-scalable due to the\nnecessary embedding of NUFFT-based data-consistency layers; (ii) superior\nreconstruction quality to a scalable version of R2D2-Net embedding an FFT-based\napproximation for data consistency; (iii) superior reconstruction quality to\nPnP, while only requiring few iterations.\n","authors":["Yiwei Chen","Chao Tang","Amir Aghabiglou","Chung San Chu","Yves Wiaux"],"pdf_url":"https://arxiv.org/pdf/2403.17905v2.pdf","comment":"submitted to IEEE EUSIPCO 2024"},{"id":"http://arxiv.org/abs/2403.18370v1","updated":"2024-03-27T09:06:36Z","published":"2024-03-27T09:06:36Z","title":"Ship in Sight: Diffusion Models for Ship-Image Super Resolution","summary":"  In recent years, remarkable advancements have been achieved in the field of\nimage generation, primarily driven by the escalating demand for high-quality\noutcomes across various image generation subtasks, such as inpainting,\ndenoising, and super resolution. A major effort is devoted to exploring the\napplication of super-resolution techniques to enhance the quality of\nlow-resolution images. In this context, our method explores in depth the\nproblem of ship image super resolution, which is crucial for coastal and port\nsurveillance. We investigate the opportunity given by the growing interest in\ntext-to-image diffusion models, taking advantage of the prior knowledge that\nsuch foundation models have already learned. In particular, we present a\ndiffusion-model-based architecture that leverages text conditioning during\ntraining while being class-aware, to best preserve the crucial details of the\nships during the generation of the super-resoluted image. Since the specificity\nof this task and the scarcity availability of off-the-shelf data, we also\nintroduce a large labeled ship dataset scraped from online ship images, mostly\nfrom ShipSpotting\\footnote{\\url{www.shipspotting.com}} website. Our method\nachieves more robust results than other deep learning models previously\nemployed for super resolution, as proven by the multiple experiments performed.\nMoreover, we investigate how this model can benefit downstream tasks, such as\nclassification and object detection, thus emphasizing practical implementation\nin a real-world scenario. Experimental results show flexibility, reliability,\nand impressive performance of the proposed framework over state-of-the-art\nmethods for different tasks. The code is available at:\nhttps://github.com/LuigiSigillo/ShipinSight .\n","authors":["Luigi Sigillo","Riccardo Fosco Gramaccioni","Alessandro Nicolosi","Danilo Comminiello"],"pdf_url":"https://arxiv.org/pdf/2403.18370v1.pdf","comment":"Accepted at 2024 International Joint Conference on Neural Networks\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2307.13352v2","updated":"2024-03-27T09:04:04Z","published":"2023-07-25T09:14:45Z","title":"High Dimensional Distributed Gradient Descent with Arbitrary Number of\n  Byzantine Attackers","summary":"  Robust distributed learning with Byzantine failures has attracted extensive\nresearch interests in recent years. However, most of existing methods suffer\nfrom curse of dimensionality, which is increasingly serious with the growing\ncomplexity of modern machine learning models. In this paper, we design a new\nmethod that is suitable for high dimensional problems, under arbitrary number\nof Byzantine attackers. The core of our design is a direct high dimensional\nsemi-verified mean estimation method. Our idea is to identify a subspace first.\nThe components of mean value perpendicular to this subspace can be estimated\nvia gradient vectors uploaded from worker machines, while the components within\nthis subspace are estimated using auxiliary dataset. We then use our new method\nas the aggregator of distributed learning problems. Our theoretical analysis\nshows that the new method has minimax optimal statistical rates. In particular,\nthe dependence on dimensionality is significantly improved compared with\nprevious works.\n","authors":["Puning Zhao","Zhiguo Wan"],"pdf_url":"https://arxiv.org/pdf/2307.13352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10158v2","updated":"2024-03-27T08:57:20Z","published":"2024-03-15T10:01:19Z","title":"Functional Graph Convolutional Networks: A unified multi-task and\n  multi-modal learning framework to facilitate health and social-care insights","summary":"  This paper introduces a novel Functional Graph Convolutional Network (funGCN)\nframework that combines Functional Data Analysis and Graph Convolutional\nNetworks to address the complexities of multi-task and multi-modal learning in\ndigital health and longitudinal studies. With the growing importance of health\nsolutions to improve health care and social support, ensure healthy lives, and\npromote well-being at all ages, funGCN offers a unified approach to handle\nmultivariate longitudinal data for multiple entities and ensures\ninterpretability even with small sample sizes. Key innovations include\ntask-specific embedding components that manage different data types, the\nability to perform classification, regression, and forecasting, and the\ncreation of a knowledge graph for insightful data interpretation. The efficacy\nof funGCN is validated through simulation experiments and a real-data\napplication.\n","authors":["Tobia Boschi","Francesca Bonin","Rodrigo Ordonez-Hurtado","Cécile Rousseau","Alessandra Pascale","John Dinsmore"],"pdf_url":"https://arxiv.org/pdf/2403.10158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18364v1","updated":"2024-03-27T08:57:15Z","published":"2024-03-27T08:57:15Z","title":"Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR","summary":"  We investigate the problem of supporting Industrial Internet of Things user\nequipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and\nrandom traffic arrival. A deep reinforcement learning (DRL) based centralized\ndynamic scheduler for time-frequency resources is proposed to learn how to\nschedule the available communication resources among the IIoT UEs. The proposed\nscheduler leverages an RL framework to adapt to the dynamic changes in the\nwireless communication system and traffic arrivals. Moreover, a graph-based\nreduction scheme is proposed to reduce the state and action space of the RL\nframework to allow fast convergence and a better learning strategy. Simulation\nresults demonstrate the effectiveness of the proposed intelligent scheduler in\nguaranteeing the expressed intent of IIoT UEs compared to several traditional\nscheduling schemes, such as round-robin, semi-static, and heuristic approaches.\nThe proposed scheduler also outperforms the contention-free and\ncontention-based schemes in maximizing the number of successfully computed\ntasks.\n","authors":["Salwa Mostafa","Mateus P. Mota","Alvaro Valcarce","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2403.18364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03325v2","updated":"2024-03-27T08:54:35Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Causal Transition and Symbolic Reasoning for\n  Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories, unseen object categories, and real-world data.\nFurther details of this work are provided at\nhttps://fqyqc.github.io/ConTranPlan/.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Yao Su","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15837v2","updated":"2024-03-27T08:54:06Z","published":"2024-03-23T13:24:31Z","title":"Centered Masking for Language-Image Pre-Training","summary":"  We introduce Gaussian masking for Language-Image Pre-Training (GLIP) a novel,\nstraightforward, and effective technique for masking image patches during\npre-training of a vision-language model. GLIP builds on Fast Language-Image\nPre-Training (FLIP), which randomly masks image patches while training a CLIP\nmodel. GLIP replaces random masking with centered masking, that uses a Gaussian\ndistribution and is inspired by the importance of image patches at the center\nof the image. GLIP retains the same computational savings as FLIP, while\nimproving performance across a range of downstream datasets and tasks, as\ndemonstrated by our experimental results. We show the benefits of GLIP to be\neasy to obtain, requiring no delicate tuning of the Gaussian, and also\napplicable to data sets containing images without an obvious center focus.\n","authors":["Mingliang Liang","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2403.15837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17767v2","updated":"2024-03-27T08:49:19Z","published":"2024-03-26T14:54:35Z","title":"Asymptotic Bayes risk of semi-supervised learning with uncertain\n  labeling","summary":"  This article considers a semi-supervised classification setting on a Gaussian\nmixture model, where the data is not labeled strictly as usual, but instead\nwith uncertain labels. Our main aim is to compute the Bayes risk for this\nmodel. We compare the behavior of the Bayes risk and the best known algorithm\nfor this model. This comparison eventually gives new insights over the\nalgorithm.\n","authors":["Victor Leger","Romain Couillet"],"pdf_url":"https://arxiv.org/pdf/2403.17767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18355v1","updated":"2024-03-27T08:48:16Z","published":"2024-03-27T08:48:16Z","title":"Supervised Multiple Kernel Learning approaches for multi-omics data\n  integration","summary":"  Advances in high-throughput technologies have originated an ever-increasing\navailability of omics datasets. The integration of multiple heterogeneous data\nsources is currently an issue for biology and bioinformatics. Multiple kernel\nlearning (MKL) has shown to be a flexible and valid approach to consider the\ndiverse nature of multi-omics inputs, despite being an underused tool in\ngenomic data mining.We provide novel MKL approaches based on different kernel\nfusion strategies.To learn from the meta-kernel of input kernels, we\nadaptedunsupervised integration algorithms for supervised tasks with support\nvector machines.We also tested deep learning architectures for kernel fusion\nand classification.The results show that MKL-based models can compete with more\ncomplex, state-of-the-art, supervised multi-omics integrative approaches.\nMultiple kernel learning offers a natural framework for predictive models in\nmulti-omics genomic data. Our results offer a direction for bio-data mining\nresearch and further development of methods for heterogeneous data integration.\n","authors":["Mitja Briscik","Gabriele Tazza","Marie-Agnes Dillies","László Vidács","Sébastien Dejean"],"pdf_url":"https://arxiv.org/pdf/2403.18355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18347v1","updated":"2024-03-27T08:38:56Z","published":"2024-03-27T08:38:56Z","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal\n  Holes","summary":"  The detection and analysis of the solar coronal holes (CHs) is an important\nfield of study in the domain of solar physics. Mainly, it is required for the\nproper prediction of the geomagnetic storms which directly or indirectly affect\nvarious space and ground-based systems. For the detection of CHs till date, the\nsolar scientist depends on manual hand-drawn approaches. However, with the\nadvancement of image processing technologies, some automated image segmentation\nmethods have been used for the detection of CHs. In-spite of this, fast and\naccurate detection of CHs are till a major issues. Here in this work, a novel\nquantum computing-based fast fuzzy c-mean technique has been developed for fast\ndetection of the CHs region. The task has been carried out in two stages, in\nfirst stage the solar image has been segmented using a quantum computing based\nfast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted\nout from the segmented image based on image morphological operation. In the\nwork, quantum computing has been used to optimize the cost function of the fast\nfuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm\n(QAOA) has been used to optimize the quadratic part of the cost function. The\nproposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image\ndatasets and has been compared with the existing techniques. The outcome shows\nthe comparable performance of the proposed method with the existing one within\na very lesser time.\n","authors":["Sanmoy Bandyopadhyay","Suman Kundu"],"pdf_url":"https://arxiv.org/pdf/2403.18347v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18343v1","updated":"2024-03-27T08:34:39Z","published":"2024-03-27T08:34:39Z","title":"The Artificial Neural Twin -- Process Optimization and Continual\n  Learning in Distributed Process Chains","summary":"  Industrial process optimization and control is crucial to increase economic\nand ecologic efficiency. However, data sovereignty, differing goals, or the\nrequired expert knowledge for implementation impede holistic implementation.\nFurther, the increasing use of data-driven AI-methods in process models and\nindustrial sensory often requires regular fine-tuning to accommodate\ndistribution drifts. We propose the Artificial Neural Twin, which combines\nconcepts from model predictive control, deep learning, and sensor networks to\naddress these issues. Our approach introduces differentiable data fusion to\nestimate the state of distributed process steps and their dependence on input\ndata. By treating the interconnected process steps as a quasi neural-network,\nwe can backpropagate loss gradients for process optimization or model\nfine-tuning to process parameters or AI models respectively. The concept is\ndemonstrated on a virtual machine park simulated in Unity, consisting of bulk\nmaterial processes in plastic recycling.\n","authors":["Johannes Emmert","Ronald Mendez","Houman Mirzaalian Dastjerdi","Christopher Syben","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2403.18343v1.pdf","comment":"20 pages, 11 figures"},{"id":"http://arxiv.org/abs/2403.18337v1","updated":"2024-03-27T08:21:41Z","published":"2024-03-27T08:21:41Z","title":"Macroscale fracture surface segmentation via semi-supervised learning\n  considering the structural similarity","summary":"  To this date the safety assessment of materials, used for example in the\nnuclear power sector, commonly relies on a fracture mechanical analysis\nutilizing macroscopic concepts, where a global load quantity K or J is compared\nto the materials fracture toughness curve. Part of the experimental effort\ninvolved in these concepts is dedicated to the quantitative analysis of\nfracture surfaces. Within the scope of this study a methodology for the\nsemi-supervised training of deep learning models for fracture surface\nsegmentation on a macroscopic level was established. Therefore, three distinct\nand unique datasets were created to analyze the influence of structural\nsimilarity on the segmentation capability. The structural similarity differs\ndue to the assessed materials and specimen, as well as imaging-induced variance\ndue to fluctuations in image acquisition in different laboratories. The\ndatasets correspond to typical isolated laboratory conditions, complex\nreal-world circumstances, and a curated subset of the two. We implemented a\nweak-to-strong consistency regularization for semi-supervised learning. On the\nheterogeneous dataset we were able to train robust and well-generalizing models\nthat learned feature representations from images across different domains\nwithout observing a significant drop in prediction quality. Furthermore, our\napproach reduced the number of labeled images required for training by a factor\nof 6. To demonstrate the success of our method and the benefit of our approach\nfor the fracture mechanics assessment, we utilized the models for initial crack\nsize measurements with the area average method. For the laboratory setting, the\ndeep learning assisted measurements proved to have the same quality as manual\nmeasurements. For models trained on the heterogeneous dataset, very good\nmeasurement accuracies with mean deviations smaller than 1 % could be\nachieved...\n","authors":["Johannes Rosenberger","Johannes Tlatlik","Sebastian Münstermann"],"pdf_url":"https://arxiv.org/pdf/2403.18337v1.pdf","comment":"During review title changed to: Deep learning based initial crack\n  size measurements utilizing macroscale fracture surface segmentation"},{"id":"http://arxiv.org/abs/2403.18336v1","updated":"2024-03-27T08:21:01Z","published":"2024-03-27T08:21:01Z","title":"A Dataset for Pharmacovigilance in German, French, and Japanese:\n  Annotating Adverse Drug Reactions across Languages","summary":"  User-generated data sources have gained significance in uncovering Adverse\nDrug Reactions (ADRs), with an increasing number of discussions occurring in\nthe digital world. However, the existing clinical corpora predominantly revolve\naround scientific articles in English. This work presents a multilingual corpus\nof texts concerning ADRs gathered from diverse sources, including patient fora,\nsocial media, and clinical reports in German, French, and Japanese. Our corpus\ncontains annotations covering 12 entity types, four attribute types, and 13\nrelation types. It contributes to the development of real-world multilingual\nlanguage models for healthcare. We provide statistics to highlight certain\nchallenges associated with the corpus and conduct preliminary experiments\nresulting in strong baselines for extracting entities and relations between\nthese entities, both within and across languages.\n","authors":["Lisa Raithel","Hui-Syuan Yeh","Shuntaro Yada","Cyril Grouin","Thomas Lavergne","Aurélie Névéol","Patrick Paroubek","Philippe Thomas","Tomohiro Nishiyama","Sebastian Möller","Eiji Aramaki","Yuji Matsumoto","Roland Roller","Pierre Zweigenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.18336v1.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2403.18330v1","updated":"2024-03-27T08:11:25Z","published":"2024-03-27T08:11:25Z","title":"Tracking-Assisted Object Detection with Event Cameras","summary":"  Event-based object detection has recently garnered attention in the computer\nvision community due to the exceptional properties of event cameras, such as\nhigh dynamic range and no motion blur. However, feature asynchronism and\nsparsity cause invisible objects due to no relative motion to the camera,\nposing a significant challenge in the task. Prior works have studied various\nmemory mechanisms to preserve as many features as possible at the current time,\nguided by temporal clues. While these implicit-learned memories retain some\nshort-term information, they still struggle to preserve long-term features\neffectively. In this paper, we consider those invisible objects as\npseudo-occluded objects and aim to reveal their features. Firstly, we introduce\nvisibility attribute of objects and contribute an auto-labeling algorithm to\nappend additional visibility labels on an existing event camera dataset.\nSecondly, we exploit tracking strategies for pseudo-occluded objects to\nmaintain their permanence and retain their bounding boxes, even when features\nhave not been available for a very long time. These strategies can be treated\nas an explicit-learned memory guided by the tracking objective to record the\ndisplacements of objects across frames. Lastly, we propose a spatio-temporal\nfeature aggregation module to enrich the latent features and a consistency loss\nto increase the robustness of the overall pipeline. We conduct comprehensive\nexperiments to verify our method's effectiveness where still objects are\nretained but real occluded objects are discarded. The results demonstrate that\n(1) the additional visibility labels can assist in supervised training, and (2)\nour method outperforms state-of-the-art approaches with a significant\nimprovement of 7.9% absolute mAP.\n","authors":["Ting-Kang Yen","Igor Morawski","Shusil Dangi","Kai He","Chung-Yi Lin","Jia-Fong Yeh","Hung-Ting Su","Winston Hsu"],"pdf_url":"https://arxiv.org/pdf/2403.18330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18326v1","updated":"2024-03-27T08:07:07Z","published":"2024-03-27T08:07:07Z","title":"Privacy-Preserving Distributed Nonnegative Matrix Factorization","summary":"  Nonnegative matrix factorization (NMF) is an effective data representation\ntool with numerous applications in signal processing and machine learning.\nHowever, deploying NMF in a decentralized manner over ad-hoc networks\nintroduces privacy concerns due to the conventional approach of sharing raw\ndata among network agents. To address this, we propose a privacy-preserving\nalgorithm for fully-distributed NMF that decomposes a distributed large data\nmatrix into left and right matrix factors while safeguarding each agent's local\ndata privacy. It facilitates collaborative estimation of the left matrix factor\namong agents and enables them to estimate their respective right factors\nwithout exposing raw data. To ensure data privacy, we secure information\nexchanges between neighboring agents utilizing the Paillier cryptosystem, a\nprobabilistic asymmetric algorithm for public-key cryptography that allows\ncomputations on encrypted data without decryption. Simulation results conducted\non synthetic and real-world datasets demonstrate the effectiveness of the\nproposed algorithm in achieving privacy-preserving distributed NMF over ad-hoc\nnetworks.\n","authors":["Ehsan Lari","Reza Arablouei","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2403.18326v1.pdf","comment":"5 pages, 1 figure, submitted to EUSIPCO 2024 conference"},{"id":"http://arxiv.org/abs/2403.18322v1","updated":"2024-03-27T07:52:10Z","published":"2024-03-27T07:52:10Z","title":"Quantum Algorithms: A New Frontier in Financial Crime Prevention","summary":"  Financial crimes fast proliferation and sophistication require novel\napproaches that provide robust and effective solutions. This paper explores the\npotential of quantum algorithms in combating financial crimes. It highlights\nthe advantages of quantum computing by examining traditional and Machine\nLearning (ML) techniques alongside quantum approaches. The study showcases\nadvanced methodologies such as Quantum Machine Learning (QML) and Quantum\nArtificial Intelligence (QAI) as powerful solutions for detecting and\npreventing financial crimes, including money laundering, financial crime\ndetection, cryptocurrency attacks, and market manipulation. These quantum\napproaches leverage the inherent computational capabilities of quantum\ncomputers to overcome limitations faced by classical methods. Furthermore, the\npaper illustrates how quantum computing can support enhanced financial risk\nmanagement analysis. Financial institutions can improve their ability to\nidentify and mitigate risks, leading to more robust risk management strategies\nby exploiting the quantum advantage. This research underscores the\ntransformative impact of quantum algorithms on financial risk management. By\nembracing quantum technologies, organisations can enhance their capabilities to\ncombat evolving threats and ensure the integrity and stability of financial\nsystems.\n","authors":["Abraham Itzhak Weinberg","Alessio Faccia"],"pdf_url":"https://arxiv.org/pdf/2403.18322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18321v1","updated":"2024-03-27T07:50:45Z","published":"2024-03-27T07:50:45Z","title":"Implementation of the Principal Component Analysis onto High-Performance\n  Computer Facilities for Hyperspectral Dimensionality Reduction: Results and\n  Comparisons","summary":"  Dimensionality reduction represents a critical preprocessing step in order to\nincrease the efficiency and the performance of many hyperspectral imaging\nalgorithms. However, dimensionality reduction algorithms, such as the Principal\nComponent Analysis (PCA), suffer from their computationally demanding nature,\nbecoming advisable for their implementation onto high-performance computer\narchitectures for applications under strict latency constraints. This work\npresents the implementation of the PCA algorithm onto two different\nhigh-performance devices, namely, an NVIDIA Graphics Processing Unit (GPU) and\na Kalray manycore, uncovering a highly valuable set of tips and tricks in order\nto take full advantage of the inherent parallelism of these high-performance\ncomputing platforms, and hence, reducing the time that is required to process a\ngiven hyperspectral image. Moreover, the achieved results obtained with\ndifferent hyperspectral images have been compared with the ones that were\nobtained with a field programmable gate array (FPGA)-based implementation of\nthe PCA algorithm that has been recently published, providing, for the first\ntime in the literature, a comprehensive analysis in order to highlight the pros\nand cons of each option.\n","authors":["E. Martel","R. Lazcano","J. Lopez","D. Madroñal","R. Salvador","S. Lopez","E. Juarez","R. Guerra","C. Sanz","R. Sarmiento"],"pdf_url":"https://arxiv.org/pdf/2403.18321v1.pdf","comment":"30 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.18316v1","updated":"2024-03-27T07:38:36Z","published":"2024-03-27T07:38:36Z","title":"Multi-Modal Contrastive Learning for Online Clinical Time-Series\n  Applications","summary":"  Electronic Health Record (EHR) datasets from Intensive Care Units (ICU)\ncontain a diverse set of data modalities. While prior works have successfully\nleveraged multiple modalities in supervised settings, we apply advanced\nself-supervised multi-modal contrastive learning techniques to ICU data,\nspecifically focusing on clinical notes and time-series for clinically relevant\nonline prediction tasks. We introduce a loss function Multi-Modal Neighborhood\nContrastive Loss (MM-NCL), a soft neighborhood function, and showcase the\nexcellent linear probe and zero-shot performance of our approach.\n","authors":["Fabian Baldenweg","Manuel Burger","Gunnar Rätsch","Rita Kuznetsova"],"pdf_url":"https://arxiv.org/pdf/2403.18316v1.pdf","comment":"Accepted as a Workshop Paper at TS4H@ICLR2024"},{"id":"http://arxiv.org/abs/2403.12820v2","updated":"2024-03-27T07:35:47Z","published":"2024-03-19T15:21:00Z","title":"A Physics-embedded Deep Learning Framework for Cloth Simulation","summary":"  Delicate cloth simulations have long been desired in computer graphics.\nVarious methods were proposed to improve engaged force interactions, collision\nhandling, and numerical integrations. Deep learning has the potential to\nachieve fast and real-time simulation, but common neural network structures\noften demand many parameters to capture cloth dynamics. This paper proposes a\nphysics-embedded learning framework that directly encodes physical features of\ncloth simulation. The convolutional neural network is used to represent spatial\ncorrelations of the mass-spring system, after which three branches are designed\nto learn linear, nonlinear, and time derivate features of cloth physics. The\nframework can also integrate with other external forces and collision handling\nthrough either traditional simulators or sub neural networks. The model is\ntested across different cloth animation cases, without training with new data.\nAgreement with baselines and predictive realism successfully validate its\ngeneralization ability. Inference efficiency of the proposed model also defeats\ntraditional physics simulation. This framework is also designed to easily\nintegrate with other visual refinement techniques like wrinkle carving, which\nleaves significant chances to incorporate prevailing macing learning techniques\nin 3D cloth amination.\n","authors":["Zhiwei Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.12820v2.pdf","comment":"A derivation is incomplete, and updations are being processed"},{"id":"http://arxiv.org/abs/2403.18310v1","updated":"2024-03-27T07:22:32Z","published":"2024-03-27T07:22:32Z","title":"A thermodynamically consistent physics-informed deep learning material\n  model for short fiber/polymer nanocomposites","summary":"  This work proposes a physics-informed deep learning (PIDL)-based constitutive\nmodel for investigating the viscoelastic-viscoplastic behavior of short\nfiber-reinforced nanoparticle-filled epoxies under various ambient conditions.\nThe deep-learning model is trained to enforce thermodynamic principles, leading\nto a thermodynamically consistent constitutive model. To accomplish this, a\nlong short-term memory network is combined with a feed-forward neural network\nto predict internal variables required for characterizing the internal\ndissipation of the nanocomposite materials. In addition, another feed-forward\nneural network is used to indicate the free-energy function, which enables\ndefining the thermodynamic state of the entire system. The PIDL model is\ninitially developed for the three-dimensional case by generating synthetic data\nfrom a classical constitutive model. The model is then trained by extracting\nthe data directly from cyclic loading-unloading experimental tests. Numerical\nexamples show that the PIDL model can accurately predict the mechanical\nbehavior of epoxy-based nanocomposites for different volume fractions of fibers\nand nanoparticles under various hygrothermal conditions.\n","authors":["Betim Bahtiri","Behrouz Arash","Sven Scheffler","Maximilian Jux","Raimund Rolfes"],"pdf_url":"https://arxiv.org/pdf/2403.18310v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08102"},{"id":"http://arxiv.org/abs/2310.17072v3","updated":"2024-03-27T07:04:58Z","published":"2023-10-26T00:28:37Z","title":"MMP++: Motion Manifold Primitives with Parametric Curve Models","summary":"  Motion Manifold Primitives (MMP), a manifold-based approach for encoding\nbasic motion skills, can produce diverse trajectories, enabling the system to\nadapt to unseen constraints. Nonetheless, we argue that current MMP models lack\ncrucial functionalities of movement primitives, such as temporal and via-points\nmodulation, found in traditional approaches. This shortfall primarily stems\nfrom MMP's reliance on discrete-time trajectories. To overcome these\nlimitations, we introduce Motion Manifold Primitives++ (MMP++), a new model\nthat integrates the strengths of both MMP and traditional methods by\nincorporating parametric curve representations into the MMP framework.\nFurthermore, we identify a significant challenge with MMP++: performance\ndegradation due to geometric distortions in the latent space, meaning that\nsimilar motions are not closely positioned. To address this, Isometric Motion\nManifold Primitives++ (IMMP++) is proposed to ensure the latent space\naccurately preserves the manifold's geometry. Our experimental results across\nvarious applications, including 2-DoF planar motions, 7-DoF robot arm motions,\nand SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing\nmethods in trajectory generation tasks, achieving substantial improvements in\nsome cases. Moreover, they enable the modulation of latent coordinates and\nvia-points, thereby allowing efficient online adaptation to dynamic\nenvironments.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2310.17072v3.pdf","comment":"12 pages. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2403.18302v1","updated":"2024-03-27T06:58:01Z","published":"2024-03-27T06:58:01Z","title":"Super-Resolution of SOHO/MDI Magnetograms of Solar Active Regions Using\n  SDO/HMI Data and an Attention-Aided Convolutional Neural Network","summary":"  Image super-resolution has been an important subject in image processing and\nrecognition. Here, we present an attention-aided convolutional neural network\n(CNN) for solar image super-resolution. Our method, named SolarCNN, aims to\nenhance the quality of line-of-sight (LOS) magnetograms of solar active regions\n(ARs) collected by the Michelson Doppler Imager (MDI) on board the Solar and\nHeliospheric Observatory (SOHO). The ground-truth labels used for training\nSolarCNN are the LOS magnetograms collected by the Helioseismic and Magnetic\nImager (HMI) on board the Solar Dynamics Observatory (SDO). Solar ARs consist\nof strong magnetic fields in which magnetic energy can suddenly be released to\nproduce extreme space weather events, such as solar flares, coronal mass\nejections, and solar energetic particles. SOHO/MDI covers Solar Cycle 23, which\nis stronger with more eruptive events than Cycle 24. Enhanced SOHO/MDI\nmagnetograms allow for better understanding and forecasting of violent events\nof space weather. Experimental results show that SolarCNN improves the quality\nof SOHO/MDI magnetograms in terms of the structural similarity index measure\n(SSIM), Pearson's correlation coefficient (PCC), and the peak signal-to-noise\nratio (PSNR).\n","authors":["Chunhui Xu","Jason T. L. Wang","Haimin Wang","Haodi Jiang","Qin Li","Yasser Abduallah","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18302v1.pdf","comment":"17 pages, 7 figures"},{"id":"http://arxiv.org/abs/2302.06912v4","updated":"2024-03-27T06:57:30Z","published":"2023-02-14T08:56:50Z","title":"Regret-Based Defense in Adversarial Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable\nto small adversarial noise in observations. Such adversarial noise can have\ndisastrous consequences in safety-critical environments. For instance, a\nself-driving car receiving adversarially perturbed sensory observations about\nnearby signs (e.g., a stop sign physically altered to be perceived as a speed\nlimit sign) or objects (e.g., cars altered to be recognized as trees) can be\nfatal. Existing approaches for making RL algorithms robust to an\nobservation-perturbing adversary have focused on reactive approaches that\niteratively improve against adversarial examples generated at each iteration.\nWhile such approaches have been shown to provide improvements over regular RL\nmethods, they are reactive and can fare significantly worse if certain\ncategories of adversarial examples are not generated during training. To that\nend, we pursue a more proactive approach that relies on directly optimizing a\nwell-studied robustness measure, regret instead of expected value. We provide a\nprincipled approach that minimizes maximum regret over a \"neighborhood\" of\nobservations to the received \"observation\". Our regret criterion can be used to\nmodify existing value- and policy-based Deep RL methods. We demonstrate that\nour approaches provide a significant improvement in performance across a wide\nvariety of benchmarks against leading approaches for robust Deep RL.\n","authors":["Roman Belaire","Pradeep Varakantham","Thanh Nguyen","David Lo"],"pdf_url":"https://arxiv.org/pdf/2302.06912v4.pdf","comment":"Accepted at AAMAS 2024"},{"id":"http://arxiv.org/abs/2403.18301v1","updated":"2024-03-27T06:55:23Z","published":"2024-03-27T06:55:23Z","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","summary":"  The rise in internet usage has led to the generation of massive amounts of\ndata, resulting in the adoption of various supervised and semi-supervised\nmachine learning algorithms, which can effectively utilize the colossal amount\nof data to train models. However, before deploying these models in the real\nworld, these must be strictly evaluated on performance measures like worst-case\nrecall and satisfy constraints such as fairness. We find that current\nstate-of-the-art empirical techniques offer sub-optimal performance on these\npractical, non-decomposable performance objectives. On the other hand, the\ntheoretical techniques necessitate training a new model from scratch for each\nperformance objective. To bridge the gap, we propose SelMix, a selective\nmixup-based inexpensive fine-tuning technique for pre-trained models, to\noptimize for the desired objective. The core idea of our framework is to\ndetermine a sampling distribution to perform a mixup of features between\nsamples from particular classes such that it optimizes the given objective. We\ncomprehensively evaluate our technique against the existing empirical and\ntheoretically principled methods on standard benchmark datasets for imbalanced\nclassification. We find that proposed SelMix fine-tuning significantly improves\nthe performance for various practical non-decomposable objectives across\nbenchmarks.\n","authors":["Shrinivas Ramasubramanian","Harsh Rangwani","Sho Takemori","Kunal Samanta","Yuhei Umeda","Venkatesh Babu Radhakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.18301v1.pdf","comment":"ICLR 2024 SpotLight"},{"id":"http://arxiv.org/abs/2403.18296v1","updated":"2024-03-27T06:46:59Z","published":"2024-03-27T06:46:59Z","title":"GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic\n  Communication Paradigm","summary":"  Traditional approaches to semantic communication tasks rely on the knowledge\nof the signal-to-noise ratio (SNR) to mitigate channel noise. However, these\nmethods necessitate training under specific SNR conditions, entailing\nconsiderable time and computational resources. In this paper, we propose GeNet,\na Graph Neural Network (GNN)-based paradigm for semantic communication aimed at\ncombating noise, thereby facilitating Task-Oriented Communication (TOC). We\npropose a novel approach where we first transform the input data image into\ngraph structures. Then we leverage a GNN-based encoder to extract semantic\ninformation from the source data. This extracted semantic information is then\ntransmitted through the channel. At the receiver's end, a GNN-based decoder is\nutilized to reconstruct the relevant semantic information from the source data\nfor TOC. Through experimental evaluation, we show GeNet's effectiveness in\nanti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's\nperformance by varying the number of nodes, revealing its versatility as a new\nparadigm for semantic communication. Additionally, we show GeNet's robustness\nto geometric transformations by testing it with different rotation angles,\nwithout resorting to data augmentation.\n","authors":["Chunhang Zheng","Kechao Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.18269v1","updated":"2024-03-27T05:50:23Z","published":"2024-03-27T05:50:23Z","title":"Clustering Change Sign Detection by Fusing Mixture Complexity","summary":"  This paper proposes an early detection method for cluster structural changes.\nCluster structure refers to discrete structural characteristics, such as the\nnumber of clusters, when data are represented using finite mixture models, such\nas Gaussian mixture models. We focused on scenarios in which the cluster\nstructure gradually changed over time. For finite mixture models, the concept\nof mixture complexity (MC) measures the continuous cluster size by considering\nthe cluster proportion bias and overlap between clusters. In this paper, we\npropose MC fusion as an extension of MC to handle situations in which multiple\nmixture numbers are possible in a finite mixture model. By incorporating the\nfusion of multiple models, our approach accurately captured the cluster\nstructure during transitional periods of gradual change. Moreover, we introduce\na method for detecting changes in the cluster structure by examining the\ntransition of MC fusion. We demonstrate the effectiveness of our method through\nempirical analysis using both artificial and real-world datasets.\n","authors":["Kento Urano","Ryo Yuki","Kenji Yamanishi"],"pdf_url":"https://arxiv.org/pdf/2403.18269v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2312.12558v2","updated":"2024-03-27T05:48:21Z","published":"2023-12-19T19:53:58Z","title":"Sample Efficient Reinforcement Learning with Partial Dynamics Knowledge","summary":"  The problem of sample complexity of online reinforcement learning is often\nstudied in the literature without taking into account any partial knowledge\nabout the system dynamics that could potentially accelerate the learning\nprocess. In this paper, we study the sample complexity of online Q-learning\nmethods when some prior knowledge about the dynamics is available or can be\nlearned efficiently. We focus on systems that evolve according to an additive\ndisturbance model of the form $S_{h+1} = f(S_h, A_h) + W_h$, where $f$\nrepresents the underlying system dynamics, and $W_h$ are unknown disturbances\nindependent of states and actions. In the setting of finite episodic Markov\ndecision processes with $S$ states, $A$ actions, and episode length $H$, we\npresent an optimistic Q-learning algorithm that achieves\n$\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{T})$ regret under perfect knowledge of\n$f$, where $T$ is the total number of interactions with the system. This is in\ncontrast to the typical $\\tilde{\\mathcal{O}}(\\text{Poly}(H)\\sqrt{SAT})$ regret\nfor existing Q-learning methods. Further, if only a noisy estimate $\\hat{f}$ of\n$f$ is available, our method can learn an approximately optimal policy in a\nnumber of samples that is independent of the cardinalities of state and action\nspaces. The sub-optimality gap depends on the approximation error $\\hat{f}-f$,\nas well as the Lipschitz constant of the corresponding optimal value function.\nOur approach does not require modeling of the transition probabilities and\nenjoys the same memory complexity as model-free methods.\n","authors":["Meshal Alharbi","Mardavij Roozbehani","Munther Dahleh"],"pdf_url":"https://arxiv.org/pdf/2312.12558v2.pdf","comment":"Published in the 38th Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2305.14258v2","updated":"2024-03-27T05:45:37Z","published":"2023-05-23T17:11:33Z","title":"Weakly Supervised AUC Optimization: A Unified Partial AUC Approach","summary":"  Since acquiring perfect supervision is usually difficult, real-world machine\nlearning tasks often confront inaccurate, incomplete, or inexact supervision,\ncollectively referred to as weak supervision. In this work, we present WSAUC, a\nunified framework for weakly supervised AUC optimization problems, which covers\nnoisy label learning, positive-unlabeled learning, multi-instance learning, and\nsemi-supervised learning scenarios. Within the WSAUC framework, we first frame\nthe AUC optimization problems in various weakly supervised scenarios as a\ncommon formulation of minimizing the AUC risk on contaminated sets, and\ndemonstrate that the empirical risk minimization problems are consistent with\nthe true AUC. Then, we introduce a new type of partial AUC, specifically, the\nreversed partial AUC (rpAUC), which serves as a robust training objective for\nAUC maximization in the presence of contaminated labels. WSAUC offers a\nuniversal solution for AUC optimization in various weakly supervised scenarios\nby maximizing the empirical rpAUC. Theoretical and experimental results under\nmultiple settings support the effectiveness of WSAUC on a range of weakly\nsupervised AUC optimization tasks.\n","authors":["Zheng Xie","Yu Liu","Hao-Yuan He","Ming Li","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.14258v2.pdf","comment":"Accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2403.18267v1","updated":"2024-03-27T05:41:50Z","published":"2024-03-27T05:41:50Z","title":"DSF-GAN: DownStream Feedback Generative Adversarial Network","summary":"  Utility and privacy are two crucial measurements of the quality of synthetic\ntabular data. While significant advancements have been made in privacy\nmeasures, generating synthetic samples with high utility remains challenging.\nTo enhance the utility of synthetic samples, we propose a novel architecture\ncalled the DownStream Feedback Generative Adversarial Network (DSF-GAN). This\napproach incorporates feedback from a downstream prediction model during\ntraining to augment the generator's loss function with valuable information.\nThus, DSF-GAN utilizes a downstream prediction task to enhance the utility of\nsynthetic samples. To evaluate our method, we tested it using two popular\ndatasets. Our experiments demonstrate improved model performance when training\non synthetic samples generated by DSF-GAN, compared to those generated by the\nsame GAN architecture without feedback. The evaluation was conducted on the\nsame validation set comprising real samples. All code and datasets used in this\nresearch will be made openly available for ease of reproduction.\n","authors":["Oriel Perets","Nadav Rappoport"],"pdf_url":"https://arxiv.org/pdf/2403.18267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18266v1","updated":"2024-03-27T05:38:48Z","published":"2024-03-27T05:38:48Z","title":"Branch-Tuning: Balancing Stability and Plasticity for Continual\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) has emerged as an effective paradigm for\nderiving general representations from vast amounts of unlabeled data. However,\nas real-world applications continually integrate new content, the high\ncomputational and resource demands of SSL necessitate continual learning rather\nthan complete retraining. This poses a challenge in striking a balance between\nstability and plasticity when adapting to new information. In this paper, we\nemploy Centered Kernel Alignment for quantitatively analyzing model stability\nand plasticity, revealing the critical roles of batch normalization layers for\nstability and convolutional layers for plasticity. Motivated by this, we\npropose Branch-tuning, an efficient and straightforward method that achieves a\nbalance between stability and plasticity in continual SSL. Branch-tuning\nconsists of branch expansion and compression, and can be easily applied to\nvarious SSL methods without the need of modifying the original methods,\nretaining old data or models. We validate our method through incremental\nexperiments on various benchmark datasets, demonstrating its effectiveness and\npractical value in real-world scenarios. We hope our work offers new insights\nfor future continual self-supervised learning research. The code will be made\npublicly available.\n","authors":["Wenzhuo Liu","Fei Zhu","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02561v2","updated":"2024-03-27T05:23:40Z","published":"2024-02-04T16:27:37Z","title":"Foundation Model Makes Clustering A Better Initialization For Cold-Start\n  Active Learning","summary":"  Active learning selects the most informative samples from the unlabelled\ndataset to annotate in the context of a limited annotation budget. While\nnumerous methods have been proposed for subsequent sample selection based on an\ninitialized model, scant attention has been paid to the indispensable phase of\nactive learning: selecting samples for model cold-start initialization. Most of\nthe previous studies resort to random sampling or naive clustering. However,\nrandom sampling is prone to fluctuation, and naive clustering suffers from\nconvergence speed, particularly when dealing with high-dimensional data such as\nimaging data. In this work, we propose to integrate foundation models with\nclustering methods to select samples for cold-start active learning\ninitialization. Foundation models refer to those trained on massive datasets by\nthe self-supervised paradigm and capable of generating informative and\ncompacted embeddings for various downstream tasks. Leveraging these embeddings\nto replace raw features such as pixel values, clustering quickly converges and\nidentifies better initial samples. For a comprehensive comparison, we included\na classic ImageNet-supervised model to acquire embeddings. Experiments on two\nclinical tasks of image classification and segmentation demonstrated that\nfoundation model-based clustering efficiently pinpointed informative initial\nsamples, leading to models showcasing enhanced performance than the baseline\nmethods. We envisage that this study provides an effective paradigm for future\ncold-start active learning.\n","authors":["Han Yuan","Chuan Hong"],"pdf_url":"https://arxiv.org/pdf/2402.02561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17458v2","updated":"2024-03-27T04:54:59Z","published":"2024-03-26T07:46:27Z","title":"Expectations Versus Reality: Evaluating Intrusion Detection Systems in\n  Practice","summary":"  Our paper provides empirical comparisons between recent IDSs to provide an\nobjective comparison between them to help users choose the most appropriate\nsolution based on their requirements. Our results show that no one solution is\nthe best, but is dependent on external variables such as the types of attacks,\ncomplexity, and network environment in the dataset. For example, BoT_IoT and\nStratosphere IoT datasets both capture IoT-related attacks, but the deep neural\nnetwork performed the best when tested using the BoT_IoT dataset while HELAD\nperformed the best when tested using the Stratosphere IoT dataset. So although\nwe found that a deep neural network solution had the highest average F1 scores\non tested datasets, it is not always the best-performing one. We further\ndiscuss difficulties in using IDS from literature and project repositories,\nwhich complicated drawing definitive conclusions regarding IDS selection.\n","authors":["Jake Hesford","Daniel Cheng","Alan Wan","Larry Huynh","Seungho Kim","Hyoungshick Kim","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2403.17458v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2312.07950v3","updated":"2024-03-27T04:51:51Z","published":"2023-12-13T07:56:27Z","title":"CBQ: Cross-Block Quantization for Large Language Models","summary":"  Post-training quantization (PTQ) has played a key role in compressing large\nlanguage models (LLMs) with ultra-low costs. However, existing PTQ methods only\nfocus on handling the outliers within one layer or one block, which ignores the\ndependency of blocks and leads to severe performance degradation in low-bit\nsettings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ\nmethod for LLMs. CBQ employs a cross-block dependency using a homologous\nreconstruction scheme, establishing long-range dependencies across multiple\nblocks to minimize error accumulation. Furthermore, CBQ incorporates a\ncoarse-to-fine preprocessing (CFP) strategy for suppressing weight and\nactivation outliers, coupled with an adaptive LoRA-Rounding technique for\nprecise weight quantization. These innovations enable CBQ to not only handle\nextreme outliers effectively but also improve overall quantization accuracy.\nExtensive experiments show that CBQ achieves superior low-bit quantization\n(W4A4, W4A8, W2A16) and outperforms existing state-of-the-art methods across\nvarious LLMs and datasets. Notably, CBQ quantizes the 4-bit LLAMA1-65B model\nwithin only 4.3 hours on a single GPU, achieving a commendable tradeoff between\nperformance and quantization efficiency.\n","authors":["Xin Ding","Xiaoyu Liu","Zhijun Tu","Yun Zhang","Wei Li","Jie Hu","Hanting Chen","Yehui Tang","Zhiwei Xiong","Baoqun Yin","Yunhe Wang"],"pdf_url":"https://arxiv.org/pdf/2312.07950v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2401.16025v2","updated":"2024-03-27T04:36:17Z","published":"2024-01-29T10:17:54Z","title":"Simple Policy Optimization","summary":"  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent\nperformance in many fields, and it is considered as a simple version of TRPO\n(Trust Region Policy Optimization) algorithm. However, the ratio clipping\noperation in PPO may not always effectively enforce the trust region\nconstraints, this can be a potential factor affecting the stability of the\nalgorithm. In this paper, we propose Simple Policy Optimization (SPO)\nalgorithm, which introduces a novel clipping method for KL divergence between\nthe old and current policies. Extensive experimental results in Atari 2600\nenvironments indicate that, compared to the mainstream variants of PPO, SPO\nachieves better sample efficiency, extremely low KL divergence, and higher\npolicy entropy, and is robust to the increase in network depth or complexity.\nMore importantly, SPO maintains the simplicity of an unconstrained first-order\nalgorithm. Code is available at\nhttps://github.com/MyRepositories-hub/Simple-Policy-Optimization.\n","authors":["Zhengpeng Xie"],"pdf_url":"https://arxiv.org/pdf/2401.16025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18241v1","updated":"2024-03-27T04:09:34Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis.\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08657v6","updated":"2024-03-27T03:53:23Z","published":"2022-06-17T09:42:35Z","title":"BridgeTower: Building Bridges Between Encoders in Vision-Language\n  Representation Learning","summary":"  Vision-Language (VL) models with the Two-Tower architecture have dominated\nvisual-language representation learning in recent years. Current VL models\neither use lightweight uni-modal encoders and learn to extract, align and fuse\nboth modalities simultaneously in a deep cross-modal encoder, or feed the\nlast-layer uni-modal representations from the deep pre-trained uni-modal\nencoders into the top cross-modal encoder. Both approaches potentially restrict\nvision-language representation learning and limit model performance. In this\npaper, we propose BridgeTower, which introduces multiple bridge layers that\nbuild a connection between the top layers of uni-modal encoders and each layer\nof the cross-modal encoder. This enables effective bottom-up cross-modal\nalignment and fusion between visual and textual representations of different\nsemantic levels of pre-trained uni-modal encoders in the cross-modal encoder.\nPre-trained with only 4M images, BridgeTower achieves state-of-the-art\nperformance on various downstream vision-language tasks. In particular, on the\nVQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming\nthe previous state-of-the-art model METER by 1.09% with the same pre-training\ndata and almost negligible additional parameters and computational costs.\nNotably, when further scaling the model, BridgeTower achieves an accuracy of\n81.15%, surpassing models that are pre-trained on orders-of-magnitude larger\ndatasets. Code and checkpoints are available at\nhttps://github.com/microsoft/BridgeTower.\n","authors":["Xiao Xu","Chenfei Wu","Shachar Rosenman","Vasudev Lal","Wanxiang Che","Nan Duan"],"pdf_url":"https://arxiv.org/pdf/2206.08657v6.pdf","comment":"Accepted by AAAI 2023, Oral"},{"id":"http://arxiv.org/abs/2301.11104v4","updated":"2024-03-27T03:47:20Z","published":"2023-01-26T13:58:46Z","title":"Discovering and Mitigating Visual Biases through Keyword Explanation","summary":"  Addressing biases in computer vision models is crucial for real-world AI\ndeployments. However, mitigating visual biases is challenging due to their\nunexplainable nature, often identified indirectly through visualization or\nsample statistics, which necessitates additional human supervision for\ninterpretation. To tackle this issue, we propose the Bias-to-Text (B2T)\nframework, which interprets visual biases as keywords. Specifically, we extract\ncommon keywords from the captions of mispredicted images to identify potential\nbiases in the model. We then validate these keywords by measuring their\nsimilarity to the mispredicted images using a vision-language scoring model.\nThe keyword explanation form of visual bias offers several advantages, such as\na clear group naming for bias discovery and a natural extension for debiasing\nusing these group names. Our experiments demonstrate that B2T can identify\nknown biases, such as gender bias in CelebA, background bias in Waterbirds, and\ndistribution shifts in ImageNet-R/C. Additionally, B2T uncovers novel biases in\nlarger datasets, such as Dollar Street and ImageNet. For example, we discovered\na contextual bias between \"bee\" and \"flower\" in ImageNet. We also highlight\nvarious applications of B2T keywords, including debiased training, CLIP\nprompting, and model comparison.\n","authors":["Younghyun Kim","Sangwoo Mo","Minkyu Kim","Kyungmin Lee","Jaeho Lee","Jinwoo Shin"],"pdf_url":"https://arxiv.org/pdf/2301.11104v4.pdf","comment":"CVPR 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18233v1","updated":"2024-03-27T03:39:57Z","published":"2024-03-27T03:39:57Z","title":"Benchmarking Image Transformers for Prostate Cancer Detection from\n  Ultrasound Data","summary":"  PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in\nultrasound images typically employ convolutional networks (CNNs) to detect\ncancer in small regions of interest (ROI) along a needle trace region. However,\nthis approach suffers from weak labelling, since the ground-truth\nhistopathology labels do not describe the properties of individual ROIs.\nRecently, multi-scale approaches have sought to mitigate this issue by\ncombining the context awareness of transformers with a CNN feature extractor to\ndetect cancer from multiple ROIs using multiple-instance learning (MIL). In\nthis work, we present a detailed study of several image transformer\narchitectures for both ROI-scale and multi-scale classification, and a\ncomparison of the performance of CNNs and transformers for ultrasound-based\nprostate cancer classification. We also design a novel multi-objective learning\nstrategy that combines both ROI and core predictions to further mitigate label\nnoise. METHODS: We evaluate 3 image transformers on ROI-scale cancer\nclassification, then use the strongest model to tune a multi-scale classifier\nwith MIL. We train our MIL models using our novel multi-objective learning\nstrategy and compare our results to existing baselines. RESULTS: We find that\nfor both ROI-scale and multi-scale PCa detection, image transformer backbones\nlag behind their CNN counterparts. This deficit in performance is even more\nnoticeable for larger models. When using multi-objective learning, we can\nimprove performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a\nspecificity of 66.3%. CONCLUSION: Convolutional networks are better suited for\nmodelling sparse datasets of prostate ultrasounds, producing more robust\nfeatures than transformers in PCa detection. Multi-scale methods remain the\nbest architecture for this task, with multi-objective learning presenting an\neffective way to improve performance.\n","authors":["Mohamed Harmanani","Paul F. R. Wilson","Fahimeh Fooladgar","Amoon Jamzad","Mahdi Gilany","Minh Nguyen Nhat To","Brian Wodlinger","Purang Abolmaesumi","Parvin Mousavi"],"pdf_url":"https://arxiv.org/pdf/2403.18233v1.pdf","comment":"early draft, 7 pages; Accepted to SPIE Medical Imaging 2024"},{"id":"http://arxiv.org/abs/2403.18228v1","updated":"2024-03-27T03:31:16Z","published":"2024-03-27T03:31:16Z","title":"Fourier or Wavelet bases as counterpart self-attention in spikformer for\n  efficient visual classification","summary":"  Energy-efficient spikformer has been proposed by integrating the biologically\nplausible spiking neural network (SNN) and artificial Transformer, whereby the\nSpiking Self-Attention (SSA) is used to achieve both higher accuracy and lower\ncomputational cost. However, it seems that self-attention is not always\nnecessary, especially in sparse spike-form calculation manners. In this paper,\nwe innovatively replace vanilla SSA (using dynamic bases calculating from Query\nand Key) with spike-form Fourier Transform, Wavelet Transform, and their\ncombinations (using fixed triangular or wavelets bases), based on a key\nhypothesis that both of them use a set of basis functions for information\ntransformation. Hence, the Fourier-or-Wavelet-based spikformer (FWformer) is\nproposed and verified in visual classification tasks, including both static\nimage and event-based video datasets. The FWformer can achieve comparable or\neven higher accuracies ($0.4\\%$-$1.5\\%$), higher running speed ($9\\%$-$51\\%$\nfor training and $19\\%$-$70\\%$ for inference), reduced theoretical energy\nconsumption ($20\\%$-$25\\%$), and reduced GPU memory usage ($4\\%$-$26\\%$),\ncompared to the standard spikformer. Our result indicates the continuous\nrefinement of new Transformers, that are inspired either by biological\ndiscovery (spike-form), or information theory (Fourier or Wavelet Transform),\nis promising.\n","authors":["Qingyu Wang","Duzhen Zhang","Tilelin Zhang","Bo Xu"],"pdf_url":"https://arxiv.org/pdf/2403.18228v1.pdf","comment":"18 pages, 2 figures. arXiv admin note: substantial text overlap with\n  arXiv:2308.02557"},{"id":"http://arxiv.org/abs/2403.18223v1","updated":"2024-03-27T03:25:45Z","published":"2024-03-27T03:25:45Z","title":"A Transformer-Based Framework for Payload Malware Detection and\n  Classification","summary":"  As malicious cyber threats become more sophisticated in breaching computer\nnetworks, the need for effective intrusion detection systems (IDSs) becomes\ncrucial. Techniques such as Deep Packet Inspection (DPI) have been introduced\nto allow IDSs analyze the content of network packets, providing more context\nfor identifying potential threats. IDSs traditionally rely on using\nanomaly-based and signature-based detection techniques to detect unrecognized\nand suspicious activity. Deep learning techniques have shown great potential in\nDPI for IDSs due to their efficiency in learning intricate patterns from the\npacket content being transmitted through the network. In this paper, we propose\na revolutionary DPI algorithm based on transformers adapted for the purpose of\ndetecting malicious traffic with a classifier head. Transformers learn the\ncomplex content of sequence data and generalize them well to similar scenarios\nthanks to their self-attention mechanism. Our proposed method uses the raw\npayload bytes that represent the packet contents and is deployed as\nman-in-the-middle. The payload bytes are used to detect malicious packets and\nclassify their types. Experimental results on the UNSW-NB15 and CIC-IOT23\ndatasets demonstrate that our transformer-based model is effective in\ndistinguishing malicious from benign traffic in the test dataset, attaining an\naverage accuracy of 79\\% using binary classification and 72\\% on the\nmulti-classification experiment, both using solely payload bytes.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2403.18223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18222v1","updated":"2024-03-27T03:19:36Z","published":"2024-03-27T03:19:36Z","title":"Uncertainty-Aware Deployment of Pre-trained Language-Conditioned\n  Imitation Learning Policies","summary":"  Large-scale robotic policies trained on data from diverse tasks and robotic\nplatforms hold great promise for enabling general-purpose robots; however,\nreliable generalization to new environment conditions remains a major\nchallenge. Toward addressing this challenge, we propose a novel approach for\nuncertainty-aware deployment of pre-trained language-conditioned imitation\nlearning agents. Specifically, we use temperature scaling to calibrate these\nmodels and exploit the calibrated model to make uncertainty-aware decisions by\naggregating the local information of candidate actions. We implement our\napproach in simulation using three such pre-trained models, and showcase its\npotential to significantly enhance task completion rates. The accompanying code\nis accessible at the link:\nhttps://github.com/BobWu1998/uncertainty_quant_all.git\n","authors":["Bo Wu","Bruce D. Lee","Kostas Daniilidis","Bernadette Bucher","Nikolai Matni"],"pdf_url":"https://arxiv.org/pdf/2403.18222v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.03256v2","updated":"2024-03-27T03:14:14Z","published":"2023-12-06T03:09:19Z","title":"CAFE: Towards Compact, Adaptive, and Fast Embedding for Large-scale\n  Recommendation Models","summary":"  Recently, the growing memory demands of embedding tables in Deep Learning\nRecommendation Models (DLRMs) pose great challenges for model training and\ndeployment. Existing embedding compression solutions cannot simultaneously meet\nthree key design requirements: memory efficiency, low latency, and adaptability\nto dynamic data distribution. This paper presents CAFE, a Compact, Adaptive,\nand Fast Embedding compression framework that addresses the above requirements.\nThe design philosophy of CAFE is to dynamically allocate more memory resources\nto important features (called hot features), and allocate less memory to\nunimportant ones. In CAFE, we propose a fast and lightweight sketch data\nstructure, named HotSketch, to capture feature importance and report hot\nfeatures in real time. For each reported hot feature, we assign it a unique\nembedding. For the non-hot features, we allow multiple features to share one\nembedding by using hash embedding technique. Guided by our design philosophy,\nwe further propose a multi-level hash embedding framework to optimize the\nembedding tables of non-hot features. We theoretically analyze the accuracy of\nHotSketch, and analyze the model convergence against deviation. Extensive\nexperiments show that CAFE significantly outperforms existing embedding\ncompression methods, yielding 3.92% and 3.68% superior testing AUC on Criteo\nKaggle dataset and CriteoTB dataset at a compression ratio of 10000x. The\nsource codes of CAFE are available at GitHub.\n","authors":["Hailin Zhang","Zirui Liu","Boxuan Chen","Yikai Zhao","Tong Zhao","Tong Yang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2312.03256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18219v1","updated":"2024-03-27T03:07:18Z","published":"2024-03-27T03:07:18Z","title":"From Two-Dimensional to Three-Dimensional Environment with Q-Learning:\n  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries","summary":"  Reinforcement learning (RL) algorithms have become indispensable tools in\nartificial intelligence, empowering agents to acquire optimal decision-making\npolicies through interactions with their environment and feedback mechanisms.\nThis study explores the performance of RL agents in both two-dimensional (2D)\nand three-dimensional (3D) environments, aiming to research the dynamics of\nlearning across different spatial dimensions. A key aspect of this\ninvestigation is the absence of pre-made libraries for learning, with the\nalgorithm developed exclusively through computational mathematics. The\nmethodological framework centers on RL principles, employing a Q-learning agent\nclass and distinct environment classes tailored to each spatial dimension. The\nresearch aims to address the question: How do reinforcement learning agents\nadapt and perform in environments of varying spatial dimensions, particularly\nin 2D and 3D settings? Through empirical analysis, the study evaluates agents'\nlearning trajectories and adaptation processes, revealing insights into the\nefficacy of RL algorithms in navigating complex, multi-dimensional spaces.\nReflections on the findings prompt considerations for future research,\nparticularly in understanding the dynamics of learning in higher-dimensional\nenvironments.\n","authors":["Ergon Cugler de Moraes Silva"],"pdf_url":"https://arxiv.org/pdf/2403.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18216v1","updated":"2024-03-27T02:59:04Z","published":"2024-03-27T02:59:04Z","title":"Minimax Optimal Fair Classification with Bounded Demographic Disparity","summary":"  Mitigating the disparate impact of statistical machine learning methods is\ncrucial for ensuring fairness. While extensive research aims to reduce\ndisparity, the effect of using a \\emph{finite dataset} -- as opposed to the\nentire population -- remains unclear. This paper explores the statistical\nfoundations of fair binary classification with two protected groups, focusing\non controlling demographic disparity, defined as the difference in acceptance\nrates between the groups. Although fairness may come at the cost of accuracy\neven with infinite data, we show that using a finite sample incurs additional\ncosts due to the need to estimate group-specific acceptance thresholds. We\nstudy the minimax optimal classification error while constraining demographic\ndisparity to a user-specified threshold. To quantify the impact of fairness\nconstraints, we introduce a novel measure called \\emph{fairness-aware excess\nrisk} and derive a minimax lower bound on this measure that all classifiers\nmust satisfy. Furthermore, we propose FairBayes-DDP+, a group-wise thresholding\nmethod with an offset that we show attains the minimax lower bound. Our lower\nbound proofs involve several innovations. Experiments support that\nFairBayes-DDP+ controls disparity at the user-specified level, while being\nfaster and having a more favorable fairness-accuracy tradeoff than several\nbaselines.\n","authors":["Xianli Zeng","Guang Cheng","Edgar Dobriban"],"pdf_url":"https://arxiv.org/pdf/2403.18216v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.18814v1","updated":"2024-03-27T17:59:04Z","published":"2024-03-27T17:59:04Z","title":"Mini-Gemini: Mining the Potential of Multi-modality Vision Language\n  Models","summary":"  In this work, we introduce Mini-Gemini, a simple and effective framework\nenhancing multi-modality Vision Language Models (VLMs). Despite the\nadvancements in VLMs facilitating basic visual dialog and reasoning, a\nperformance gap persists compared to advanced models like GPT-4 and Gemini. We\ntry to narrow the gap by mining the potential of VLMs for better performance\nand any-to-any workflow from three aspects, i.e., high-resolution visual\ntokens, high-quality data, and VLM-guided generation. To enhance visual tokens,\nwe propose to utilize an additional visual encoder for high-resolution\nrefinement without increasing the visual token count. We further construct a\nhigh-quality dataset that promotes precise image comprehension and\nreasoning-based generation, expanding the operational scope of current VLMs. In\ngeneral, Mini-Gemini further mines the potential of VLMs and empowers current\nframeworks with image understanding, reasoning, and generation simultaneously.\nMini-Gemini supports a series of dense and MoE Large Language Models (LLMs)\nfrom 2B to 34B. It is demonstrated to achieve leading performance in several\nzero-shot benchmarks and even surpasses the developed private models. Code and\nmodels are available at https://github.com/dvlab-research/MiniGemini.\n","authors":["Yanwei Li","Yuechen Zhang","Chengyao Wang","Zhisheng Zhong","Yixin Chen","Ruihang Chu","Shaoteng Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2403.18814v1.pdf","comment":"Code and models are available at\n  https://github.com/dvlab-research/MiniGemini"},{"id":"http://arxiv.org/abs/2403.18807v1","updated":"2024-03-27T17:53:30Z","published":"2024-03-27T17:53:30Z","title":"ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth\n  Estimation","summary":"  In the absence of parallax cues, a learning-based single image depth\nestimation (SIDE) model relies heavily on shading and contextual cues in the\nimage. While this simplicity is attractive, it is necessary to train such\nmodels on large and varied datasets, which are difficult to capture. It has\nbeen shown that using embeddings from pre-trained foundational models, such as\nCLIP, improves zero shot transfer in several applications. Taking inspiration\nfrom this, in our paper we explore the use of global image priors generated\nfrom a pre-trained ViT model to provide more detailed contextual information.\nWe argue that the embedding vector from a ViT model, pre-trained on a large\ndataset, captures greater relevant information for SIDE than the usual route of\ngenerating pseudo image captions, followed by CLIP based text embeddings. Based\non this idea, we propose a new SIDE model using a diffusion backbone which is\nconditioned on ViT embeddings. Our proposed design establishes a new\nstate-of-the-art (SOTA) for SIDE on NYUv2 dataset, achieving Abs Rel error of\n0.059(14% improvement) compared to 0.069 by the current SOTA (VPD). And on\nKITTI dataset, achieving Sq Rel error of 0.139 (2% improvement) compared to\n0.142 by the current SOTA (GEDepth). For zero-shot transfer with a model\ntrained on NYUv2, we report mean relative improvement of (20%, 23%, 81%, 25%)\nover NeWCRFs on (Sun-RGBD, iBims1, DIODE, HyperSim) datasets, compared to (16%,\n18%, 45%, 9%) by ZoeDepth. The code is available at\nhttps://github.com/Aradhye2002/EcoDepth.\n","authors":["Suraj Patni","Aradhye Agarwal","Chetan Arora"],"pdf_url":"https://arxiv.org/pdf/2403.18807v1.pdf","comment":"Accepted at IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2024"},{"id":"http://arxiv.org/abs/2403.18802v1","updated":"2024-03-27T17:48:55Z","published":"2024-03-27T17:48:55Z","title":"Long-form factuality in large language models","summary":"  Large language models (LLMs) often generate content that contains factual\nerrors when responding to fact-seeking prompts on open-ended topics. To\nbenchmark a model's long-form factuality in open domains, we first use GPT-4 to\ngenerate LongFact, a prompt set comprising thousands of questions spanning 38\ntopics. We then propose that LLM agents can be used as automated evaluators for\nlong-form factuality through a method which we call Search-Augmented Factuality\nEvaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into\na set of individual facts and to evaluate the accuracy of each fact using a\nmulti-step reasoning process comprising sending search queries to Google Search\nand determining whether a fact is supported by the search results. Furthermore,\nwe propose extending F1 score as an aggregated metric for long-form factuality.\nTo do so, we balance the percentage of supported facts in a response\n(precision) with the percentage of provided facts relative to a hyperparameter\nrepresenting a user's preferred response length (recall).\n  Empirically, we demonstrate that LLM agents can achieve superhuman rating\nperformance - on a set of ~16k individual facts, SAFE agrees with crowdsourced\nhuman annotators 72% of the time, and on a random subset of 100 disagreement\ncases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times\ncheaper than human annotators. We also benchmark thirteen language models on\nLongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding\nthat larger language models generally achieve better long-form factuality.\nLongFact, SAFE, and all experimental code are available at\nhttps://github.com/google-deepmind/long-form-factuality.\n","authors":["Jerry Wei","Chengrun Yang","Xinying Song","Yifeng Lu","Nathan Hu","Dustin Tran","Daiyi Peng","Ruibo Liu","Da Huang","Cosmo Du","Quoc V. Le"],"pdf_url":"https://arxiv.org/pdf/2403.18802v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13525v2","updated":"2024-03-27T17:47:56Z","published":"2023-05-22T22:41:49Z","title":"A 4D Hybrid Algorithm to Scale Parallel Training to Thousands of GPUs","summary":"  Large communication costs are a critical bottleneck in training\nstate-of-the-art neural networks on distributed systems. This paper introduces\nAxoNN, a novel four-dimensional (4D) parallelization approach, inspired by\nAgarwal's algorithm for matrix multiplication, for parallelizing tensor\ncomputations in deep learning, AxoNN employs two key strategies to minimize\ncommunication overhead. First, we optimize communication by overlapping\nexpensive collective operations (reduce-scatter, all-gather, all-reduce) with\ncomputations. Our experiments with a 20-billion parameter transformer model\ndemonstrate that these optimizations deliver nearly 53\\% improvement. Second,\nwe present an analytical model to assist users in identifying\ncommunication-minimizing configurations within the vast search space defined by\nour 4D algorithm. This model empowers practitioners by simplifying the tuning\nprocess for their specific training workloads. When training an 80-billion\nparameter model on 1024 GPUs of Perlmutter, AxoNN surpasses Megatron-LM, a\nstate-of-the-art framework, by a significant 26%. Additionally, it achieves 57%\nof the theoretical peak FLOP/s.\n","authors":["Siddharth Singh","Prajwal Singhania","Aditya K. Ranjan","Zack Sating","Abhinav Bhatele"],"pdf_url":"https://arxiv.org/pdf/2305.13525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.10319v4","updated":"2024-03-27T17:41:50Z","published":"2023-11-17T04:04:29Z","title":"Shifting to Machine Supervision: Annotation-Efficient Semi and\n  Self-Supervised Learning for Automatic Medical Image Segmentation and\n  Classification","summary":"  Advancements in clinical treatment are increasingly constrained by the\nlimitations of supervised learning techniques, which depend heavily on large\nvolumes of annotated data. The annotation process is not only costly but also\ndemands substantial time from clinical specialists. Addressing this issue, we\nintroduce the S4MI (Self-Supervision and Semi-Supervision for Medical Imaging)\npipeline, a novel approach that leverages advancements in self-supervised and\nsemi-supervised learning. These techniques engage in auxiliary tasks that do\nnot require labeling, thus simplifying the scaling of machine supervision\ncompared to fully-supervised methods. Our study benchmarks these techniques on\nthree distinct medical imaging datasets to evaluate their effectiveness in\nclassification and segmentation tasks. Notably, we observed that self\nsupervised learning significantly surpassed the performance of supervised\nmethods in the classification of all evaluated datasets. Remarkably, the\nsemi-supervised approach demonstrated superior outcomes in segmentation,\noutperforming fully-supervised methods while using 50% fewer labels across all\ndatasets. In line with our commitment to contributing to the scientific\ncommunity, we have made the S4MI code openly accessible, allowing for broader\napplication and further development of these methods.\n","authors":["Pranav Singh","Raviteja Chukkapalli","Shravan Chaudhari","Luoyao Chen","Mei Chen","Jinqian Pan","Craig Smuda","Jacopo Cirrone"],"pdf_url":"https://arxiv.org/pdf/2311.10319v4.pdf","comment":"Seventeen pages (incl. references), five figures, and one table.\n  (Under Review)"},{"id":"http://arxiv.org/abs/2403.18795v1","updated":"2024-03-27T17:40:14Z","published":"2024-03-27T17:40:14Z","title":"Gamba: Marry Gaussian Splatting with Mamba for single view 3D\n  reconstruction","summary":"  We tackle the challenge of efficiently reconstructing a 3D asset from a\nsingle image with growing demands for automated 3D content creation pipelines.\nPrevious methods primarily rely on Score Distillation Sampling (SDS) and Neural\nRadiance Fields (NeRF). Despite their significant success, these approaches\nencounter practical limitations due to lengthy optimization and considerable\nmemory usage. In this report, we introduce Gamba, an end-to-end amortized 3D\nreconstruction model from single-view images, emphasizing two main insights:\n(1) 3D representation: leveraging a large number of 3D Gaussians for an\nefficient 3D Gaussian splatting process; (2) Backbone design: introducing a\nMamba-based sequential network that facilitates context-dependent reasoning and\nlinear scalability with the sequence (token) length, accommodating a\nsubstantial number of Gaussians. Gamba incorporates significant advancements in\ndata preprocessing, regularization design, and training methodologies. We\nassessed Gamba against existing optimization-based and feed-forward 3D\ngeneration approaches using the real-world scanned OmniObject3D dataset. Here,\nGamba demonstrates competitive generation capabilities, both qualitatively and\nquantitatively, while achieving remarkable speed, approximately 0.6 second on a\nsingle NVIDIA A100 GPU.\n","authors":["Qiuhong Shen","Xuanyu Yi","Zike Wu","Pan Zhou","Hanwang Zhang","Shuicheng Yan","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17574v2","updated":"2024-03-27T17:34:57Z","published":"2024-02-27T15:09:20Z","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and\n  Optimization","summary":"  Large Language Models exhibit robust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are designed as specific task solvers\nwith sophisticated prompt engineering, rather than agents capable of learning\nand evolving through interactions. These task solvers necessitate manually\ncrafted prompts to inform task rules and regulate LLM behaviors, inherently\nincapacitating to address complex dynamic scenarios e.g., large interactive\ngames. In light of this, we propose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that can learn a wealth of expertise\nfrom interactive experiences and progressively elevate its behavioral policy.\nSpecifically, it involves a dynamic belief generation and reflection process\nfor policy evolution. Rather than action-level reflection, Agent-Pro\niteratively reflects on past trajectories and beliefs, fine-tuning its\nirrational beliefs for a better policy. Moreover, a depth-first search is\nemployed for policy optimization, ensuring continual enhancement in policy\npayoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,\noutperforming vanilla LLM and specialized models. Our results show Agent-Pro\ncan learn and evolve in complex and dynamic scenes, which also benefits\nnumerous LLM-based applications.\n","authors":["Wenqi Zhang","Ke Tang","Hai Wu","Mengna Wang","Yongliang Shen","Guiyang Hou","Zeqi Tan","Peng Li","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2402.17574v2.pdf","comment":"LLM-based Agent"},{"id":"http://arxiv.org/abs/2401.02009v2","updated":"2024-03-27T17:24:47Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18775v1","updated":"2024-03-27T17:23:39Z","published":"2024-03-27T17:23:39Z","title":"ImageNet-D: Benchmarking Neural Network Robustness on Diffusion\n  Synthetic Object","summary":"  We establish rigorous benchmarks for visual perception robustness. Synthetic\nimages such as ImageNet-C, ImageNet-9, and Stylized ImageNet provide specific\ntype of evaluation over synthetic corruptions, backgrounds, and textures, yet\nthose robustness benchmarks are restricted in specified variations and have low\nsynthetic quality. In this work, we introduce generative model as a data source\nfor synthesizing hard images that benchmark deep models' robustness. Leveraging\ndiffusion models, we are able to generate images with more diversified\nbackgrounds, textures, and materials than any prior work, where we term this\nbenchmark as ImageNet-D. Experimental results show that ImageNet-D results in a\nsignificant accuracy drop to a range of vision models, from the standard ResNet\nvisual classifier to the latest foundation models like CLIP and MiniGPT-4,\nsignificantly reducing their accuracy by up to 60\\%. Our work suggests that\ndiffusion models can be an effective source to test vision models. The code and\ndataset are available at https://github.com/chenshuang-zhang/imagenet_d.\n","authors":["Chenshuang Zhang","Fei Pan","Junmo Kim","In So Kweon","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18775v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2309.04381v2","updated":"2024-03-27T17:07:47Z","published":"2023-09-08T15:23:40Z","title":"Generalization Bounds: Perspectives from Information Theory and\n  PAC-Bayes","summary":"  A fundamental question in theoretical machine learning is generalization.\nOver the past decades, the PAC-Bayesian approach has been established as a\nflexible framework to address the generalization capabilities of machine\nlearning algorithms, and design new ones. Recently, it has garnered increased\ninterest due to its potential applicability for a variety of learning\nalgorithms, including deep neural networks. In parallel, an\ninformation-theoretic view of generalization has developed, wherein the\nrelation between generalization and various information measures has been\nestablished. This framework is intimately connected to the PAC-Bayesian\napproach, and a number of results have been independently discovered in both\nstrands. In this monograph, we highlight this strong connection and present a\nunified treatment of PAC-Bayesian and information-theoretic generalization\nbounds. We present techniques and results that the two perspectives have in\ncommon, and discuss the approaches and interpretations that differ. In\nparticular, we demonstrate how many proofs in the area share a modular\nstructure, through which the underlying ideas can be intuited. We pay special\nattention to the conditional mutual information (CMI) framework; analytical\nstudies of the information complexity of learning algorithms; and the\napplication of the proposed methods to deep learning. This monograph is\nintended to provide a comprehensive introduction to information-theoretic\ngeneralization bounds and their connection to PAC-Bayes, serving as a\nfoundation from which the most recent developments are accessible. It is aimed\nbroadly towards researchers with an interest in generalization and theoretical\nmachine learning.\n","authors":["Fredrik Hellström","Giuseppe Durisi","Benjamin Guedj","Maxim Raginsky"],"pdf_url":"https://arxiv.org/pdf/2309.04381v2.pdf","comment":"228 pages"},{"id":"http://arxiv.org/abs/2403.06054v4","updated":"2024-03-27T17:06:10Z","published":"2024-03-10T00:47:05Z","title":"Decoupled Data Consistency with Diffusion Purification for Image\n  Restoration","summary":"  Diffusion models have recently gained traction as a powerful class of deep\ngenerative priors, excelling in a wide range of image restoration tasks due to\ntheir exceptional ability to model data distributions. To solve image\nrestoration problems, many existing techniques achieve data consistency by\nincorporating additional likelihood gradient steps into the reverse sampling\nprocess of diffusion models. However, the additional gradient steps pose a\nchallenge for real-world practical applications as they incur a large\ncomputational overhead, thereby increasing inference time. They also present\nadditional difficulties when using accelerated diffusion model samplers, as the\nnumber of data consistency steps is limited by the number of reverse sampling\nsteps. In this work, we propose a novel diffusion-based image restoration\nsolver that addresses these issues by decoupling the reverse process from the\ndata consistency steps. Our method involves alternating between a\nreconstruction phase to maintain data consistency and a refinement phase that\nenforces the prior via diffusion purification. Our approach demonstrates\nversatility, making it highly adaptable for efficient problem-solving in latent\nspace. Additionally, it reduces the necessity for numerous sampling steps\nthrough the integration of consistency models. The efficacy of our approach is\nvalidated through comprehensive experiments across various image restoration\ntasks, including image denoising, deblurring, inpainting, and super-resolution.\n","authors":["Xiang Li","Soo Min Kwon","Ismail R. Alkhouri","Saiprasad Ravishankar","Qing Qu"],"pdf_url":"https://arxiv.org/pdf/2403.06054v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18766v1","updated":"2024-03-27T17:05:03Z","published":"2024-03-27T17:05:03Z","title":"Superior Parallel Big Data Clustering through Competitive Stochastic\n  Sample Size Optimization in Big-means","summary":"  This paper introduces a novel K-means clustering algorithm, an advancement on\nthe conventional Big-means methodology. The proposed method efficiently\nintegrates parallel processing, stochastic sampling, and competitive\noptimization to create a scalable variant designed for big data applications.\nIt addresses scalability and computation time challenges typically faced with\ntraditional techniques. The algorithm adjusts sample sizes dynamically for each\nworker during execution, optimizing performance. Data from these sample sizes\nare continually analyzed, facilitating the identification of the most efficient\nconfiguration. By incorporating a competitive element among workers using\ndifferent sample sizes, efficiency within the Big-means algorithm is further\nstimulated. In essence, the algorithm balances computational time and\nclustering quality by employing a stochastic, competitive sampling strategy in\na parallel computing setting.\n","authors":["Rustam Mussabayev","Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2403.18766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18762v1","updated":"2024-03-27T17:01:10Z","published":"2024-03-27T17:01:10Z","title":"ModaLink: Unifying Modalities for Efficient Image-to-PointCloud Place\n  Recognition","summary":"  Place recognition is an important task for robots and autonomous cars to\nlocalize themselves and close loops in pre-built maps. While single-modal\nsensor-based methods have shown satisfactory performance, cross-modal place\nrecognition that retrieving images from a point-cloud database remains a\nchallenging problem. Current cross-modal methods transform images into 3D\npoints using depth estimation for modality conversion, which are usually\ncomputationally intensive and need expensive labeled data for depth\nsupervision. In this work, we introduce a fast and lightweight framework to\nencode images and point clouds into place-distinctive descriptors. We propose\nan effective Field of View (FoV) transformation module to convert point clouds\ninto an analogous modality as images. This module eliminates the necessity for\ndepth estimation and helps subsequent modules achieve real-time performance. We\nfurther design a non-negative factorization-based encoder to extract mutually\nconsistent semantic features between point clouds and images. This encoder\nyields more distinctive global descriptors for retrieval. Experimental results\non the KITTI dataset show that our proposed methods achieve state-of-the-art\nperformance while running in real time. Additional evaluation on the HAOMO\ndataset covering a 17 km trajectory further shows the practical generalization\ncapabilities. We have released the implementation of our methods as open source\nat: https://github.com/haomo-ai/ModaLink.git.\n","authors":["Weidong Xie","Lun Luo","Nanfei Ye","Yi Ren","Shaoyi Du","Minhang Wang","Jintao Xu","Rui Ai","Weihao Gu","Xieyuanli Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18762v1.pdf","comment":"8 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2311.01483v3","updated":"2024-03-27T16:56:23Z","published":"2023-11-02T14:47:06Z","title":"FedSN: A Novel Federated Learning Framework over LEO Satellite Networks","summary":"  Recently, a large number of Low Earth Orbit (LEO) satellites have been\nlaunched and deployed successfully in space by commercial companies, such as\nSpaceX. Due to multimodal sensors equipped by the LEO satellites, they serve\nnot only for communication but also for various machine learning applications,\nsuch as space modulation recognition, remote sensing image classification, etc.\nHowever, the ground station (GS) may be incapable of downloading such a large\nvolume of raw sensing data for centralized model training due to the limited\ncontact time with LEO satellites (e.g. 5 minutes). Therefore, federated\nlearning (FL) has emerged as the promising solution to address this problem via\non-device training. Unfortunately, to enable FL on LEO satellites, we still\nface three critical challenges that are i) heterogeneous computing and memory\ncapabilities, ii) limited uplink rate, and iii) model staleness. To this end,\nwe propose FedSN as a general FL framework to tackle the above challenges, and\nfully explore data diversity on LEO satellites. Specifically, we first present\na novel sub-structure scheme to enable heterogeneous local model training\nconsidering different computing, memory, and communication constraints on LEO\nsatellites. Additionally, we propose a pseudo-synchronous model aggregation\nstrategy to dynamically schedule model aggregation for compensating model\nstaleness. To further demonstrate the effectiveness of the FedSN, we evaluate\nit using space modulation recognition and remote sensing image classification\ntasks by leveraging the data from real-world satellite networks. Extensive\nexperimental results demonstrate that FedSN framework achieves higher accuracy,\nlower computing, and communication overhead than the state-of-the-art\nbenchmarks and the effectiveness of each components in FedSN.\n","authors":["Zheng Lin","Zhe Chen","Zihan Fang","Xianhao Chen","Xiong Wang","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2311.01483v3.pdf","comment":"14 pages, 17 figures"},{"id":"http://arxiv.org/abs/2403.18756v1","updated":"2024-03-27T16:56:14Z","published":"2024-03-27T16:56:14Z","title":"Detection of subclinical atherosclerosis by image-based deep learning on\n  chest x-ray","summary":"  Aims. To develop a deep-learning based system for recognition of subclinical\natherosclerosis on a plain frontal chest x-ray. Methods and Results. A\ndeep-learning algorithm to predict coronary artery calcium (CAC) score (the\nAI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%\ninternal validation cohort) of primary prevention patients (58.4% male, median\nage 63 [51-74] years) with available paired chest x-ray and chest computed\ntomography (CT) indicated for any clinical reason and performed within 3\nmonths. The CAC score calculated on chest CT was used as ground truth. The\nmodel was validated on an temporally-independent cohort of 90 patients from the\nsame institution (external validation). The diagnostic accuracy of the AI-CAC\nmodel assessed by the area under the curve (AUC) was the primary outcome.\nOverall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.\nAUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation\ncohort and 0.77 in the external validation cohort. Sensitivity was consistently\nabove 92% in both cohorts. In the overall cohort (n=540), among patients with\nAI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with\nAI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events\n(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to\naccurately detect subclinical atherosclerosis on chest x-ray with elevated\nsensitivity, and to predict ASCVD events with elevated negative predictive\nvalue. Adoption of the AI-CAC model to refine CV risk stratification or as an\nopportunistic screening tool requires prospective evaluation.\n","authors":["Guglielmo Gallone","Francesco Iodice","Alberto Presta","Davide Tore","Ovidio de Filippo","Michele Visciano","Carlo Alberto Barbano","Alessandro Serafini","Paola Gorrini","Alessandro Bruno","Walter Grosso Marra","James Hughes","Mario Iannaccone","Paolo Fonio","Attilio Fiandrotti","Alessandro Depaoli","Marco Grangetto","Gaetano Maria de Ferrari","Fabrizio D'Ascenzo"],"pdf_url":"https://arxiv.org/pdf/2403.18756v1.pdf","comment":"Submitted to European Heart Journal - Cardiovascular Imaging Added\n  also the additional material 44 pages (30 main paper, 14 additional\n  material), 14 figures (5 main manuscript, 9 additional material)"},{"id":"http://arxiv.org/abs/2403.18755v1","updated":"2024-03-27T16:54:45Z","published":"2024-03-27T16:54:45Z","title":"Many-Objective Evolutionary Influence Maximization: Balancing Spread,\n  Budget, Fairness, and Time","summary":"  The Influence Maximization (IM) problem seeks to discover the set of nodes in\na graph that can spread the information propagation at most. This problem is\nknown to be NP-hard, and it is usually studied by maximizing the influence\n(spread) and, optionally, optimizing a second objective, such as minimizing the\nseed set size or maximizing the influence fairness. However, in many practical\nscenarios multiple aspects of the IM problem must be optimized at the same\ntime. In this work, we propose a first case study where several IM-specific\nobjective functions, namely budget, fairness, communities, and time, are\noptimized on top of the maximization of influence and minimization of the seed\nset size. To this aim, we introduce MOEIM (Many-Objective Evolutionary\nAlgorithm for Influence Maximization) a Multi-Objective Evolutionary Algorithm\n(MOEA) based on NSGA-II incorporating graph-aware operators and a smart\ninitialization. We compare MOEIM in two experimental settings, including a\ntotal of nine graph datasets, two heuristic methods, a related MOEA, and a\nstate-of-the-art Deep Learning approach. The experiments show that MOEIM\noverall outperforms the competitors in most of the tested many-objective\nsettings. To conclude, we also investigate the correlation between the\nobjectives, leading to novel insights into the topic. The codebase is available\nat https://github.com/eliacunegatti/MOEIM.\n","authors":["Elia Cunegatti","Leonardo Lucio Custode","Giovanni Iacca"],"pdf_url":"https://arxiv.org/pdf/2403.18755v1.pdf","comment":"To appear in Genetic and Evolutionary Computation Conference (GECCO\n  24 Companion), July 14 18, 2024, Melbourne, VIC, Australia. ACM, New York,\n  NY, USA"},{"id":"http://arxiv.org/abs/2201.06180v2","updated":"2024-03-27T16:45:26Z","published":"2022-01-17T02:30:25Z","title":"Nonlinear Control Allocation: A Learning Based Approach","summary":"  Modern aircraft are designed with redundant control effectors to cater for\nfault tolerance and maneuverability requirements. This leads to aircraft being\nover-actuated and requires control allocation schemes to distribute the control\ncommands among control effectors. Traditionally, optimization-based control\nallocation schemes are used; however, for nonlinear allocation problems, these\nmethods require large computational resources. In this work, an artificial\nneural network (ANN) based nonlinear control allocation scheme is proposed. The\nproposed scheme is composed of learning the inverse of the control\neffectiveness map through ANN, and then implementing it as an allocator instead\nof solving an online optimization problem. Stability conditions are presented\nfor closed-loop systems incorporating the allocator, and computational\nchallenges are explored with piece-wise linear effectiveness functions and\nANN-based allocators. To demonstrate the efficacy of the proposed scheme, it is\ncompared with a standard quadratic programming-based method for control\nallocation.\n","authors":["Hafiz Zeeshan Iqbal Khan","Surrayya Mobeen","Jahanzeb Rajput","Jamshed Riaz"],"pdf_url":"https://arxiv.org/pdf/2201.06180v2.pdf","comment":"submitted to IEEE Conference on Decision and Control (CDC), 2024"},{"id":"http://arxiv.org/abs/2403.18742v1","updated":"2024-03-27T16:39:28Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18731v1","updated":"2024-03-27T16:21:24Z","published":"2024-03-27T16:21:24Z","title":"Enhancing Manufacturing Quality Prediction Models through the\n  Integration of Explainability Methods","summary":"  This research presents a method that utilizes explainability techniques to\namplify the performance of machine learning (ML) models in forecasting the\nquality of milling processes, as demonstrated in this paper through a\nmanufacturing use case. The methodology entails the initial training of ML\nmodels, followed by a fine-tuning phase where irrelevant features identified\nthrough explainability methods are eliminated. This procedural refinement\nresults in performance enhancements, paving the way for potential reductions in\nmanufacturing costs and a better understanding of the trained ML models. This\nstudy highlights the usefulness of explainability techniques in both explaining\nand optimizing predictive models in the manufacturing realm.\n","authors":["Dennis Gross","Helge Spieker","Arnaud Gotlieb","Ricardo Knoblauch"],"pdf_url":"https://arxiv.org/pdf/2403.18731v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18725v1","updated":"2024-03-27T16:15:21Z","published":"2024-03-27T16:15:21Z","title":"Probabilistic Model Checking of Stochastic Reinforcement Learning\n  Policies","summary":"  We introduce a method to verify stochastic reinforcement learning (RL)\npolicies. This approach is compatible with any RL algorithm as long as the\nalgorithm and its corresponding environment collectively adhere to the Markov\nproperty. In this setting, the future state of the environment should depend\nsolely on its current state and the action executed, independent of any\nprevious states or actions. Our method integrates a verification technique,\nreferred to as model checking, with RL, leveraging a Markov decision process, a\ntrained RL policy, and a probabilistic computation tree logic (PCTL) formula to\nbuild a formal model that can be subsequently verified via the model checker\nStorm. We demonstrate our method's applicability across multiple benchmarks,\ncomparing it to baseline methods called deterministic safety estimates and\nnaive monolithic model checking. Our results show that our method is suited to\nverify stochastic RL policies.\n","authors":["Dennis Gross","Helge Spieker"],"pdf_url":"https://arxiv.org/pdf/2403.18725v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.03100v2","updated":"2024-03-27T16:14:34Z","published":"2024-03-05T16:35:25Z","title":"NaturalSpeech 3: Zero-Shot Speech Synthesis with Factorized Codec and\n  Diffusion Models","summary":"  While recent large-scale text-to-speech (TTS) models have achieved\nsignificant progress, they still fall short in speech quality, similarity, and\nprosody. Considering speech intricately encompasses various attributes (e.g.,\ncontent, prosody, timbre, and acoustic details) that pose significant\nchallenges for generation, a natural idea is to factorize speech into\nindividual subspaces representing different attributes and generate them\nindividually. Motivated by it, we propose NaturalSpeech 3, a TTS system with\nnovel factorized diffusion models to generate natural speech in a zero-shot\nway. Specifically, 1) we design a neural codec with factorized vector\nquantization (FVQ) to disentangle speech waveform into subspaces of content,\nprosody, timbre, and acoustic details; 2) we propose a factorized diffusion\nmodel to generate attributes in each subspace following its corresponding\nprompt. With this factorization design, NaturalSpeech 3 can effectively and\nefficiently model intricate speech with disentangled subspaces in a\ndivide-and-conquer way. Experiments show that NaturalSpeech 3 outperforms the\nstate-of-the-art TTS systems on quality, similarity, prosody, and\nintelligibility, and achieves on-par quality with human recordings.\nFurthermore, we achieve better performance by scaling to 1B parameters and 200K\nhours of training data.\n","authors":["Zeqian Ju","Yuancheng Wang","Kai Shen","Xu Tan","Detai Xin","Dongchao Yang","Yanqing Liu","Yichong Leng","Kaitao Song","Siliang Tang","Zhizheng Wu","Tao Qin","Xiang-Yang Li","Wei Ye","Shikun Zhang","Jiang Bian","Lei He","Jinyu Li","Sheng Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.03100v2.pdf","comment":"Achieving human-level quality and naturalness on multi-speaker\n  datasets (e.g., LibriSpeech) in a zero-shot way"},{"id":"http://arxiv.org/abs/2403.18717v1","updated":"2024-03-27T16:06:37Z","published":"2024-03-27T16:06:37Z","title":"Semi-Supervised Learning for Deep Causal Generative Models","summary":"  Developing models that can answer questions of the form \"How would $x$ change\nif $y$ had been $z$?\" is fundamental for advancing medical image analysis.\nTraining causal generative models that address such counterfactual questions,\nthough, currently requires that all relevant variables have been observed and\nthat corresponding labels are available in training data. However, clinical\ndata may not have complete records for all patients and state of the art causal\ngenerative models are unable to take full advantage of this. We thus develop,\nfor the first time, a semi-supervised deep causal generative model that\nexploits the causal relationships between variables to maximise the use of all\navailable data. We explore this in the setting where each sample is either\nfully labelled or fully unlabelled, as well as the more clinically realistic\ncase of having different labels missing for each sample. We leverage techniques\nfrom causal inference to infer missing values and subsequently generate\nrealistic counterfactuals, even for samples with incomplete labels.\n","authors":["Yasin Ibrahim","Hermione Warr","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2403.18717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18715v1","updated":"2024-03-27T16:04:47Z","published":"2024-03-27T16:04:47Z","title":"Mitigating Hallucinations in Large Vision-Language Models with\n  Instruction Contrastive Decoding","summary":"  Large Vision-Language Models (LVLMs) are increasingly adept at generating\ncontextually detailed and coherent responses from visual inputs. However, their\napplication in multimodal decision-making and open-ended generation is hindered\nby a notable rate of hallucinations, where generated text inaccurately\nrepresents the visual contents. To address this issue, this paper introduces\nthe Instruction Contrastive Decoding (ICD) method, a novel approach designed to\nreduce hallucinations during LVLM inference. Our method is inspired by our\nobservation that what we call disturbance instructions significantly exacerbate\nhallucinations in multimodal fusion modules. ICD contrasts distributions from\nstandard and instruction disturbance, thereby increasing alignment uncertainty\nand effectively subtracting hallucinated concepts from the original\ndistribution. Through comprehensive experiments on discriminative benchmarks\n(POPE and MME) and a generative benchmark (LLaVa-Bench), we demonstrate that\nICD significantly mitigates both object-level and attribute-level\nhallucinations. Moreover, our method not only addresses hallucinations but also\nsignificantly enhances the general perception and recognition capabilities of\nLVLMs.\n","authors":["Xintong Wang","Jingheng Pan","Liang Ding","Chris Biemann"],"pdf_url":"https://arxiv.org/pdf/2403.18715v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.03123v3","updated":"2024-03-27T16:03:32Z","published":"2023-04-13T16:01:28Z","title":"ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and\n  Ethics) Evaluation: A Review","summary":"  ChatGPT is another large language model (LLM) vastly available for the\nconsumers on their devices but due to its performance and ability to converse\neffectively, it has gained a huge popularity amongst research as well as\nindustrial community. Recently, many studies have been published to show the\neffectiveness, efficiency, integration, and sentiments of chatGPT and other\nLLMs. In contrast, this study focuses on the important aspects that are mostly\noverlooked, i.e. sustainability, privacy, digital divide, and ethics and\nsuggests that not only chatGPT but every subsequent entry in the category of\nconversational bots should undergo Sustainability, PrivAcy, Digital divide, and\nEthics (SPADE) evaluation. This paper discusses in detail the issues and\nconcerns raised over chatGPT in line with aforementioned characteristics. We\nalso discuss the recent EU AI Act briefly in accordance with the SPADE\nevaluation. We support our hypothesis by some preliminary data collection and\nvisualizations along with hypothesized facts. We also suggest mitigations and\nrecommendations for each of the concerns. Furthermore, we also suggest some\npolicies and recommendations for EU AI policy act concerning ethics, digital\ndivide, and sustainability.\n","authors":["Sunder Ali Khowaja","Parus Khuwaja","Kapal Dev","Weizheng Wang","Lewis Nkenyereye"],"pdf_url":"https://arxiv.org/pdf/2305.03123v3.pdf","comment":"29 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.18711v1","updated":"2024-03-27T15:58:25Z","published":"2024-03-27T15:58:25Z","title":"SAT-NGP : Unleashing Neural Graphics Primitives for Fast Relightable\n  Transient-Free 3D reconstruction from Satellite Imagery","summary":"  Current stereo-vision pipelines produce high accuracy 3D reconstruction when\nusing multiple pairs or triplets of satellite images. However, these pipelines\nare sensitive to the changes between images that can occur as a result of\nmulti-date acquisitions. Such variations are mainly due to variable shadows,\nreflexions and transient objects (cars, vegetation). To take such changes into\naccount, Neural Radiance Fields (NeRF) have recently been applied to multi-date\nsatellite imagery. However, Neural methods are very compute-intensive, taking\ndozens of hours to learn, compared with minutes for standard stereo-vision\npipelines. Following the ideas of Instant Neural Graphics Primitives we propose\nto use an efficient sampling strategy and multi-resolution hash encoding to\naccelerate the learning. Our model, Satellite Neural Graphics Primitives\n(SAT-NGP) decreases the learning time to 15 minutes while maintaining the\nquality of the 3D reconstruction.\n","authors":["Camille Billouard","Dawa Derksen","Emmanuelle Sarrazin","Bruno Vallet"],"pdf_url":"https://arxiv.org/pdf/2403.18711v1.pdf","comment":"5 pages, 3 figures, 1 table; Accepted to International Geoscience and\n  Remote Sensing Symposium (IGARSS) 2024; Code available at\n  https://github.com/Ellimac0/SAT-NGP"},{"id":"http://arxiv.org/abs/2401.15120v2","updated":"2024-03-27T15:49:52Z","published":"2024-01-26T03:44:58Z","title":"Incorporating simulated spatial context information improves the\n  effectiveness of contrastive learning models","summary":"  Visual learning often occurs in a specific context, where an agent acquires\nskills through exploration and tracking of its location in a consistent\nenvironment. The historical spatial context of the agent provides a similarity\nsignal for self-supervised contrastive learning. We present a unique approach,\ntermed Environmental Spatial Similarity (ESS), that complements existing\ncontrastive learning methods. Using images from simulated, photorealistic\nenvironments as an experimental setting, we demonstrate that ESS outperforms\ntraditional instance discrimination approaches. Moreover, sampling additional\ndata from the same environment substantially improves accuracy and provides new\naugmentations. ESS allows remarkable proficiency in room classification and\nspatial prediction tasks, especially in unfamiliar environments. This learning\nparadigm has the potential to enable rapid visual learning in agents operating\nin new environments with unique visual characteristics. Potentially\ntransformative applications span from robotics to space exploration. Our proof\nof concept demonstrates improved efficiency over methods that rely on\nextensive, disconnected datasets.\n","authors":["Lizhen Zhu","James Z. Wang","Wonseuk Lee","Brad Wyble"],"pdf_url":"https://arxiv.org/pdf/2401.15120v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11800v3","updated":"2024-03-27T15:48:29Z","published":"2024-02-19T03:08:02Z","title":"Stochastic Approximation with Delayed Updates: Finite-Time Rates under\n  Markovian Sampling","summary":"  Motivated by applications in large-scale and multi-agent reinforcement\nlearning, we study the non-asymptotic performance of stochastic approximation\n(SA) schemes with delayed updates under Markovian sampling. While the effect of\ndelays has been extensively studied for optimization, the manner in which they\ninteract with the underlying Markov process to shape the finite-time\nperformance of SA remains poorly understood. In this context, our first main\ncontribution is to show that under time-varying bounded delays, the delayed SA\nupdate rule guarantees exponentially fast convergence of the \\emph{last\niterate} to a ball around the SA operator's fixed point. Notably, our bound is\n\\emph{tight} in its dependence on both the maximum delay $\\tau_{max}$, and the\nmixing time $\\tau_{mix}$. To achieve this tight bound, we develop a novel\ninductive proof technique that, unlike various existing delayed-optimization\nanalyses, relies on establishing uniform boundedness of the iterates. As such,\nour proof may be of independent interest. Next, to mitigate the impact of the\nmaximum delay on the convergence rate, we provide the first finite-time\nanalysis of a delay-adaptive SA scheme under Markovian sampling. In particular,\nwe show that the exponent of convergence of this scheme gets scaled down by\n$\\tau_{avg}$, as opposed to $\\tau_{max}$ for the vanilla delayed SA rule; here,\n$\\tau_{avg}$ denotes the average delay across all iterations. Moreover, the\nadaptive scheme requires no prior knowledge of the delay sequence for step-size\ntuning. Our theoretical findings shed light on the finite-time effects of\ndelays for a broad class of algorithms, including TD learning, Q-learning, and\nstochastic gradient descent under Markovian sampling.\n","authors":["Arman Adibi","Nicolo Dal Fabbro","Luca Schenato","Sanjeev Kulkarni","H. Vincent Poor","George J. Pappas","Hamed Hassani","Aritra Mitra"],"pdf_url":"https://arxiv.org/pdf/2402.11800v3.pdf","comment":"Accepted to the 27th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2024!"},{"id":"http://arxiv.org/abs/2403.18699v1","updated":"2024-03-27T15:48:16Z","published":"2024-03-27T15:48:16Z","title":"Contrastive Learning with Orthonormal Anchors (CLOA)","summary":"  This study focuses on addressing the instability issues prevalent in\ncontrastive learning, specifically examining the InfoNCE loss function and its\nderivatives. We reveal a critical observation that these loss functions exhibit\na restrictive behavior, leading to a convergence phenomenon where embeddings\ntend to merge into a singular point. This \"over-fusion\" effect detrimentally\naffects classification accuracy in subsequent supervised-learning tasks.\nThrough theoretical analysis, we demonstrate that embeddings, when equalized or\nconfined to a rank-1 linear subspace, represent a local minimum for InfoNCE. In\nresponse to this challenge, our research introduces an innovative strategy that\nleverages the same or fewer labeled data than typically used in the fine-tuning\nphase. The loss we proposed, Orthonormal Anchor Regression Loss, is designed to\ndisentangle embedding clusters, significantly enhancing the distinctiveness of\neach embedding while simultaneously ensuring their aggregation into dense,\nwell-defined clusters. Our method demonstrates remarkable improvements with\njust a fraction of the conventional label requirements, as evidenced by our\nresults on CIFAR10 and CIFAR100 datasets.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18699v1.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2303.12091v3","updated":"2024-03-27T15:44:25Z","published":"2023-03-21T09:07:15Z","title":"Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised learning (SSL) methods assume that labeled data, unlabeled\ndata and test data are from the same distribution. Open-set semi-supervised\nlearning (Open-set SSL) considers a more practical scenario, where unlabeled\ndata and test data contain new categories (outliers) not observed in labeled\ndata (inliers). Most previous works focused on outlier detection via binary\nclassifiers, which suffer from insufficient scalability and inability to\ndistinguish different types of uncertainty. In this paper, we propose a novel\nframework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these\nlimitations. Concretely, we first introduce evidential deep learning (EDL) as\nan outlier detector to quantify different types of uncertainty, and design\ndifferent uncertainty metrics for self-training and inference. Furthermore, we\npropose a novel adaptive negative optimization strategy, making EDL more\ntailored to the unlabeled dataset containing both inliers and outliers. As\ndemonstrated empirically, our proposed method outperforms existing\nstate-of-the-art methods across four datasets.\n","authors":["Yang Yu","Danruo Deng","Furui Liu","Yueming Jin","Qi Dou","Guangyong Chen","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2303.12091v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2403.18690v1","updated":"2024-03-27T15:41:23Z","published":"2024-03-27T15:41:23Z","title":"Annolid: Annotate, Segment, and Track Anything You Need","summary":"  Annolid is a deep learning-based software package designed for the\nsegmentation, labeling, and tracking of research targets within video files,\nfocusing primarily on animal behavior analysis. Based on state-of-the-art\ninstance segmentation methods, Annolid now harnesses the Cutie video object\nsegmentation model to achieve resilient, markerless tracking of multiple\nanimals from single annotated frames, even in environments in which they may be\npartially or entirely concealed by environmental features or by one another.\nOur integration of Segment Anything and Grounding-DINO strategies additionally\nenables the automatic masking and segmentation of recognizable animals and\nobjects by text command, removing the need for manual annotation. Annolid's\ncomprehensive approach to object segmentation flexibly accommodates a broad\nspectrum of behavior analysis applications, enabling the classification of\ndiverse behavioral states such as freezing, digging, pup huddling, and social\ninteractions in addition to the tracking of animals and their body parts.\n","authors":["Chen Yang","Thomas A. Cleland"],"pdf_url":"https://arxiv.org/pdf/2403.18690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18681v1","updated":"2024-03-27T15:24:54Z","published":"2024-03-27T15:24:54Z","title":"TransFusion: Contrastive Learning with Transformers","summary":"  This paper proposes a novel framework, TransFusion, designed to make the\nprocess of contrastive learning more analytical and explainable. TransFusion\nconsists of attention blocks whose softmax being replaced by ReLU, and its\nfinal block's weighted-sum operation is truncated to leave the adjacency matrix\nas the output. The model is trained by minimizing the Jensen-Shannon Divergence\nbetween its output and the target affinity matrix, which indicates whether each\npair of samples belongs to the same or different classes. The main contribution\nof TransFusion lies in defining a theoretical limit for answering two\nfundamental questions in the field: the maximum level of data augmentation and\nthe minimum batch size required for effective contrastive learning.\nFurthermore, experimental results indicate that TransFusion successfully\nextracts features that isolate clusters from complex real-world data, leading\nto improved classification accuracy in downstream tasks.\n","authors":["Huanran Li","Daniel Pimentel-Alarcón"],"pdf_url":"https://arxiv.org/pdf/2403.18681v1.pdf","comment":"17 pages, 4 figures,"},{"id":"http://arxiv.org/abs/2403.18668v1","updated":"2024-03-27T15:11:07Z","published":"2024-03-27T15:11:07Z","title":"Aiming for Relevance","summary":"  Vital signs are crucial in intensive care units (ICUs). They are used to\ntrack the patient's state and to identify clinically significant changes.\nPredicting vital sign trajectories is valuable for early detection of adverse\nevents. However, conventional machine learning metrics like RMSE often fail to\ncapture the true clinical relevance of such predictions. We introduce novel\nvital sign prediction performance metrics that align with clinical contexts,\nfocusing on deviations from clinical norms, overall trends, and trend\ndeviations. These metrics are derived from empirical utility curves obtained in\na previous study through interviews with ICU clinicians. We validate the\nmetrics' usefulness using simulated and real clinical datasets (MIMIC and\neICU). Furthermore, we employ these metrics as loss functions for neural\nnetworks, resulting in models that excel in predicting clinically significant\nevents. This research paves the way for clinically relevant machine learning\nmodel evaluation and optimization, promising to improve ICU patient care. 10\npages, 9 figures.\n","authors":["Bar Eini Porat","Danny Eytan","Uri Shalit"],"pdf_url":"https://arxiv.org/pdf/2403.18668v1.pdf","comment":"10 pages, 9 figures, AMIA Informatics 2024"},{"id":"http://arxiv.org/abs/2403.17219v2","updated":"2024-03-27T15:08:31Z","published":"2024-03-25T21:48:22Z","title":"SeSaMe: A Framework to Simulate Self-Reported Ground Truth for Mental\n  Health Sensing Studies","summary":"  Advances in mobile and wearable technologies have enabled the potential to\npassively monitor a person's mental, behavioral, and affective health. These\napproaches typically rely on longitudinal collection of self-reported outcomes,\ne.g., depression, stress, and anxiety, to train machine learning (ML) models.\nHowever, the need to continuously self-report adds a significant burden on the\nparticipants, often resulting in attrition, missing labels, or insincere\nresponses. In this work, we introduce the Scale Scores Simulation using Mental\nModels (SeSaMe) framework to alleviate participants' burden in digital mental\nhealth studies. By leveraging pre-trained large language models (LLMs), SeSaMe\nenables the simulation of participants' responses on psychological scales. In\nSeSaMe, researchers can prompt LLMs with information on participants' internal\nbehavioral dispositions, enabling LLMs to construct mental models of\nparticipants to simulate their responses on psychological scales. We\ndemonstrate an application of SeSaMe, where we use GPT-4 to simulate responses\non one scale using responses from another as behavioral information. We also\nevaluate the alignment between human and SeSaMe-simulated responses to\npsychological scales. Then, we present experiments to inspect the utility of\nSeSaMe-simulated responses as ground truth in training ML models by replicating\nestablished depression and anxiety screening tasks from a previous study. Our\nresults indicate SeSaMe to be a promising approach, but its alignment may vary\nacross scales and specific prediction objectives. We also observed that model\nperformance with simulated data was on par with using the real data for\ntraining in most evaluation scenarios. We conclude by discussing the potential\nimplications of SeSaMe in addressing some challenges researchers face with\nground-truth collection in passive sensing studies.\n","authors":["Akshat Choube","Vedant Das Swain","Varun Mishra"],"pdf_url":"https://arxiv.org/pdf/2403.17219v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18659v1","updated":"2024-03-27T15:03:33Z","published":"2024-03-27T15:03:33Z","title":"INEXA: Interactive and Explainable Process Model Abstraction Through\n  Object-Centric Process Mining","summary":"  Process events are recorded by multiple information systems at different\ngranularity levels. Based on the resulting event logs, process models are\ndiscovered at different granularity levels, as well. Events stored at a\nfine-grained granularity level, for example, may hinder the discovered process\nmodel to be displayed due the high number of resulting model elements. The\ndiscovered process model of a real-world manufacturing process, for example,\nconsists of 1,489 model elements and over 2,000 arcs. Existing process model\nabstraction techniques could help reducing the size of the model, but would\ndisconnect it from the underlying event log. Existing event abstraction\ntechniques do neither support the analysis of mixed granularity levels, nor\ninteractive exploration of a suitable granularity level. To enable the\nexploration of discovered process models at different granularity levels, we\npropose INEXA, an interactive, explainable process model abstraction method\nthat keeps the link to the event log. As a starting point, INEXA aggregates\nlarge process models to a \"displayable\" size, e.g., for the manufacturing use\ncase to a process model with 58 model elements. Then, the process analyst can\nexplore granularity levels interactively, while applied abstractions are\nautomatically traced in the event log for explainability.\n","authors":["Janik-Vasily Benzin","Gyunam Park","Juergen Mangler","Stefanie Rinderle-Ma"],"pdf_url":"https://arxiv.org/pdf/2403.18659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13374v3","updated":"2024-03-27T14:57:54Z","published":"2024-03-20T08:15:08Z","title":"Byzantine-resilient Federated Learning With Adaptivity to Data\n  Heterogeneity","summary":"  This paper deals with federated learning (FL) in the presence of malicious\nByzantine attacks and data heterogeneity. A novel Robust Average Gradient\nAlgorithm (RAGA) is proposed, which leverages the geometric median for\naggregation and can freely select the round number for local updating.\nDifferent from most existing resilient approaches, which perform convergence\nanalysis based on strongly-convex loss function or homogeneously distributed\ndataset, we conduct convergence analysis for not only strongly-convex but also\nnon-convex loss function over heterogeneous dataset. According to our\ntheoretical analysis, as long as the fraction of dataset from malicious users\nis less than half, RAGA can achieve convergence at rate\n$\\mathcal{O}({1}/{T^{2/3- \\delta}})$ where $T$ is the iteration number and\n$\\delta \\in (0, 2/3)$ for non-convex loss function, and at linear rate for\nstrongly-convex loss function. Moreover, stationary point or global optimal\nsolution is proved to obtainable as data heterogeneity vanishes. Experimental\nresults corroborate the robustness of RAGA to Byzantine attacks and verifies\nthe advantage of RAGA over baselines on convergence performance under various\nintensity of Byzantine attacks, for heterogeneous dataset.\n","authors":["Shiyuan Zuo","Xingrun Yan","Rongfei Fan","Han Hu","Hangguan Shan","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2403.13374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17251v2","updated":"2024-03-27T14:48:48Z","published":"2023-03-30T09:29:53Z","title":"Demystifying Misconceptions in Social Bots Research","summary":"  Research on social bots aims at advancing knowledge and providing solutions\nto one of the most debated forms of online manipulation. Yet, social bot\nresearch is plagued by widespread biases, hyped results, and misconceptions\nthat set the stage for ambiguities, unrealistic expectations, and seemingly\nirreconcilable findings. Overcoming such issues is instrumental towards\nensuring reliable solutions and reaffirming the validity of the scientific\nmethod. In this contribution, we review some recent results in social bots\nresearch, highlighting and revising factual errors as well as methodological\nand conceptual biases. More importantly, we demystify common misconceptions,\naddressing fundamental points on how social bots research is discussed. Our\nanalysis surfaces the need to discuss research about online disinformation and\nmanipulation in a rigorous, unbiased, and responsible way. This article\nbolsters such effort by identifying and refuting common fallacious arguments\nused by both proponents and opponents of social bots research, as well as\nproviding directions toward sound methodologies for future research in the\nfield.\n","authors":["Stefano Cresci","Kai-Cheng Yang","Angelo Spognardi","Roberto Di Pietro","Filippo Menczer","Marinella Petrocchi"],"pdf_url":"https://arxiv.org/pdf/2303.17251v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16451v3","updated":"2024-03-27T14:36:21Z","published":"2024-03-25T06:30:54Z","title":"DeepMachining: Online Prediction of Machining Errors of Lathe Machines","summary":"  We describe DeepMachining, a deep learning-based AI system for online\nprediction of machining errors of lathe machine operations. We have built and\nevaluated DeepMachining based on manufacturing data from factories.\nSpecifically, we first pretrain a deep learning model for a given lathe\nmachine's operations to learn the salient features of machining states. Then,\nwe fine-tune the pretrained model to adapt to specific machining tasks. We\ndemonstrate that DeepMachining achieves high prediction accuracy for multiple\ntasks that involve different workpieces and cutting tools. To the best of our\nknowledge, this work is one of the first factory experiments using pre-trained\ndeep-learning models to predict machining errors of lathe machines.\n","authors":["Xiang-Li Lu","Hwai-Jung Hsu","Che-Wei Chou","H. T. Kung","Chen-Hsin Lee"],"pdf_url":"https://arxiv.org/pdf/2403.16451v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13284v2","updated":"2024-03-27T14:30:44Z","published":"2024-02-19T09:07:59Z","title":"Structure Guided Large Language Model for SQL Generation","summary":"  Generating accurate Structured Querying Language (SQL) is a long-standing\nproblem, especially in matching users' semantic queries with structured\ndatabases and then generating structured SQL. Existing models typically input\nqueries and database schemas into the LLM and rely on the LLM to perform\nsemantic-structure matching and generate structured SQL. However, such\nsolutions overlook the structural information within user queries and\ndatabases, which can be utilized to enhance the generation of structured SQL.\nThis oversight can lead to inaccurate or unexecutable SQL generation. To fully\nexploit the structure, we propose a structure-to-SQL framework, which leverages\nthe inherent structure information to improve the SQL generation of LLMs.\nSpecifically, we introduce our Structure Guided SQL~(SGU-SQL) generation model.\nSGU-SQL first links user queries and databases in a structure-enhanced manner.\nIt then decomposes complicated linked structures with grammar trees to guide\nthe LLM to generate the SQL step by step. Extensive experiments on two\nbenchmark datasets illustrate that SGU-SQL can outperform sixteen SQL\ngeneration baselines.\n","authors":["Qinggang Zhang","Junnan Dong","Hao Chen","Wentao Li","Feiran Huang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2402.13284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18607v1","updated":"2024-03-27T14:25:02Z","published":"2024-03-27T14:25:02Z","title":"Spikewhisper: Temporal Spike Backdoor Attacks on Federated Neuromorphic\n  Learning over Low-power Devices","summary":"  Federated neuromorphic learning (FedNL) leverages event-driven spiking neural\nnetworks and federated learning frameworks to effectively execute intelligent\nanalysis tasks over amounts of distributed low-power devices but also perform\nvulnerability to poisoning attacks. The threat of backdoor attacks on\ntraditional deep neural networks typically comes from time-invariant data.\nHowever, in FedNL, unknown threats may be hidden in time-varying spike signals.\nIn this paper, we start to explore a novel vulnerability of FedNL-based systems\nwith the concept of time division multiplexing, termed Spikewhisper, which\nallows attackers to evade detection as much as possible, as multiple malicious\nclients can imperceptibly poison with different triggers at different\ntimeslices. In particular, the stealthiness of Spikewhisper is derived from the\ntime-domain divisibility of global triggers, in which each malicious client\npastes only one local trigger to a certain timeslice in the neuromorphic\nsample, and also the polarity and motion of each local trigger can be\nconfigured by attackers. Extensive experiments based on two different\nneuromorphic datasets demonstrate that the attack success rate of Spikewispher\nis higher than the temporally centralized attacks. Besides, it is validated\nthat the effect of Spikewispher is sensitive to the trigger duration.\n","authors":["Hanqing Fu","Gaolei Li","Jun Wu","Jianhua Li","Xi Lin","Kai Zhou","Yuchen Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18600v1","updated":"2024-03-27T14:22:40Z","published":"2024-03-27T14:22:40Z","title":"RAP: Retrieval-Augmented Planner for Adaptive Procedure Planning in\n  Instructional Videos","summary":"  Procedure Planning in instructional videos entails generating a sequence of\naction steps based on visual observations of the initial and target states.\nDespite the rapid progress in this task, there remain several critical\nchallenges to be solved: (1) Adaptive procedures: Prior works hold an\nunrealistic assumption that the number of action steps is known and fixed,\nleading to non-generalizable models in real-world scenarios where the sequence\nlength varies. (2) Temporal relation: Understanding the step temporal relation\nknowledge is essential in producing reasonable and executable plans. (3)\nAnnotation cost: Annotating instructional videos with step-level labels (i.e.,\ntimestamp) or sequence-level labels (i.e., action category) is demanding and\nlabor-intensive, limiting its generalizability to large-scale datasets.In this\nwork, we propose a new and practical setting, called adaptive procedure\nplanning in instructional videos, where the procedure length is not fixed or\npre-determined. To address these challenges we introduce Retrieval-Augmented\nPlanner (RAP) model. Specifically, for adaptive procedures, RAP adaptively\ndetermines the conclusion of actions using an auto-regressive model\narchitecture. For temporal relation, RAP establishes an external memory module\nto explicitly retrieve the most relevant state-action pairs from the training\nvideos and revises the generated procedures. To tackle high annotation cost,\nRAP utilizes a weakly-supervised learning manner to expand the training dataset\nto other task-relevant, unannotated videos by generating pseudo labels for\naction steps. Experiments on CrossTask and COIN benchmarks show the superiority\nof RAP over traditional fixed-length models, establishing it as a strong\nbaseline solution for adaptive procedure planning.\n","authors":["Ali Zare","Yulei Niu","Hammad Ayyubi","Shih-fu Chang"],"pdf_url":"https://arxiv.org/pdf/2403.18600v1.pdf","comment":"23 pages, 6 figures, 12 tables"},{"id":"http://arxiv.org/abs/2403.18593v1","updated":"2024-03-27T14:18:09Z","published":"2024-03-27T14:18:09Z","title":"Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote\n  Sensing Image Understanding","summary":"  The tokenizer, as one of the fundamental components of large models, has long\nbeen overlooked or even misunderstood in visual tasks. One key factor of the\ngreat comprehension power of the large language model is that natural language\ntokenizers utilize meaningful words or subwords as the basic elements of\nlanguage. In contrast, mainstream visual tokenizers, represented by patch-based\nmethods such as Patch Embed, rely on meaningless rectangular patches as basic\nelements of vision, which cannot serve as effectively as words or subwords in\nlanguage. Starting from the essence of the tokenizer, we defined semantically\nindependent regions (SIRs) for vision. We designed a simple HOmogeneous visual\ntOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception\nModule (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity,\nthe OPM splits the image into 4*4 pixel seeds and then utilizes the attention\nmechanism to perceive SIRs. The OVM employs cross-attention to merge seeds\nwithin the same SIR. To achieve adaptability, the OVM defines a variable number\nof learnable vectors as cross-attention queries, allowing for the adjustment of\ntoken quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19\nclassification dataset, and GID5 segmentation dataset for sparse and dense\ntasks. The results demonstrate that the visual tokens obtained by HOOK\ncorrespond to individual objects, which demonstrates homogeneity. HOOK\noutperformed Patch Embed by 6\\% and 10\\% in the two tasks and achieved\nstate-of-the-art performance compared to the baselines used for comparison.\nCompared to Patch Embed, which requires more than one hundred tokens for one\nimage, HOOK requires only 6 and 8 tokens for sparse and dense tasks,\nrespectively, resulting in efficiency improvements of 1.5 to 2.8 times. The\ncode is available at https://github.com/GeoX-Lab/Hook.\n","authors":["Run Shao","Zhaoyang Zhang","Chao Tao","Yunsheng Zhang","Chengli Peng","Haifeng Li"],"pdf_url":"https://arxiv.org/pdf/2403.18593v1.pdf","comment":"20 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2306.09459v3","updated":"2024-03-27T14:02:58Z","published":"2023-06-15T19:29:08Z","title":"Recurrent Action Transformer with Memory","summary":"  Recently, the use of transformers in offline reinforcement learning has\nbecome a rapidly developing area. This is due to their ability to treat the\nagent's trajectory in the environment as a sequence, thereby reducing the\npolicy learning problem to sequence modeling. In environments where the agent's\ndecisions depend on past events, it is essential to capture both the event\nitself and the decision point in the context of the model. However, the\nquadratic complexity of the attention mechanism limits the potential for\ncontext expansion. One solution to this problem is to enhance transformers with\nmemory mechanisms. In this paper, we propose the Recurrent Action Transformer\nwith Memory (RATE) - a model that incorporates recurrent memory. To evaluate\nour model, we conducted extensive experiments on both memory-intensive\nenvironments (VizDoom-Two-Color, T-Maze) and classic Atari games and MuJoCo\ncontrol environments. The results show that the use of memory can significantly\nimprove performance in memory-intensive environments while maintaining or\nimproving results in classic environments. We hope that our findings will\nstimulate research on memory mechanisms for transformers applicable to offline\nreinforcement learning.\n","authors":["Alexey Staroverov","Egor Cherepanov","Dmitry Yudin","Alexey K. Kovalev","Aleksandr I. Panov"],"pdf_url":"https://arxiv.org/pdf/2306.09459v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2309.11427v2","updated":"2024-03-27T14:02:57Z","published":"2023-09-20T16:01:45Z","title":"Generative Pre-Training of Time-Series Data for Unsupervised Fault\n  Detection in Semiconductor Manufacturing","summary":"  This paper introduces TRACE-GPT, which stands for Time-seRies\nAnomaly-detection with Convolutional Embedding and Generative Pre-trained\nTransformers. TRACE-GPT is designed to pre-train univariate time-series sensor\ndata and detect faults on unlabeled datasets in semiconductor manufacturing. In\nsemiconductor industry, classifying abnormal time-series sensor data from\nnormal data is important because it is directly related to wafer defect.\nHowever, small, unlabeled, and even mixed training data without enough\nanomalies make classification tasks difficult. In this research, we capture\nfeatures of time-series data with temporal convolutional embedding and\nGenerative Pre-trained Transformer (GPT) to classify abnormal sequences from\nnormal sequences using cross entropy loss. We prove that our model shows better\nperformance than previous unsupervised models with both an open dataset, the\nUniversity of California Riverside (UCR) time-series classification archive,\nand the process log of our Chemical Vapor Deposition (CVD) equipment. Our model\nhas the highest F1 score at Equal Error Rate (EER) across all datasets and is\nonly 0.026 below the supervised state-of-the-art baseline on the open dataset.\n","authors":["Sewoong Lee","JinKyou Choi","Min Su Kim"],"pdf_url":"https://arxiv.org/pdf/2309.11427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09283v3","updated":"2024-03-27T13:55:14Z","published":"2024-02-14T16:14:03Z","title":"Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey","summary":"  Large Language Models (LLMs) are now commonplace in conversation\napplications. However, their risks of misuse for generating harmful responses\nhave raised serious societal concerns and spurred recent research on LLM\nconversation safety. Therefore, in this survey, we provide a comprehensive\noverview of recent studies, covering three critical aspects of LLM conversation\nsafety: attacks, defenses, and evaluations. Our goal is to provide a structured\nsummary that enhances understanding of LLM conversation safety and encourages\nfurther investigation into this important subject. For easy reference, we have\ncategorized all the studies mentioned in this survey according to our taxonomy,\navailable at: https://github.com/niconi19/LLM-conversation-safety.\n","authors":["Zhichen Dong","Zhanhui Zhou","Chao Yang","Jing Shao","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2402.09283v3.pdf","comment":"Accepted to NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18570v1","updated":"2024-03-27T13:51:26Z","published":"2024-03-27T13:51:26Z","title":"Physics-Informed Graph Neural Networks for Water Distribution Systems","summary":"  Water distribution systems (WDS) are an integral part of critical\ninfrastructure which is pivotal to urban development. As 70% of the world's\npopulation will likely live in urban environments in 2050, efficient simulation\nand planning tools for WDS play a crucial role in reaching UN's sustainable\ndevelopmental goal (SDG) 6 - \"Clean water and sanitation for all\". In this\nrealm, we propose a novel and efficient machine learning emulator, more\nprecisely, a physics-informed deep learning (DL) model, for hydraulic state\nestimation in WDS. Using a recursive approach, our model only needs a few graph\nconvolutional neural network (GCN) layers and employs an innovative algorithm\nbased on message passing. Unlike conventional machine learning tasks, the model\nuses hydraulic principles to infer two additional hydraulic state features in\nthe process of reconstructing the available ground truth feature in an\nunsupervised manner. To the best of our knowledge, this is the first DL\napproach to emulate the popular hydraulic simulator EPANET, utilizing no\nadditional information. Like most DL models and unlike the hydraulic simulator,\nour model demonstrates vastly faster emulation times that do not increase\ndrastically with the size of the WDS. Moreover, we achieve high accuracy on the\nground truth and very similar results compared to the hydraulic simulator as\ndemonstrated through experiments on five real-world WDS datasets.\n","authors":["Inaam Ashraf","Janine Strotherm","Luca Hermes","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18570v1.pdf","comment":"Extended version of the paper with the same title published at\n  Proceedings of the AAAI Conference on Artificial Intelligence 2024"},{"id":"http://arxiv.org/abs/2403.18569v1","updated":"2024-03-27T13:50:13Z","published":"2024-03-27T13:50:13Z","title":"PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop\n  Prediction","summary":"  IR drop on the power delivery network (PDN) is closely related to PDN's\nconfiguration and cell current consumption. As the integrated circuit (IC)\ndesign is growing larger, dynamic IR drop simulation becomes computationally\nunaffordable and machine learning based IR drop prediction has been explored as\na promising solution. Although CNN-based methods have been adapted to IR drop\nprediction task in several works, the shortcomings of overlooking PDN\nconfiguration is non-negligible. In this paper, we consider not only how to\nproperly represent cell-PDN relation, but also how to model IR drop following\nits physical nature in the feature aggregation procedure. Thus, we propose a\nnovel graph structure, PDNGraph, to unify the representations of the PDN\nstructure and the fine-grained cell-PDN relation. We further propose a\ndual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN\nbranches to favorably capture the above features during the learning process.\nSeveral key designs are presented to make the dynamic IR drop prediction highly\neffective and interpretable. We are the first work to apply graph structure to\ndeep-learning based dynamic IR drop prediction method. Experiments show that\nPDNNet outperforms the state-of-the-art CNN-based methods by up to 39.3%\nreduction in prediction error and achieves 545x speedup compared to the\ncommercial tool, which demonstrates the superiority of our method.\n","authors":["Yuxiang Zhao","Zhuomin Chai","Xun Jiang","Yibo Lin","Runsheng Wang","Ru Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09700v2","updated":"2024-03-27T13:42:25Z","published":"2024-03-05T22:19:21Z","title":"Shapley Values-Powered Framework for Fair Reward Split in Content\n  Produced by GenAI","summary":"  It is evident that, currently, generative models are surpassed in quality by\nhuman professionals. However, with the advancements in Artificial Intelligence,\nthis gap will narrow, leading to scenarios where individuals who have dedicated\nyears of their lives to mastering a skill become obsolete due to their high\ncosts, which are inherently linked to the time they require to complete a task\n-- a task that AI could accomplish in minutes or seconds. To avoid future\nsocial upheavals, we must, even now, contemplate how to fairly assess the\ncontributions of such individuals in training generative models and how to\ncompensate them for the reduction or complete loss of their incomes. In this\nwork, we propose a method to structure collaboration between model developers\nand data providers. To achieve this, we employ Shapley Values to quantify the\ncontribution of artist(s) in an image generated by the Stable Diffusion-v1.5\nmodel and to equitably allocate the reward among them.\n","authors":["Alex Glinsky","Alexey Sokolsky"],"pdf_url":"https://arxiv.org/pdf/2403.09700v2.pdf","comment":"36 pages, 32 figures"},{"id":"http://arxiv.org/abs/2310.00117v4","updated":"2024-03-27T13:38:00Z","published":"2023-09-29T20:11:15Z","title":"ABScribe: Rapid Exploration & Organization of Multiple Writing\n  Variations in Human-AI Co-Writing Tasks using Large Language Models","summary":"  Exploring alternative ideas by rewriting text is integral to the writing\nprocess. State-of-the-art Large Language Models (LLMs) can simplify writing\nvariation generation. However, current interfaces pose challenges for\nsimultaneous consideration of multiple variations: creating new variations\nwithout overwriting text can be difficult, and pasting them sequentially can\nclutter documents, increasing workload and disrupting writers' flow. To tackle\nthis, we present ABScribe, an interface that supports rapid, yet visually\nstructured, exploration and organization of writing variations in human-AI\nco-writing tasks. With ABScribe, users can swiftly modify variations using LLM\nprompts, which are auto-converted into reusable buttons. Variations are stored\nadjacently within text fields for rapid in-place comparisons using mouse-over\ninteractions on a popup toolbar. Our user study with 12 writers shows that\nABScribe significantly reduces task workload (d = 1.20, p < 0.001), enhances\nuser perceptions of the revision process (d = 2.41, p < 0.001) compared to a\npopular baseline workflow, and provides insights into how writers explore\nvariations using LLMs.\n","authors":["Mohi Reza","Nathan Laundry","Ilya Musabirov","Peter Dushniku","Zhi Yuan \"Michael\" Yu","Kashish Mittal","Tovi Grossman","Michael Liut","Anastasia Kuzminykh","Joseph Jay Williams"],"pdf_url":"https://arxiv.org/pdf/2310.00117v4.pdf","comment":"CHI 2024"},{"id":"http://arxiv.org/abs/2403.18547v1","updated":"2024-03-27T13:25:43Z","published":"2024-03-27T13:25:43Z","title":"Neural Architecture Search for Sentence Classification with BERT","summary":"  Pre training of language models on large text corpora is common practice in\nNatural Language Processing. Following, fine tuning of these models is\nperformed to achieve the best results on a variety of tasks. In this paper we\nquestion the common practice of only adding a single output layer as a\nclassification head on top of the network. We perform an AutoML search to find\narchitectures that outperform the current single layer at only a small compute\ncost. We validate our classification architecture on a variety of NLP\nbenchmarks from the GLUE dataset.\n","authors":["Philip Kenneweg","Sarah Schröder","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18546v1","updated":"2024-03-27T13:24:58Z","published":"2024-03-27T13:24:58Z","title":"Efficient Heatmap-Guided 6-Dof Grasp Detection in Cluttered Scenes","summary":"  Fast and robust object grasping in clutter is a crucial component of\nrobotics. Most current works resort to the whole observed point cloud for 6-Dof\ngrasp generation, ignoring the guidance information excavated from global\nsemantics, thus limiting high-quality grasp generation and real-time\nperformance. In this work, we show that the widely used heatmaps are\nunderestimated in the efficiency of 6-Dof grasp generation. Therefore, we\npropose an effective local grasp generator combined with grasp heatmaps as\nguidance, which infers in a global-to-local semantic-to-point way.\nSpecifically, Gaussian encoding and the grid-based strategy are applied to\npredict grasp heatmaps as guidance to aggregate local points into graspable\nregions and provide global semantic information. Further, a novel non-uniform\nanchor sampling mechanism is designed to improve grasp accuracy and diversity.\nBenefiting from the high-efficiency encoding in the image space and focusing on\npoints in local graspable regions, our framework can perform high-quality grasp\ndetection in real-time and achieve state-of-the-art results. In addition, real\nrobot experiments demonstrate the effectiveness of our method with a success\nrate of 94% and a clutter completion rate of 100%. Our code is available at\nhttps://github.com/THU-VCLab/HGGD.\n","authors":["Siang Chen","Wei Tang","Pengwei Xie","Wenming Yang","Guijin Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18546v1.pdf","comment":"Extensive results on GraspNet-1B dataset"},{"id":"http://arxiv.org/abs/2403.18537v1","updated":"2024-03-27T13:12:57Z","published":"2024-03-27T13:12:57Z","title":"A Path Towards Legal Autonomy: An interoperable and explainable approach\n  to extracting, transforming, loading and computing legal information using\n  large language models, expert systems and Bayesian networks","summary":"  Legal autonomy - the lawful activity of artificial intelligence agents - can\nbe achieved in one of two ways. It can be achieved either by imposing\nconstraints on AI actors such as developers, deployers and users, and on AI\nresources such as data, or by imposing constraints on the range and scope of\nthe impact that AI agents can have on the environment. The latter approach\ninvolves encoding extant rules concerning AI driven devices into the software\nof AI agents controlling those devices (e.g., encoding rules about limitations\non zones of operations into the agent software of an autonomous drone device).\nThis is a challenge since the effectivity of such an approach requires a method\nof extracting, loading, transforming and computing legal information that would\nbe both explainable and legally interoperable, and that would enable AI agents\nto reason about the law. In this paper, we sketch a proof of principle for such\na method using large language models (LLMs), expert legal systems known as\nlegal decision paths, and Bayesian networks. We then show how the proposed\nmethod could be applied to extant regulation in matters of autonomous cars,\nsuch as the California Vehicle Code.\n","authors":["Axel Constant","Hannes Westermann","Bryan Wilson","Alex Kiefer","Ines Hipolito","Sylvain Pronovost","Steven Swanson","Mahault Albarracin","Maxwell J. D. Ramstead"],"pdf_url":"https://arxiv.org/pdf/2403.18537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18536v1","updated":"2024-03-27T13:12:41Z","published":"2024-03-27T13:12:41Z","title":"A Novel Behavior-Based Recommendation System for E-commerce","summary":"  The majority of existing recommender systems rely on user ratings, which are\nlimited by the lack of user collaboration and the sparsity problem. To address\nthese issues, this study proposes a behavior-based recommender system that\nleverages customers' natural behaviors, such as browsing and clicking, on\ne-commerce platforms. The proposed recommendation system involves clustering\nactive customers, determining neighborhoods, collecting similar users,\ncalculating product reputation based on similar users, and recommending\nhigh-reputation products. To overcome the complexity of customer behaviors and\ntraditional clustering methods, an unsupervised clustering approach based on\nproduct categories is developed to enhance the recommendation methodology. This\nstudy makes notable contributions in several aspects. Firstly, a groundbreaking\nbehavior-based recommendation methodology is developed, incorporating customer\nbehavior to generate accurate and tailored recommendations leading to improved\ncustomer satisfaction and engagement. Secondly, an original unsupervised\nclustering method, focusing on product categories, enables more precise\nclustering and facilitates accurate recommendations. Finally, an approach to\ndetermine neighborhoods for active customers within clusters is established,\nensuring grouping of customers with similar behavioral patterns to enhance\nrecommendation accuracy and relevance. The proposed recommendation methodology\nand clustering method contribute to improved recommendation performance,\noffering valuable insights for researchers and practitioners in the field of\ne-commerce recommendation systems. Additionally, the proposed method\noutperforms benchmark methods in experiments conducted using a behavior dataset\nfrom the well-known e-commerce site Alibaba.\n","authors":["Reza Barzegar Nozari","Mahdi Divsalar","Sepehr Akbarzadeh Abkenar","Mohammadreza Fadavi Amiri","Ali Divsalar"],"pdf_url":"https://arxiv.org/pdf/2403.18536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10365v3","updated":"2024-03-27T12:53:12Z","published":"2023-03-18T08:48:16Z","title":"CroSel: Cross Selection of Confident Pseudo Labels for Partial-Label\n  Learning","summary":"  Partial-label learning (PLL) is an important weakly supervised learning\nproblem, which allows each training example to have a candidate label set\ninstead of a single ground-truth label. Identification-based methods have been\nwidely explored to tackle label ambiguity issues in PLL, which regard the true\nlabel as a latent variable to be identified. However, identifying the true\nlabels accurately and completely remains challenging, causing noise in pseudo\nlabels during model training. In this paper, we propose a new method called\nCroSel, which leverages historical predictions from the model to identify true\nlabels for most training examples. First, we introduce a cross selection\nstrategy, which enables two deep models to select true labels of partially\nlabeled data for each other. Besides, we propose a novel consistency\nregularization term called co-mix to avoid sample waste and tiny noise caused\nby false selection. In this way, CroSel can pick out the true labels of most\nexamples with high precision. Extensive experiments demonstrate the superiority\nof CroSel, which consistently outperforms previous state-of-the-art methods on\nbenchmark datasets. Additionally, our method achieves over 90\\% accuracy and\nquantity for selecting true labels on CIFAR-type datasets under various\nsettings.\n","authors":["Shiyu Tian","Hongxin Wei","Yiqun Wang","Lei Feng"],"pdf_url":"https://arxiv.org/pdf/2303.10365v3.pdf","comment":"Accepted by CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18519v1","updated":"2024-03-27T12:50:27Z","published":"2024-03-27T12:50:27Z","title":"Improving Line Search Methods for Large Scale Neural Network Training","summary":"  In recent studies, line search methods have shown significant improvements in\nthe performance of traditional stochastic gradient descent techniques,\neliminating the need for a specific learning rate schedule. In this paper, we\nidentify existing issues in state-of-the-art line search methods, propose\nenhancements, and rigorously evaluate their effectiveness. We test these\nmethods on larger datasets and more complex data domains than before.\nSpecifically, we improve the Armijo line search by integrating the momentum\nterm from ADAM in its search direction, enabling efficient large-scale\ntraining, a task that was previously prone to failure using Armijo line search\nmethods. Our optimization approach outperforms both the previous Armijo\nimplementation and tuned learning rate schedules for Adam. Our evaluation\nfocuses on Transformers and CNNs in the domains of NLP and image data. Our work\nis publicly available as a Python package, which provides a hyperparameter free\nPytorch optimizer.\n","authors":["Philip Kenneweg","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18519v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18506v1","updated":"2024-03-27T12:35:23Z","published":"2024-03-27T12:35:23Z","title":"Faster Convergence for Transformer Fine-tuning with Line Search Methods","summary":"  Recent works have shown that line search methods greatly increase performance\nof traditional stochastic gradient descent methods on a variety of datasets and\narchitectures [1], [2]. In this work we succeed in extending line search\nmethods to the novel and highly popular Transformer architecture and dataset\ndomains in natural language processing. More specifically, we combine the\nArmijo line search with the Adam optimizer and extend it by subdividing the\nnetworks architecture into sensible units and perform the line search\nseparately on these local units. Our optimization method outperforms the\ntraditional Adam optimizer and achieves significant performance improvements\nfor small data sets or small training budgets, while performing equal or better\nfor other tested cases. Our work is publicly available as a python package,\nwhich provides a hyperparameter-free pytorch optimizer that is compatible with\narbitrary network architectures.\n","authors":["Philip Kenneweg","Leonardo Galli","Tristan Kenneweg","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2403.18506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04698v3","updated":"2024-03-27T12:24:17Z","published":"2023-11-08T14:10:19Z","title":"Challenging Common Paradigms in Multi-Task Learning","summary":"  While multi-task learning (MTL) has gained significant attention in recent\nyears, its underlying mechanisms remain poorly understood. Recent methods did\nnot yield consistent performance improvements over single task learning (STL)\nbaselines, underscoring the importance of gaining more profound insights about\nchallenges specific to MTL. In our study, we challenge paradigms in MTL in the\ncontext of STL: First, the impact of the choice of optimizer has only been\nmildly investigated in MTL. We show the pivotal role of common STL tools such\nas the Adam optimizer in MTL empirically in various experiments. To further\ninvestigate Adam's effectiveness, we theoretical derive a partial loss-scale\ninvariance under mild assumptions. Second, the notion of gradient conflicts has\noften been phrased as a specific problem in MTL. We delve into the role of\ngradient conflicts in MTL and compare it to STL. For angular gradient alignment\nwe find no evidence that this is a unique problem in MTL. We emphasize\ndifferences in gradient magnitude as the main distinguishing factor. Lastly, we\ncompare the transferability of features learned through MTL and STL on common\nimage corruptions, and find light evidence that MTL can lead to superior\ntransferability. Overall, we find surprising similarities between STL and MTL\nsuggesting to consider methods from both fields in a broader context.\n","authors":["Cathrin Elich","Lukas Kirchdorfer","Jan M. Köhler","Lukas Schott"],"pdf_url":"https://arxiv.org/pdf/2311.04698v3.pdf","comment":"-"},{"id":"http://arxiv.org/abs/2403.15114v2","updated":"2024-03-27T12:13:42Z","published":"2024-03-22T11:16:11Z","title":"Solving a Real-World Package Delivery Routing Problem Using Quantum\n  Annealers","summary":"  Research focused on the conjunction between quantum computing and routing\nproblems has been very prolific in recent years. Most of the works revolve\naround classical problems such as the Traveling Salesman Problem or the Vehicle\nRouting Problem. Even though working on these problems is valuable, it is also\nundeniable that their academic-oriented nature falls short of real-world\nrequirements. The main objective of this research is to present a solving\nmethod for realistic instances, avoiding problem relaxations or technical\nshortcuts. Instead, a quantum-classical hybrid solver has been developed,\ncoined Q4RPD, that considers a set of real constraints such as a heterogeneous\nfleet of vehicles, priority deliveries, and capacities characterized by two\nvalues: weight and dimensions of the packages. Q4RPD resorts to the Leap\nConstrained Quadratic Model Hybrid Solver of D-Wave. To demonstrate the\napplication of Q4RPD, an experimentation composed of six different instances\nhas been conducted, aiming to serve as illustrative examples.\n","authors":["Eneko Osaba","Esther Villar-Rodriguez","Antón Asla"],"pdf_url":"https://arxiv.org/pdf/2403.15114v2.pdf","comment":"15 pages, 11 figures and 4 tables. Paper submitted for review in\n  Scientific Reports"},{"id":"http://arxiv.org/abs/2403.18489v1","updated":"2024-03-27T12:01:51Z","published":"2024-03-27T12:01:51Z","title":"Impact of Employing Weather Forecast Data as Input to the Estimation of\n  Evapotranspiration by Deep Neural Network Models","summary":"  Reference Evapotranspiration (ET0) is a key parameter for designing smart\nirrigation scheduling, since it is related by a coefficient to the water needs\nof a crop. The United Nations Food and Agriculture Organization, proposed a\nstandard method for ET0 computation (FAO56PM), based on the parameterization of\nthe Penman-Monteith equation, that is widely adopted in the literature. To\ncompute ET0 using the FAO56-PM method, four main weather parameters are needed:\ntemperature, humidity, wind, and solar radiation (SR). One way to make daily\nET0 estimations for future days is to use freely available weather forecast\nservices (WFSs), where many meteorological parameters are estimated up to the\nnext 15 days. A problem with this method is that currently, SR is not provided\nas a free forecast parameter on most of those online services or, normally,\nsuch forecasts present a financial cost penalty. For this reason, several ET0\nestimation models using machine and deep learning were developed and presented\nin the literature, that use as input features a reduced set of carefully\nselected weather parameters, that are compatible with common freely available\nWFSs. However, most studies on this topic have only evaluated model performance\nusing data from weather stations (WSs), without considering the effect of using\nweather forecast data. In this study, the performance of authors' previous\nmodels is evaluated when using weather forecast data from two online WFSs, in\nthe following scenarios: (i) direct ET0 estimation by an ANN model, and (ii)\nestimate SR by ANN model, and then use that estimation for ET0 computation,\nusing the FAO56-PM method. Employing data collected from two WFSs and a WS\nlocated in Vale do Lobo, Portugal, the latter approach achieved the best\nresult, with a coefficient of determination (R2) ranging between 0.893 and\n0.667, when considering forecasts up to 15 days.\n","authors":["Pedro J. Vaz","Gabriela Schütz","Carlos Guerrero","Pedro J. S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2403.18489v1.pdf","comment":"A partial version of the work submitted to ESRE/INTERNATIONAL\n  CONFERENCE ON ENVIRONMENTAL SCIENCES AND RENEWABLE ENERGY"},{"id":"http://arxiv.org/abs/2403.18486v1","updated":"2024-03-27T11:58:45Z","published":"2024-03-27T11:58:45Z","title":"Synthesizing EEG Signals from Event-Related Potential Paradigms with\n  Conditional Diffusion Models","summary":"  Data scarcity in the brain-computer interface field can be alleviated through\nthe use of generative models, specifically diffusion models. While diffusion\nmodels have previously been successfully applied to electroencephalogram (EEG)\ndata, existing models lack flexibility w.r.t.~sampling or require alternative\nrepresentations of the EEG data. To overcome these limitations, we introduce a\nnovel approach to conditional diffusion models that utilizes classifier-free\nguidance to directly generate subject-, session-, and class-specific EEG data.\nIn addition to commonly used metrics, domain-specific metrics are employed to\nevaluate the specificity of the generated samples. The results indicate that\nthe proposed model can generate EEG data that resembles real data for each\nsubject, session, and class.\n","authors":["Guido Klein","Pierre Guetschel","Gianluigi Silvestri","Michael Tangermann"],"pdf_url":"https://arxiv.org/pdf/2403.18486v1.pdf","comment":"submitted to 9th Graz BCI conference, 6 pages, 3 figures, first\n  figure is split into two subfigures, 1 table"},{"id":"http://arxiv.org/abs/2311.12028v2","updated":"2024-03-27T11:43:28Z","published":"2023-11-20T18:59:51Z","title":"Hourglass Tokenizer for Efficient Transformer-Based 3D Human Pose\n  Estimation","summary":"  Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a plug-and-play pruning-and-recovering framework,\ncalled Hourglass Tokenizer (HoT), for efficient transformer-based 3D human pose\nestimation from videos. Our HoT begins with pruning pose tokens of redundant\nframes and ends with recovering full-length tokens, resulting in a few pose\ntokens in the intermediate transformer blocks and thus improving the model\nefficiency. To effectively achieve this, we propose a token pruning cluster\n(TPC) that dynamically selects a few representative tokens with high semantic\ndiversity while eliminating the redundancy of video frames. In addition, we\ndevelop a token recovering attention (TRA) to restore the detailed\nspatio-temporal information based on the selected tokens, thereby expanding the\nnetwork output to the original full-length temporal resolution for fast\ninference. Extensive experiments on two benchmark datasets (i.e., Human3.6M and\nMPI-INF-3DHP) demonstrate that our method can achieve both high efficiency and\nestimation accuracy compared to the original VPT models. For instance, applying\nto MotionBERT and MixSTE on Human3.6M, our HoT can save nearly 50% FLOPs\nwithout sacrificing accuracy and nearly 40% FLOPs with only 0.2% accuracy drop,\nrespectively. Code and models are available at\nhttps://github.com/NationalGAILab/HoT.\n","authors":["Wenhao Li","Mengyuan Liu","Hong Liu","Pichao Wang","Jialun Cai","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2311.12028v2.pdf","comment":"Accepted by CVPR 2024, Open Sourced"},{"id":"http://arxiv.org/abs/2403.16432v2","updated":"2024-03-27T11:37:58Z","published":"2024-03-25T05:27:35Z","title":"$\\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on\n  Prompt-based Language Models","summary":"  Prompt-based learning is a new language model training paradigm that adapts\nthe Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes\nthe performance benchmarks across various natural language processing (NLP)\ntasks. Instead of using a fixed prompt template to fine-tune the model, some\nresearch demonstrates the effectiveness of searching for the prompt via\noptimization. Such prompt optimization process of prompt-based learning on PLMs\nalso gives insight into generating adversarial prompts to mislead the model,\nraising concerns about the adversarial vulnerability of this paradigm. Recent\nstudies have shown that universal adversarial triggers (UATs) can be generated\nto alter not only the predictions of the target PLMs but also the prediction of\ncorresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based\nlearning paradigm. However, UATs found in previous works are often unreadable\ntokens or characters and can be easily distinguished from natural texts with\nadaptive defenses. In this work, we consider the naturalness of the UATs and\ndevelop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs\nby a gradient-based beam search algorithm that not only effectively attacks the\ntarget PLMs and PFMs but also maintains the naturalness among the trigger\ntokens. Extensive results demonstrate the effectiveness of\n$\\textit{LinkPrompt}$, as well as the transferability of UATs generated by\n$\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and\nAPI-accessed LLM GPT-3.5-turbo.\n","authors":["Yue Xu","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2403.16432v2.pdf","comment":"Accepted to the main conference of NAACL2024"},{"id":"http://arxiv.org/abs/2311.07838v3","updated":"2024-03-27T11:36:46Z","published":"2023-11-14T01:38:02Z","title":"LLatrieval: LLM-Verified Retrieval for Verifiable Generation","summary":"  Verifiable generation aims to let the large language model (LLM) generate\ntext with supporting documents, which enables the user to flexibly verify the\nanswer and makes the LLM's output more reliable. Retrieval plays a crucial role\nin verifiable generation. Specifically, the retrieved documents not only\nsupplement knowledge to help the LLM generate correct answers, but also serve\nas supporting evidence for the user to verify the LLM's output. However, the\nwidely used retrievers become the bottleneck of the entire pipeline and limit\nthe overall performance. Their capabilities are usually inferior to LLMs since\nthey often have much fewer parameters than the large language model and have\nnot been demonstrated to scale well to the size of LLMs. If the retriever does\nnot correctly find the supporting documents, the LLM can not generate the\ncorrect and verifiable answer, which overshadows the LLM's remarkable\nabilities. To address these limitations, we propose \\LLatrieval (Large Language\nModel Verified Retrieval), where the LLM updates the retrieval result until it\nverifies that the retrieved documents can sufficiently support answering the\nquestion. Thus, the LLM can iteratively provide feedback to retrieval and\nfacilitate the retrieval result to fully support verifiable generation.\nExperiments show that LLatrieval significantly outperforms extensive baselines\nand achieves state-of-the-art results.\n","authors":["Xiaonan Li","Changtai Zhu","Linyang Li","Zhangyue Yin","Tianxiang Sun","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2311.07838v3.pdf","comment":"Accepted by NAACL 2024 (Main Conference)"},{"id":"http://arxiv.org/abs/2403.18469v1","updated":"2024-03-27T11:28:57Z","published":"2024-03-27T11:28:57Z","title":"Density-guided Translator Boosts Synthetic-to-Real Unsupervised Domain\n  Adaptive Segmentation of 3D Point Clouds","summary":"  3D synthetic-to-real unsupervised domain adaptive segmentation is crucial to\nannotating new domains. Self-training is a competitive approach for this task,\nbut its performance is limited by different sensor sampling patterns (i.e.,\nvariations in point density) and incomplete training strategies. In this work,\nwe propose a density-guided translator (DGT), which translates point density\nbetween domains, and integrates it into a two-stage self-training pipeline\nnamed DGT-ST. First, in contrast to existing works that simultaneously conduct\ndata generation and feature/output alignment within unstable adversarial\ntraining, we employ the non-learnable DGT to bridge the domain gap at the input\nlevel. Second, to provide a well-initialized model for self-training, we\npropose a category-level adversarial network in stage one that utilizes the\nprototype to prevent negative transfer. Finally, by leveraging the designs\nabove, a domain-mixed self-training method with source-aware consistency loss\nis proposed in stage two to narrow the domain gap further. Experiments on two\nsynthetic-to-real segmentation tasks (SynLiDAR $\\rightarrow$ semanticKITTI and\nSynLiDAR $\\rightarrow$ semanticPOSS) demonstrate that DGT-ST outperforms\nstate-of-the-art methods, achieving 9.4$\\%$ and 4.3$\\%$ mIoU improvements,\nrespectively. Code is available at \\url{https://github.com/yuan-zm/DGT-ST}.\n","authors":["Zhimin Yuan","Wankang Zeng","Yanfei Su","Weiquan Liu","Ming Cheng","Yulan Guo","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18469v1.pdf","comment":"CVPR2024"},{"id":"http://arxiv.org/abs/2311.10522v4","updated":"2024-03-27T11:18:51Z","published":"2023-11-17T13:43:43Z","title":"Enhancing Object Coherence in Layout-to-Image Synthesis","summary":"  Layout-to-image synthesis is an emerging technique in conditional image\ngeneration. It aims to generate complex scenes, where users require fine\ncontrol over the layout of the objects in a scene. However, it remains\nchallenging to control the object coherence, including semantic coherence\n(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the\nhand and the racket should not be misaligned). In this paper, we propose a\nnovel diffusion model with effective global semantic fusion (GSF) and\nself-similarity feature enhancement modules to guide the object coherence for\nthis task. For semantic coherence, we argue that the image caption contains\nrich information for defining the semantic relationship within the objects in\nthe images. Instead of simply employing cross-attention between captions and\ngenerated images, which addresses the highly relevant layout restriction and\nsemantic coherence separately and thus leads to unsatisfying results shown in\nour experiments, we develop GSF to fuse the supervision from the layout\nrestriction and semantic coherence requirement and exploit it to guide the\nimage synthesis process. Moreover, to improve the physical coherence, we\ndevelop a Self-similarity Coherence Attention (SCA) module to explicitly\nintegrate local contextual physical coherence into each pixel's generation\nprocess. Specifically, we adopt a self-similarity map to encode the coherence\nrestrictions and employ it to extract coherent features from text embedding.\nThrough visualization of our self-similarity map, we explore the essence of\nSCA, revealing that its effectiveness is not only in capturing reliable\nphysical coherence patterns but also in enhancing complex texture generation.\nExtensive experiments demonstrate the superiority of our proposed method in\nboth image generation quality and controllability.\n","authors":["Yibin Wang","Weizhong Zhang","Jianwei Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2311.10522v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18459v1","updated":"2024-03-27T11:18:01Z","published":"2024-03-27T11:18:01Z","title":"CoBOS: Constraint-Based Online Scheduler for Human-Robot Collaboration","summary":"  Assembly processes involving humans and robots are challenging scenarios\nbecause the individual activities and access to shared workspace have to be\ncoordinated. Fixed robot programs leave no room to diverge from a fixed\nprotocol. Working on such a process can be stressful for the user and lead to\nineffective behavior or failure. We propose a novel approach of online\nconstraint-based scheduling in a reactive execution control framework\nfacilitating behavior trees called CoBOS. This allows the robot to adapt to\nuncertain events such as delayed activity completions and activity selection\n(by the human). The user will experience less stress as the robotic coworkers\nadapt their behavior to best complement the human-selected activities to\ncomplete the common task. In addition to the improved working conditions, our\nalgorithm leads to increased efficiency, even in highly uncertain scenarios. We\nevaluate our algorithm using a probabilistic simulation study with 56000\nexperiments. We outperform all baselines by a margin of 4-10%. Initial real\nrobot experiments using a Franka Emika Panda robot and human tracking based on\nHTC Vive VR gloves look promising.\n","authors":["Marina Ionova","Jan Kristof Behrens"],"pdf_url":"https://arxiv.org/pdf/2403.18459v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2403.18451v1","updated":"2024-03-27T11:11:06Z","published":"2024-03-27T11:11:06Z","title":"CoRAST: Towards Foundation Model-Powered Correlated Data Analysis in\n  Resource-Constrained CPS and IoT","summary":"  Foundation models (FMs) emerge as a promising solution to harness distributed\nand diverse environmental data by leveraging prior knowledge to understand the\ncomplicated temporal and spatial correlations within heterogeneous datasets.\nUnlike distributed learning frameworks such as federated learning, which often\nstruggle with multimodal data, FMs can transform diverse inputs into\nembeddings. This process facilitates the integration of information from\nvarious modalities and the application of prior learning to new domains.\nHowever, deploying FMs in resource-constrained edge systems poses significant\nchallenges. To this end, we introduce CoRAST, a novel learning framework that\nutilizes FMs for enhanced analysis of distributed, correlated heterogeneous\ndata. Utilizing a server-based FM, CoRAST can exploit existing environment\ninformation to extract temporal, spatial, and cross-modal correlations among\nsensor data. This enables CoRAST to offer context-aware insights for localized\nclient tasks through FM-powered global representation learning. Our evaluation\non real-world weather dataset demonstrates CoRAST's ability to exploit\ncorrelated heterogeneous data through environmental representation learning to\nreduce the forecast errors by up to 50.3% compared to the baselines.\n","authors":["Yi Hu","Jinhang Zuo","Alanis Zhao","Bob Iannucci","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.18451v1.pdf","comment":"accepted and to be published in 2024 IEEE International Workshop on\n  Foundation Models for Cyber-Physical Systems & Internet of Things (FMSys)"},{"id":"http://arxiv.org/abs/2403.18425v1","updated":"2024-03-27T10:26:42Z","published":"2024-03-27T10:26:42Z","title":"U-Sketch: An Efficient Approach for Sketch to Image Diffusion Models","summary":"  Diffusion models have demonstrated remarkable performance in text-to-image\nsynthesis, producing realistic and high resolution images that faithfully\nadhere to the corresponding text-prompts. Despite their great success, they\nstill fall behind in sketch-to-image synthesis tasks, where in addition to\ntext-prompts, the spatial layout of the generated images has to closely follow\nthe outlines of certain reference sketches. Employing an MLP latent edge\npredictor to guide the spatial layout of the synthesized image by predicting\nedge maps at each denoising step has been recently proposed. Despite yielding\npromising results, the pixel-wise operation of the MLP does not take into\naccount the spatial layout as a whole, and demands numerous denoising\niterations to produce satisfactory images, leading to time inefficiency. To\nthis end, we introduce U-Sketch, a framework featuring a U-Net type latent edge\npredictor, which is capable of efficiently capturing both local and global\nfeatures, as well as spatial correlations between pixels. Moreover, we propose\nthe addition of a sketch simplification network that offers the user the choice\nof preprocessing and simplifying input sketches for enhanced outputs. The\nexperimental results, corroborated by user feedback, demonstrate that our\nproposed U-Net latent edge predictor leads to more realistic results, that are\nbetter aligned with the spatial outlines of the reference sketches, while\ndrastically reducing the number of required denoising steps and, consequently,\nthe overall execution time.\n","authors":["Ilias Mitsouras","Eleftherios Tsonis","Paraskevi Tzouveli","Athanasios Voulodimos"],"pdf_url":"https://arxiv.org/pdf/2403.18425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01739v2","updated":"2024-03-27T10:21:24Z","published":"2024-01-29T12:05:02Z","title":"OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models","summary":"  To help the open-source community have a better understanding of\nMixture-of-Experts (MoE) based large language models (LLMs), we train and\nrelease OpenMoE, a series of fully open-sourced and reproducible decoder-only\nMoE LLMs, ranging from 650M to 34B parameters and trained on up to over 1T\ntokens. Our investigation confirms that MoE-based LLMs can offer a more\nfavorable cost-effectiveness trade-off than dense LLMs, highlighting the\npotential effectiveness for future LLM development.\n  One more important contribution of this study is an in-depth analysis of the\nrouting mechanisms within our OpenMoE models, leading to three significant\nfindings: Context-Independent Specialization, Early Routing Learning, and\nDrop-towards-the-End. We discovered that routing decisions in MoE models are\npredominantly based on token IDs, with minimal context relevance. The\ntoken-to-expert assignments are determined early in the pre-training phase and\nremain largely unchanged. This imperfect routing can result in performance\ndegradation, particularly in sequential tasks like multi-turn conversations,\nwhere tokens appearing later in a sequence are more likely to be dropped.\nFinally, we rethink our design based on the above-mentioned observations and\nanalysis. To facilitate future MoE LLM development, we propose potential\nstrategies for mitigating the issues we found and further improving\noff-the-shelf MoE LLM designs.\n","authors":["Fuzhao Xue","Zian Zheng","Yao Fu","Jinjie Ni","Zangwei Zheng","Wangchunshu Zhou","Yang You"],"pdf_url":"https://arxiv.org/pdf/2402.01739v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18421v1","updated":"2024-03-27T10:18:21Z","published":"2024-03-27T10:18:21Z","title":"BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text","summary":"  Models such as GPT-4 and Med-PaLM 2 have demonstrated impressive performance\non a wide variety of biomedical NLP tasks. However, these models have hundreds\nof billions of parameters, are computationally expensive to run, require users\nto send their input data over the internet, and are trained on unknown data\nsources. Can smaller, more targeted models compete? To address this question,\nwe build and release BioMedLM, a 2.7 billion parameter GPT-style autoregressive\nmodel trained exclusively on PubMed abstracts and full articles. When\nfine-tuned, BioMedLM can produce strong multiple-choice biomedical\nquestion-answering results competitive with much larger models, such as\nachieving a score of 57.3% on MedMCQA (dev) and 69.0% on the MMLU Medical\nGenetics exam. BioMedLM can also be fine-tuned to produce useful answers to\npatient questions on medical topics. This demonstrates that smaller models can\npotentially serve as transparent, privacy-preserving, economical and\nenvironmentally friendly foundations for particular NLP applications, such as\nin biomedicine. The model is available on the Hugging Face Hub:\nhttps://huggingface.co/stanford-crfm/BioMedLM.\n","authors":["Elliot Bolton","Abhinav Venigalla","Michihiro Yasunaga","David Hall","Betty Xiong","Tony Lee","Roxana Daneshjou","Jonathan Frankle","Percy Liang","Michael Carbin","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2403.18421v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2311.01191v2","updated":"2024-03-27T10:12:31Z","published":"2023-11-02T12:36:19Z","title":"VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node\n  Classification","summary":"  Class imbalance in graph data presents significant challenges for node\nclassification. While existing methods, such as SMOTE-based approaches,\npartially mitigate this issue, they still exhibit limitations in constructing\nimbalanced graphs. Generative self-supervised learning (SSL) methods,\nexemplified by graph autoencoders (GAEs), offer a promising solution by\ndirectly generating minority nodes from the data itself, yet their potential\nremains underexplored. In this paper, we delve into the shortcomings of\nSMOTE-based approaches in the construction of imbalanced graphs. Furthermore,\nwe introduce VIGraph, a simple yet effective generative SSL approach that\nrelies on the Variational GAE as the fundamental model. VIGraph strictly\nadheres to the concept of imbalance when constructing imbalanced graphs and\ninnovatively leverages the variational inference (VI) ability of Variational\nGAE to generate nodes for minority classes. VIGraph introduces comprehensive\ntraining strategies, including cross-view contrastive learning at the decoding\nphase to capture semantic knowledge, adjacency matrix reconstruction to\npreserve graph structure, and alignment strategy to ensure stable training.\nVIGraph can generate high-quality nodes directly usable for classification,\neliminating the need to integrate the generated nodes back to the graph as well\nas additional retraining found in SMOTE-based methods. We conduct extensive\nexperiments, results from which demonstrate the superiority and generality of\nour approach.\n","authors":["Yulan Hu","Sheng Ouyang","Zhirui Yang","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2311.01191v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18407v1","updated":"2024-03-27T09:49:37Z","published":"2024-03-27T09:49:37Z","title":"A Channel-ensemble Approach: Unbiased and Low-variance Pseudo-labels is\n  Critical for Semi-supervised Classification","summary":"  Semi-supervised learning (SSL) is a practical challenge in computer vision.\nPseudo-label (PL) methods, e.g., FixMatch and FreeMatch, obtain the State Of\nThe Art (SOTA) performances in SSL. These approaches employ a\nthreshold-to-pseudo-label (T2L) process to generate PLs by truncating the\nconfidence scores of unlabeled data predicted by the self-training method.\nHowever, self-trained models typically yield biased and high-variance\npredictions, especially in the scenarios when a little labeled data are\nsupplied. To address this issue, we propose a lightweight channel-based\nensemble method to effectively consolidate multiple inferior PLs into the\ntheoretically guaranteed unbiased and low-variance one. Importantly, our\napproach can be readily extended to any SSL framework, such as FixMatch or\nFreeMatch. Experimental results demonstrate that our method significantly\noutperforms state-of-the-art techniques on CIFAR10/100 in terms of\neffectiveness and efficiency.\n","authors":["Jiaqi Wu","Junbiao Pang","Baochang Zhang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2403.18407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18406v1","updated":"2024-03-27T09:48:23Z","published":"2024-03-27T09:48:23Z","title":"An Image Grid Can Be Worth a Video: Zero-shot Video Question Answering\n  Using a VLM","summary":"  Stimulated by the sophisticated reasoning capabilities of recent Large\nLanguage Models (LLMs), a variety of strategies for bridging video modality\nhave been devised. A prominent strategy involves Video Language Models\n(VideoLMs), which train a learnable interface with video data to connect\nadvanced vision encoders with LLMs. Recently, an alternative strategy has\nsurfaced, employing readily available foundation models, such as VideoLMs and\nLLMs, across multiple stages for modality bridging. In this study, we introduce\na simple yet novel strategy where only a single Vision Language Model (VLM) is\nutilized. Our starting point is the plain insight that a video comprises a\nseries of images, or frames, interwoven with temporal information. The essence\nof video comprehension lies in adeptly managing the temporal aspects along with\nthe spatial details of each frame. Initially, we transform a video into a\nsingle composite image by arranging multiple frames in a grid layout. The\nresulting single image is termed as an image grid. This format, while\nmaintaining the appearance of a solitary image, effectively retains temporal\ninformation within the grid structure. Therefore, the image grid approach\nenables direct application of a single high-performance VLM without\nnecessitating any video-data training. Our extensive experimental analysis\nacross ten zero-shot video question answering benchmarks, including five\nopen-ended and five multiple-choice benchmarks, reveals that the proposed Image\nGrid Vision Language Model (IG-VLM) surpasses the existing methods in nine out\nof ten benchmarks.\n","authors":["Wonkyun Kim","Changin Choi","Wonseok Lee","Wonjong Rhee"],"pdf_url":"https://arxiv.org/pdf/2403.18406v1.pdf","comment":"Our code is available at https://github.com/imagegridworth/IG-VLM"},{"id":"http://arxiv.org/abs/2403.18405v1","updated":"2024-03-27T09:46:56Z","published":"2024-03-27T09:46:56Z","title":"Leveraging Large Language Models for Relevance Judgments in Legal Case\n  Retrieval","summary":"  Collecting relevant judgments for legal case retrieval is a challenging and\ntime-consuming task. Accurately judging the relevance between two legal cases\nrequires a considerable effort to read the lengthy text and a high level of\ndomain expertise to extract Legal Facts and make juridical judgments. With the\nadvent of advanced large language models, some recent studies have suggested\nthat it is promising to use LLMs for relevance judgment. Nonetheless, the\nmethod of employing a general large language model for reliable relevance\njudgments in legal case retrieval is yet to be thoroughly explored. To fill\nthis research gap, we devise a novel few-shot workflow tailored to the relevant\njudgment of legal cases. The proposed workflow breaks down the annotation\nprocess into a series of stages, imitating the process employed by human\nannotators and enabling a flexible integration of expert reasoning to enhance\nthe accuracy of relevance judgments. By comparing the relevance judgments of\nLLMs and human experts, we empirically show that we can obtain reliable\nrelevance judgments with the proposed workflow. Furthermore, we demonstrate the\ncapacity to augment existing legal case retrieval models through the synthesis\nof data generated by the large language model.\n","authors":["Shengjie Ma","Chong Chen","Qi Chu","Jiaxin Mao"],"pdf_url":"https://arxiv.org/pdf/2403.18405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18397v1","updated":"2024-03-27T09:35:56Z","published":"2024-03-27T09:35:56Z","title":"Colour and Brush Stroke Pattern Recognition in Abstract Art using\n  Modified Deep Convolutional Generative Adversarial Networks","summary":"  Abstract Art is an immensely popular, discussed form of art that often has\nthe ability to depict the emotions of an artist. Many researchers have made\nattempts to study abstract art in the form of edge detection, brush stroke and\nemotion recognition algorithms using machine and deep learning. This papers\ndescribes the study of a wide distribution of abstract paintings using\nGenerative Adversarial Neural Networks(GAN). GANs have the ability to learn and\nreproduce a distribution enabling researchers and scientists to effectively\nexplore and study the generated image space. However, the challenge lies in\ndeveloping an efficient GAN architecture that overcomes common training\npitfalls. This paper addresses this challenge by introducing a modified-DCGAN\n(mDCGAN) specifically designed for high-quality artwork generation. The\napproach involves a thorough exploration of the modifications made, delving\ninto the intricate workings of DCGANs, optimisation techniques, and\nregularisation methods aimed at improving stability and realism in art\ngeneration enabling effective study of generated patterns. The proposed mDCGAN\nincorporates meticulous adjustments in layer configurations and architectural\nchoices, offering tailored solutions to the unique demands of art generation\nwhile effectively combating issues like mode collapse and gradient vanishing.\nFurther this paper explores the generated latent space by performing random\nwalks to understand vector relationships between brush strokes and colours in\nthe abstract art space and a statistical analysis of unstable outputs after a\ncertain period of GAN training and compare its significant difference. These\nfindings validate the effectiveness of the proposed approach, emphasising its\npotential to revolutionise the field of digital art generation and digital art\necosystem.\n","authors":["Srinitish Srinivasan","Varenya Pathak"],"pdf_url":"https://arxiv.org/pdf/2403.18397v1.pdf","comment":"28 pages, 5 tables, 7 figures"},{"id":"http://arxiv.org/abs/2403.18388v1","updated":"2024-03-27T09:25:20Z","published":"2024-03-27T09:25:20Z","title":"FTBC: Forward Temporal Bias Correction for Optimizing ANN-SNN Conversion","summary":"  Spiking Neural Networks (SNNs) offer a promising avenue for energy-efficient\ncomputing compared with Artificial Neural Networks (ANNs), closely mirroring\nbiological neural processes. However, this potential comes with inherent\nchallenges in directly training SNNs through spatio-temporal backpropagation --\nstemming from the temporal dynamics of spiking neurons and their discrete\nsignal processing -- which necessitates alternative ways of training, most\nnotably through ANN-SNN conversion. In this work, we introduce a lightweight\nForward Temporal Bias Correction (FTBC) technique, aimed at enhancing\nconversion accuracy without the computational overhead. We ground our method on\nprovided theoretical findings that through proper temporal bias calibration the\nexpected error of ANN-SNN conversion can be reduced to be zero after each time\nstep. We further propose a heuristic algorithm for finding the temporal bias\nonly in the forward pass, thus eliminating the computational burden of\nbackpropagation and we evaluate our method on CIFAR-10/100 and ImageNet\ndatasets, achieving a notable increase in accuracy on all datasets. Codes are\nreleased at a GitHub repository.\n","authors":["Xiaofeng Wu","Velibor Bojkovic","Bin Gu","Kun Suo","Kai Zou"],"pdf_url":"https://arxiv.org/pdf/2403.18388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14864v2","updated":"2024-03-27T09:24:55Z","published":"2024-03-21T22:18:59Z","title":"Learning Quadruped Locomotion Using Differentiable Simulation","summary":"  While most recent advancements in legged robot control have been driven by\nmodel-free reinforcement learning, we explore the potential of differentiable\nsimulation. Differentiable simulation promises faster convergence and more\nstable training by computing low-variant first-order gradients using the robot\nmodel, but so far, its use for legged robot control has remained limited to\nsimulation. The main challenge with differentiable simulation lies in the\ncomplex optimization landscape of robotic tasks due to discontinuities in\ncontact-rich environments, e.g., quadruped locomotion. This work proposes a\nnew, differentiable simulation framework to overcome these challenges. The key\nidea involves decoupling the complex whole-body simulation, which may exhibit\ndiscontinuities due to contact, into two separate continuous domains.\nSubsequently, we align the robot state resulting from the simplified model with\na more precise, non-differentiable simulator to maintain sufficient simulation\naccuracy. Our framework enables learning quadruped walking in minutes using a\nsingle simulated robot without any parallelization. When augmented with GPU\nparallelization, our approach allows the quadruped robot to master diverse\nlocomotion skills, including trot, pace, bound, and gallop, on challenging\nterrains in minutes. Additionally, our policy achieves robust locomotion\nperformance in the real world zero-shot. To the best of our knowledge, this\nwork represents the first demonstration of using differentiable simulation for\ncontrolling a real quadruped robot. This work provides several important\ninsights into using differentiable simulations for legged locomotion in the\nreal world.\n","authors":["Yunlong Song","Sangbae Kim","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2403.14864v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18383v1","updated":"2024-03-27T09:21:07Z","published":"2024-03-27T09:21:07Z","title":"Generative Multi-modal Models are Good Class-Incremental Learners","summary":"  In class-incremental learning (CIL) scenarios, the phenomenon of catastrophic\nforgetting caused by the classifier's bias towards the current task has long\nposed a significant challenge. It is mainly caused by the characteristic of\ndiscriminative models. With the growing popularity of the generative\nmulti-modal models, we would explore replacing discriminative models with\ngenerative ones for CIL. However, transitioning from discriminative to\ngenerative models requires addressing two key challenges. The primary challenge\nlies in transferring the generated textual information into the classification\nof distinct categories. Additionally, it requires formulating the task of CIL\nwithin a generative framework. To this end, we propose a novel generative\nmulti-modal model (GMM) framework for class-incremental learning. Our approach\ndirectly generates labels for images using an adapted generative model. After\nobtaining the detailed text, we use a text encoder to extract text features and\nemploy feature matching to determine the most similar label as the\nclassification prediction. In the conventional CIL settings, we achieve\nsignificantly better results in long-sequence task scenarios. Under the\nFew-shot CIL setting, we have improved by at least 14\\% accuracy over all the\ncurrent state-of-the-art methods with significantly less forgetting. Our code\nis available at \\url{https://github.com/DoubleClass/GMM}.\n","authors":["Xusheng Cao","Haori Lu","Linlan Huang","Xialei Liu","Ming-Ming Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.18383v1.pdf","comment":"Accepted at CVPR 2024"},{"id":"http://arxiv.org/abs/2403.18381v1","updated":"2024-03-27T09:19:13Z","published":"2024-03-27T09:19:13Z","title":"Improving Attributed Text Generation of Large Language Models via\n  Preference Learning","summary":"  Large language models have been widely adopted in natural language\nprocessing, yet they face the challenge of generating unreliable content.\nRecent works aim to reduce misinformation and hallucinations by resorting to\nattribution as a means to provide evidence (i.e., citations). However, current\nattribution methods usually focus on the retrieval stage and automatic\nevaluation that neglect mirroring the citation mechanisms in human scholarly\nwriting to bolster credibility. In this paper, we address these challenges by\nmodelling the attribution task as preference learning and introducing an\nAutomatic Preference Optimization (APO) framework. First, we create a curated\ncollection for post-training with 6,330 examples by collecting and filtering\nfrom existing datasets. Second, considering the high cost of labelling\npreference data, we further propose an automatic method to synthesize\nattribution preference data resulting in 95,263 pairs. Moreover, inspired by\nthe human citation process, we further propose a progressive preference\noptimization method by leveraging fine-grained information. Extensive\nexperiments on three datasets (i.e., ASQA, StrategyQA, and ELI5) demonstrate\nthat APO achieves state-of-the-art citation F1 with higher answer quality.\n","authors":["Dongfang Li","Zetian Sun","Baotian Hu","Zhenyu Liu","Xinshuo Hu","Xuebo Liu","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.18381v1.pdf","comment":"23 pages, 15 tables, 2 figures"},{"id":"http://arxiv.org/abs/2403.18379v1","updated":"2024-03-27T09:17:50Z","published":"2024-03-27T09:17:50Z","title":"IIP-Mixer:Intra-Inter Patch Mixing Architecture for Battery Remaining\n  Useful Life Prediction","summary":"  Accurately estimating the Remaining Useful Life (RUL) of lithium-ion\nbatteries is crucial for maintaining the safe and stable operation of\nrechargeable battery management systems. However, this task is often\nchallenging due to the complex temporal dynamics involved. Recently,\nattention-based networks, such as Transformers and Informer, have been the\npopular architecture in time series forecasting. Despite their effectiveness,\nthese models with abundant parameters necessitate substantial training time to\nunravel temporal patterns. To tackle these challenges, we propose a simple\nMLP-Mixer-based architecture named 'Intra-Inter Patch Mixer' (IIP-Mixer), which\nis an architecture based exclusively on multi-layer perceptrons (MLPs),\nextracting information by mixing operations along both intra-patch and\ninter-patch dimensions for battery RUL prediction. The proposed IIP-Mixer\ncomprises parallel dual-head mixer layers: the intra-patch mixing MLP,\ncapturing local temporal patterns in the short-term period, and the inter-patch\nmixing MLP, capturing global temporal patterns in the long-term period.\nNotably, to address the varying importance of features in RUL prediction, we\nintroduce a weighted loss function in the MLP-Mixer-based architecture, marking\nthe first time such an approach has been employed. Our experiments demonstrate\nthat IIP-Mixer achieves competitive performance in battery RUL prediction,\noutperforming other popular time-series frameworks\n","authors":["Guangzai Ye","Li Feng","Jianlan Guo","Yuqiang Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10997v5","updated":"2024-03-27T09:16:57Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) showcase impressive capabilities but encounter\nchallenges like hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the generation,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval, the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces up-to-date evaluation framework and\nbenchmark. At the end, this article delineates the challenges currently faced\nand points out prospective avenues for research and development.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v5.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2312.08533v4","updated":"2024-03-27T09:11:48Z","published":"2023-12-13T21:46:09Z","title":"World Models via Policy-Guided Trajectory Diffusion","summary":"  World models are a powerful tool for developing intelligent agents. By\npredicting the outcome of a sequence of actions, world models enable policies\nto be optimised via on-policy reinforcement learning (RL) using synthetic data,\ni.e. in \"in imagination\". Existing world models are autoregressive in that they\ninterleave predicting the next state with sampling the next action from the\npolicy. Prediction error inevitably compounds as the trajectory length grows.\nIn this work, we propose a novel world modelling approach that is not\nautoregressive and generates entire on-policy trajectories in a single pass\nthrough a diffusion model. Our approach, Policy-Guided Trajectory Diffusion\n(PolyGRAD), leverages a denoising model in addition to the gradient of the\naction distribution of the policy to diffuse a trajectory of initially random\nstates and actions into an on-policy synthetic trajectory. We analyse the\nconnections between PolyGRAD, score-based generative models, and\nclassifier-guided diffusion models. Our results demonstrate that PolyGRAD\noutperforms state-of-the-art baselines in terms of trajectory prediction error\nfor short trajectories, with the exception of autoregressive diffusion. For\nshort trajectories, PolyGRAD obtains similar errors to autoregressive\ndiffusion, but with lower computational requirements. For long trajectories,\nPolyGRAD obtains comparable performance to baselines. Our experiments\ndemonstrate that PolyGRAD enables performant policies to be trained via\non-policy RL in imagination for MuJoCo continuous control domains. Thus,\nPolyGRAD introduces a new paradigm for accurate on-policy world modelling\nwithout autoregressive sampling.\n","authors":["Marc Rigter","Jun Yamada","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2312.08533v4.pdf","comment":"Published in TMLR, March 2024"},{"id":"http://arxiv.org/abs/2403.10158v2","updated":"2024-03-27T08:57:20Z","published":"2024-03-15T10:01:19Z","title":"Functional Graph Convolutional Networks: A unified multi-task and\n  multi-modal learning framework to facilitate health and social-care insights","summary":"  This paper introduces a novel Functional Graph Convolutional Network (funGCN)\nframework that combines Functional Data Analysis and Graph Convolutional\nNetworks to address the complexities of multi-task and multi-modal learning in\ndigital health and longitudinal studies. With the growing importance of health\nsolutions to improve health care and social support, ensure healthy lives, and\npromote well-being at all ages, funGCN offers a unified approach to handle\nmultivariate longitudinal data for multiple entities and ensures\ninterpretability even with small sample sizes. Key innovations include\ntask-specific embedding components that manage different data types, the\nability to perform classification, regression, and forecasting, and the\ncreation of a knowledge graph for insightful data interpretation. The efficacy\nof funGCN is validated through simulation experiments and a real-data\napplication.\n","authors":["Tobia Boschi","Francesca Bonin","Rodrigo Ordonez-Hurtado","Cécile Rousseau","Alessandra Pascale","John Dinsmore"],"pdf_url":"https://arxiv.org/pdf/2403.10158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18364v1","updated":"2024-03-27T08:57:15Z","published":"2024-03-27T08:57:15Z","title":"Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR","summary":"  We investigate the problem of supporting Industrial Internet of Things user\nequipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and\nrandom traffic arrival. A deep reinforcement learning (DRL) based centralized\ndynamic scheduler for time-frequency resources is proposed to learn how to\nschedule the available communication resources among the IIoT UEs. The proposed\nscheduler leverages an RL framework to adapt to the dynamic changes in the\nwireless communication system and traffic arrivals. Moreover, a graph-based\nreduction scheme is proposed to reduce the state and action space of the RL\nframework to allow fast convergence and a better learning strategy. Simulation\nresults demonstrate the effectiveness of the proposed intelligent scheduler in\nguaranteeing the expressed intent of IIoT UEs compared to several traditional\nscheduling schemes, such as round-robin, semi-static, and heuristic approaches.\nThe proposed scheduler also outperforms the contention-free and\ncontention-based schemes in maximizing the number of successfully computed\ntasks.\n","authors":["Salwa Mostafa","Mateus P. Mota","Alvaro Valcarce","Mehdi Bennis"],"pdf_url":"https://arxiv.org/pdf/2403.18364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03325v2","updated":"2024-03-27T08:54:35Z","published":"2023-10-05T05:41:21Z","title":"Learning Concept-Based Causal Transition and Symbolic Reasoning for\n  Visual Planning","summary":"  Visual planning simulates how humans make decisions to achieve desired goals\nin the form of searching for visual causal transitions between an initial\nvisual state and a final visual goal state. It has become increasingly\nimportant in egocentric vision with its advantages in guiding agents to perform\ndaily tasks in complex environments. In this paper, we propose an interpretable\nand generalizable visual planning framework consisting of i) a novel\nSubstitution-based Concept Learner (SCL) that abstracts visual inputs into\ndisentangled concept representations, ii) symbol abstraction and reasoning that\nperforms task planning via the self-learned symbols, and iii) a Visual Causal\nTransition model (ViCT) that grounds visual causal transitions to semantically\nsimilar real-world actions. Given an initial state, we perform goal-conditioned\nvisual planning with a symbolic reasoning method fueled by the learned\nrepresentations and causal transitions to reach the goal state. To verify the\neffectiveness of the proposed model, we collect a large-scale visual planning\ndataset based on AI2-THOR, dubbed as CCTP. Extensive experiments on this\nchallenging dataset demonstrate the superior performance of our method in\nvisual task planning. Empirically, we show that our framework can generalize to\nunseen task trajectories, unseen object categories, and real-world data.\nFurther details of this work are provided at\nhttps://fqyqc.github.io/ConTranPlan/.\n","authors":["Yilue Qian","Peiyu Yu","Ying Nian Wu","Yao Su","Wei Wang","Lifeng Fan"],"pdf_url":"https://arxiv.org/pdf/2310.03325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02151v2","updated":"2024-03-27T08:43:28Z","published":"2023-05-03T14:33:23Z","title":"Identifying the Correlation Between Language Distance and Cross-Lingual\n  Transfer in a Multilingual Representation Space","summary":"  Prior research has investigated the impact of various linguistic features on\ncross-lingual transfer performance. In this study, we investigate the manner in\nwhich this effect can be mapped onto the representation space. While past\nstudies have focused on the impact on cross-lingual alignment in multilingual\nlanguage models during fine-tuning, this study examines the absolute evolution\nof the respective language representation spaces produced by MLLMs. We place a\nspecific emphasis on the role of linguistic characteristics and investigate\ntheir inter-correlation with the impact on representation spaces and\ncross-lingual transfer performance. Additionally, this paper provides\npreliminary evidence of how these findings can be leveraged to enhance transfer\nto linguistically distant languages.\n","authors":["Fred Philippy","Siwen Guo","Shohreh Haddadan"],"pdf_url":"https://arxiv.org/pdf/2305.02151v2.pdf","comment":"SIGTYP Workshop 2023 (co-located with EACL 2023)"},{"id":"http://arxiv.org/abs/2403.18351v1","updated":"2024-03-27T08:42:47Z","published":"2024-03-27T08:42:47Z","title":"Generating Diverse Agricultural Data for Vision-Based Farming\n  Applications","summary":"  We present a specialized procedural model for generating synthetic\nagricultural scenes, focusing on soybean crops, along with various weeds. This\nmodel is capable of simulating distinct growth stages of these plants, diverse\nsoil conditions, and randomized field arrangements under varying lighting\nconditions. The integration of real-world textures and environmental factors\ninto the procedural generation process enhances the photorealism and\napplicability of the synthetic data. Our dataset includes 12,000 images with\nsemantic labels, offering a comprehensive resource for computer vision tasks in\nprecision agriculture, such as semantic segmentation for autonomous weed\ncontrol. We validate our model's effectiveness by comparing the synthetic data\nagainst real agricultural images, demonstrating its potential to significantly\naugment training data for machine learning models in agriculture. This approach\nnot only provides a cost-effective solution for generating high-quality,\ndiverse data but also addresses specific needs in agricultural vision tasks\nthat are not fully covered by general-purpose models.\n","authors":["Mikolaj Cieslak","Umabharathi Govindarajan","Alejandro Garcia","Anuradha Chandrashekar","Torsten Hädrich","Aleksander Mendoza-Drosik","Dominik L. Michels","Sören Pirk","Chia-Chun Fu","Wojciech Pałubicki"],"pdf_url":"https://arxiv.org/pdf/2403.18351v1.pdf","comment":"10 pages, 8 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18347v1","updated":"2024-03-27T08:38:56Z","published":"2024-03-27T08:38:56Z","title":"A Quantum Fuzzy-based Approach for Real-Time Detection of Solar Coronal\n  Holes","summary":"  The detection and analysis of the solar coronal holes (CHs) is an important\nfield of study in the domain of solar physics. Mainly, it is required for the\nproper prediction of the geomagnetic storms which directly or indirectly affect\nvarious space and ground-based systems. For the detection of CHs till date, the\nsolar scientist depends on manual hand-drawn approaches. However, with the\nadvancement of image processing technologies, some automated image segmentation\nmethods have been used for the detection of CHs. In-spite of this, fast and\naccurate detection of CHs are till a major issues. Here in this work, a novel\nquantum computing-based fast fuzzy c-mean technique has been developed for fast\ndetection of the CHs region. The task has been carried out in two stages, in\nfirst stage the solar image has been segmented using a quantum computing based\nfast fuzzy c-mean (QCFFCM) and in the later stage the CHs has been extracted\nout from the segmented image based on image morphological operation. In the\nwork, quantum computing has been used to optimize the cost function of the fast\nfuzzy c-mean (FFCM) algorithm, where quantum approximate optimization algorithm\n(QAOA) has been used to optimize the quadratic part of the cost function. The\nproposed method has been tested for 193 \\AA{} SDO/AIA full-disk solar image\ndatasets and has been compared with the existing techniques. The outcome shows\nthe comparable performance of the proposed method with the existing one within\na very lesser time.\n","authors":["Sanmoy Bandyopadhyay","Suman Kundu"],"pdf_url":"https://arxiv.org/pdf/2403.18347v1.pdf","comment":"14 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2403.18344v1","updated":"2024-03-27T08:34:55Z","published":"2024-03-27T08:34:55Z","title":"LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions\n  with Large Language Models","summary":"  To ensure safe driving in dynamic environments, autonomous vehicles should\npossess the capability to accurately predict the lane change intentions of\nsurrounding vehicles in advance and forecast their future trajectories.\nExisting motion prediction approaches have ample room for improvement,\nparticularly in terms of long-term prediction accuracy and interpretability. In\nthis paper, we address these challenges by proposing LC-LLM, an explainable\nlane change prediction model that leverages the strong reasoning capabilities\nand self-explanation abilities of Large Language Models (LLMs). Essentially, we\nreformulate the lane change prediction task as a language modeling problem,\nprocessing heterogeneous driving scenario information in natural language as\nprompts for input into the LLM and employing a supervised fine-tuning technique\nto tailor the LLM specifically for our lane change prediction task. This allows\nus to utilize the LLM's powerful common sense reasoning abilities to understand\ncomplex interactive information, thereby improving the accuracy of long-term\npredictions. Furthermore, we incorporate explanatory requirements into the\nprompts in the inference stage. Therefore, our LC-LLM model not only can\npredict lane change intentions and trajectories but also provides explanations\nfor its predictions, enhancing the interpretability. Extensive experiments on\nthe large-scale highD dataset demonstrate the superior performance and\ninterpretability of our LC-LLM in lane change prediction task. To the best of\nour knowledge, this is the first attempt to utilize LLMs for predicting lane\nchange behavior. Our study shows that LLMs can encode comprehensive interaction\ninformation for driving behavior understanding.\n","authors":["Mingxing Peng","Xusen Guo","Xianda Chen","Meixin Zhu","Kehua Chen"," Hao"," Yang","Xuesong Wang","Yinhai Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18338v1","updated":"2024-03-27T08:25:28Z","published":"2024-03-27T08:25:28Z","title":"mALBERT: Is a Compact Multilingual BERT Model Still Worth It?","summary":"  Within the current trend of Pretained Language Models (PLM), emerge more and\nmore criticisms about the ethical andecological impact of such models. In this\narticle, considering these critical remarks, we propose to focus on\nsmallermodels, such as compact models like ALBERT, which are more ecologically\nvirtuous than these PLM. However,PLMs enable huge breakthroughs in Natural\nLanguage Processing tasks, such as Spoken and Natural LanguageUnderstanding,\nclassification, Question--Answering tasks. PLMs also have the advantage of\nbeing multilingual, and,as far as we know, a multilingual version of compact\nALBERT models does not exist. Considering these facts, wepropose the free\nrelease of the first version of a multilingual compact ALBERT model,\npre-trained using Wikipediadata, which complies with the ethical aspect of such\na language model. We also evaluate the model against classicalmultilingual PLMs\nin classical NLP tasks. Finally, this paper proposes a rare study on the\nsubword tokenizationimpact on language performances.\n","authors":["Christophe Servan","Sahar Ghannay","Sophie Rosset"],"pdf_url":"https://arxiv.org/pdf/2403.18338v1.pdf","comment":"The 2024 Joint International Conference on Computational Linguistics,\n  Language Resources and Evaluation, May 2024, Torino, Italy"},{"id":"http://arxiv.org/abs/2403.18327v1","updated":"2024-03-27T08:08:00Z","published":"2024-03-27T08:08:00Z","title":"Can LLMs Converse Formally? Automatically Assessing LLMs in Translating\n  and Interpreting Formal Specifications","summary":"  Stakeholders often describe system requirements using natural language which\nare then converted to formal syntax by a domain-expert leading to increased\ndesign costs. This paper assesses the capabilities of Large Language Models\n(LLMs) in converting between natural language descriptions and formal\nspecifications. Existing work has evaluated the capabilities of LLMs in\ngenerating formal syntax such as source code but such experiments are typically\nhand-crafted and use problems that are likely to be in the training set of\nLLMs, and often require human-annotated datasets. We propose an approach that\ncan use two copies of an LLM in conjunction with an off-the-shelf verifier to\nautomatically evaluate its translation abilities without any additional human\ninput. Our approach generates formal syntax using language grammars to\nautomatically generate a dataset. We conduct an empirical evaluation to measure\nthe accuracy of this translation task and show that SOTA LLMs cannot adequately\nsolve this task, limiting their current utility in the design of complex\nsystems.\n","authors":["Rushang Karia","Daksh Dobhal","Daniel Bramblett","Pulkit Verma","Siddharth Srivastava"],"pdf_url":"https://arxiv.org/pdf/2403.18327v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18314v1","updated":"2024-03-27T07:34:44Z","published":"2024-03-27T07:34:44Z","title":"Chinese Offensive Language Detection:Current Status and Future\n  Directions","summary":"  Despite the considerable efforts being made to monitor and regulate\nuser-generated content on social media platforms, the pervasiveness of\noffensive language, such as hate speech or cyberbullying, in the digital space\nremains a significant challenge. Given the importance of maintaining a\ncivilized and respectful online environment, there is an urgent and growing\nneed for automatic systems capable of detecting offensive speech in real time.\nHowever, developing effective systems for processing languages such as Chinese\npresents a significant challenge, owing to the language's complex and nuanced\nnature, which makes it difficult to process automatically. This paper provides\na comprehensive overview of offensive language detection in Chinese, examining\ncurrent benchmarks and approaches and highlighting specific models and tools\nfor addressing the unique challenges of detecting offensive language in this\ncomplex language. The primary objective of this survey is to explore the\nexisting techniques and identify potential avenues for further research that\ncan address the cultural and linguistic complexities of Chinese.\n","authors":["Yunze Xiao","Houda Bouamor","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2403.18314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18310v1","updated":"2024-03-27T07:22:32Z","published":"2024-03-27T07:22:32Z","title":"A thermodynamically consistent physics-informed deep learning material\n  model for short fiber/polymer nanocomposites","summary":"  This work proposes a physics-informed deep learning (PIDL)-based constitutive\nmodel for investigating the viscoelastic-viscoplastic behavior of short\nfiber-reinforced nanoparticle-filled epoxies under various ambient conditions.\nThe deep-learning model is trained to enforce thermodynamic principles, leading\nto a thermodynamically consistent constitutive model. To accomplish this, a\nlong short-term memory network is combined with a feed-forward neural network\nto predict internal variables required for characterizing the internal\ndissipation of the nanocomposite materials. In addition, another feed-forward\nneural network is used to indicate the free-energy function, which enables\ndefining the thermodynamic state of the entire system. The PIDL model is\ninitially developed for the three-dimensional case by generating synthetic data\nfrom a classical constitutive model. The model is then trained by extracting\nthe data directly from cyclic loading-unloading experimental tests. Numerical\nexamples show that the PIDL model can accurately predict the mechanical\nbehavior of epoxy-based nanocomposites for different volume fractions of fibers\nand nanoparticles under various hygrothermal conditions.\n","authors":["Betim Bahtiri","Behrouz Arash","Sven Scheffler","Maximilian Jux","Raimund Rolfes"],"pdf_url":"https://arxiv.org/pdf/2403.18310v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08102"},{"id":"http://arxiv.org/abs/2307.09136v2","updated":"2024-03-27T07:16:28Z","published":"2023-07-18T10:34:21Z","title":"The Effects of Mixed Sample Data Augmentation are Class Dependent","summary":"  Mixed Sample Data Augmentation (MSDA) techniques, such as Mixup, CutMix, and\nPuzzleMix, have been widely acknowledged for enhancing performance in a variety\nof tasks. A previous study reported the class dependency of traditional data\naugmentation (DA), where certain classes benefit disproportionately compared to\nothers. This paper reveals a class dependent effect of MSDA, where some classes\nexperience improved performance while others experience degraded performance.\nThis research addresses the issue of class dependency in MSDA and proposes an\nalgorithm to mitigate it. The approach involves training on a mixture of MSDA\nand non-MSDA data, which not only mitigates the negative impact on the affected\nclasses, but also improves overall accuracy. Furthermore, we provide in-depth\nanalysis and discussion of why MSDA introduced class dependencies and which\nclasses are most likely to have them.\n","authors":["Haeil Lee","Hansang Lee","Junmo Kim"],"pdf_url":"https://arxiv.org/pdf/2307.09136v2.pdf","comment":"21 pages, 18 figures, Overall Revision"},{"id":"http://arxiv.org/abs/2402.18920v5","updated":"2024-03-27T07:16:21Z","published":"2024-02-29T07:26:23Z","title":"Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation","summary":"  Although 3D shape matching and interpolation are highly interrelated, they\nare often studied separately and applied sequentially to relate different 3D\nshapes, thus resulting in sub-optimal performance. In this work we present a\nunified framework to predict both point-wise correspondences and shape\ninterpolation between 3D shapes. To this end, we combine the deep functional\nmap framework with classical surface deformation models to map shapes in both\nspectral and spatial domains. On the one hand, by incorporating spatial maps,\nour method obtains more accurate and smooth point-wise correspondences compared\nto previous functional map methods for shape matching. On the other hand, by\nintroducing spectral maps, our method gets rid of commonly used but\ncomputationally expensive geodesic distance constraints that are only valid for\nnear-isometric shape deformations. Furthermore, we propose a novel test-time\nadaptation scheme to capture both pose-dominant and shape-dominant\ndeformations. Using different challenging datasets, we demonstrate that our\nmethod outperforms previous state-of-the-art methods for both shape matching\nand interpolation, even compared to supervised approaches.\n","authors":["Dongliang Cao","Marvin Eisenberger","Nafie El Amrani","Daniel Cremers","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2402.18920v5.pdf","comment":"accepted by CVPR2024"},{"id":"http://arxiv.org/abs/2306.12609v2","updated":"2024-03-27T07:11:30Z","published":"2023-06-22T00:12:30Z","title":"Towards Regulatable AI Systems: Technical Gaps and Policy Opportunities","summary":"  There is increasing attention being given to how to regulate AI systems. As\ngoverning bodies grapple with what values to encapsulate into regulation, we\nconsider the technical half of the question: To what extent can AI experts vet\nan AI system for adherence to regulatory requirements? We investigate this\nquestion through the lens of two public sector procurement checklists,\nidentifying what we can do now, what should be possible with technical\ninnovation, and what requirements need a more interdisciplinary approach.\n","authors":["Xudong Shen","Hannah Brown","Jiashu Tao","Martin Strobel","Yao Tong","Akshay Narayan","Harold Soh","Finale Doshi-Velez"],"pdf_url":"https://arxiv.org/pdf/2306.12609v2.pdf","comment":"scheduled for publication in the Communications of the ACM, titled\n  \"Directions of Technical Innovation for Regulatable AI Systems\""},{"id":"http://arxiv.org/abs/2310.17072v3","updated":"2024-03-27T07:04:58Z","published":"2023-10-26T00:28:37Z","title":"MMP++: Motion Manifold Primitives with Parametric Curve Models","summary":"  Motion Manifold Primitives (MMP), a manifold-based approach for encoding\nbasic motion skills, can produce diverse trajectories, enabling the system to\nadapt to unseen constraints. Nonetheless, we argue that current MMP models lack\ncrucial functionalities of movement primitives, such as temporal and via-points\nmodulation, found in traditional approaches. This shortfall primarily stems\nfrom MMP's reliance on discrete-time trajectories. To overcome these\nlimitations, we introduce Motion Manifold Primitives++ (MMP++), a new model\nthat integrates the strengths of both MMP and traditional methods by\nincorporating parametric curve representations into the MMP framework.\nFurthermore, we identify a significant challenge with MMP++: performance\ndegradation due to geometric distortions in the latent space, meaning that\nsimilar motions are not closely positioned. To address this, Isometric Motion\nManifold Primitives++ (IMMP++) is proposed to ensure the latent space\naccurately preserves the manifold's geometry. Our experimental results across\nvarious applications, including 2-DoF planar motions, 7-DoF robot arm motions,\nand SE(3) trajectory planning, show that MMP++ and IMMP++ outperform existing\nmethods in trajectory generation tasks, achieving substantial improvements in\nsome cases. Moreover, they enable the modulation of latent coordinates and\nvia-points, thereby allowing efficient online adaptation to dynamic\nenvironments.\n","authors":["Yonghyeon Lee"],"pdf_url":"https://arxiv.org/pdf/2310.17072v3.pdf","comment":"12 pages. This work has been submitted to the IEEE for possible\n  publication"},{"id":"http://arxiv.org/abs/2403.18305v1","updated":"2024-03-27T06:59:39Z","published":"2024-03-27T06:59:39Z","title":"A Recommender System for NFT Collectibles with Item Feature","summary":"  Recommender systems have been actively studied and applied in various domains\nto deal with information overload. Although there are numerous studies on\nrecommender systems for movies, music, and e-commerce, comparatively less\nattention has been paid to the recommender system for NFTs despite the\ncontinuous growth of the NFT market. This paper presents a recommender system\nfor NFTs that utilizes a variety of data sources, from NFT transaction records\nto external item features, to generate precise recommendations that cater to\nindividual preferences. We develop a data-efficient graph-based recommender\nsystem to efficiently capture the complex relationship between each item and\nusers and generate node(item) embeddings which incorporate both node feature\ninformation and graph structure. Furthermore, we exploit inputs beyond\nuser-item interactions, such as image feature, text feature, and price feature.\nNumerical experiments verify the performance of the graph-based recommender\nsystem improves significantly after utilizing all types of item features as\nside information, thereby outperforming all other baselines.\n","authors":["Minjoo Choi","Seonmi Kim","Yejin Kim","Youngbin Lee","Joohwan Hong","Yongjae Lee"],"pdf_url":"https://arxiv.org/pdf/2403.18305v1.pdf","comment":"Presented at the AAAI 2023 Bridge on AI for Financial Services\n  (https://sites.google.com/view/aaai-ai-fin/home)"},{"id":"http://arxiv.org/abs/2302.06912v4","updated":"2024-03-27T06:57:30Z","published":"2023-02-14T08:56:50Z","title":"Regret-Based Defense in Adversarial Reinforcement Learning","summary":"  Deep Reinforcement Learning (DRL) policies have been shown to be vulnerable\nto small adversarial noise in observations. Such adversarial noise can have\ndisastrous consequences in safety-critical environments. For instance, a\nself-driving car receiving adversarially perturbed sensory observations about\nnearby signs (e.g., a stop sign physically altered to be perceived as a speed\nlimit sign) or objects (e.g., cars altered to be recognized as trees) can be\nfatal. Existing approaches for making RL algorithms robust to an\nobservation-perturbing adversary have focused on reactive approaches that\niteratively improve against adversarial examples generated at each iteration.\nWhile such approaches have been shown to provide improvements over regular RL\nmethods, they are reactive and can fare significantly worse if certain\ncategories of adversarial examples are not generated during training. To that\nend, we pursue a more proactive approach that relies on directly optimizing a\nwell-studied robustness measure, regret instead of expected value. We provide a\nprincipled approach that minimizes maximum regret over a \"neighborhood\" of\nobservations to the received \"observation\". Our regret criterion can be used to\nmodify existing value- and policy-based Deep RL methods. We demonstrate that\nour approaches provide a significant improvement in performance across a wide\nvariety of benchmarks against leading approaches for robust Deep RL.\n","authors":["Roman Belaire","Pradeep Varakantham","Thanh Nguyen","David Lo"],"pdf_url":"https://arxiv.org/pdf/2302.06912v4.pdf","comment":"Accepted at AAMAS 2024"},{"id":"http://arxiv.org/abs/2403.18301v1","updated":"2024-03-27T06:55:23Z","published":"2024-03-27T06:55:23Z","title":"Selective Mixup Fine-Tuning for Optimizing Non-Decomposable Objectives","summary":"  The rise in internet usage has led to the generation of massive amounts of\ndata, resulting in the adoption of various supervised and semi-supervised\nmachine learning algorithms, which can effectively utilize the colossal amount\nof data to train models. However, before deploying these models in the real\nworld, these must be strictly evaluated on performance measures like worst-case\nrecall and satisfy constraints such as fairness. We find that current\nstate-of-the-art empirical techniques offer sub-optimal performance on these\npractical, non-decomposable performance objectives. On the other hand, the\ntheoretical techniques necessitate training a new model from scratch for each\nperformance objective. To bridge the gap, we propose SelMix, a selective\nmixup-based inexpensive fine-tuning technique for pre-trained models, to\noptimize for the desired objective. The core idea of our framework is to\ndetermine a sampling distribution to perform a mixup of features between\nsamples from particular classes such that it optimizes the given objective. We\ncomprehensively evaluate our technique against the existing empirical and\ntheoretically principled methods on standard benchmark datasets for imbalanced\nclassification. We find that proposed SelMix fine-tuning significantly improves\nthe performance for various practical non-decomposable objectives across\nbenchmarks.\n","authors":["Shrinivas Ramasubramanian","Harsh Rangwani","Sho Takemori","Kunal Samanta","Yuhei Umeda","Venkatesh Babu Radhakrishnan"],"pdf_url":"https://arxiv.org/pdf/2403.18301v1.pdf","comment":"ICLR 2024 SpotLight"},{"id":"http://arxiv.org/abs/2403.18296v1","updated":"2024-03-27T06:46:59Z","published":"2024-03-27T06:46:59Z","title":"GeNet: A Graph Neural Network-based Anti-noise Task-Oriented Semantic\n  Communication Paradigm","summary":"  Traditional approaches to semantic communication tasks rely on the knowledge\nof the signal-to-noise ratio (SNR) to mitigate channel noise. However, these\nmethods necessitate training under specific SNR conditions, entailing\nconsiderable time and computational resources. In this paper, we propose GeNet,\na Graph Neural Network (GNN)-based paradigm for semantic communication aimed at\ncombating noise, thereby facilitating Task-Oriented Communication (TOC). We\npropose a novel approach where we first transform the input data image into\ngraph structures. Then we leverage a GNN-based encoder to extract semantic\ninformation from the source data. This extracted semantic information is then\ntransmitted through the channel. At the receiver's end, a GNN-based decoder is\nutilized to reconstruct the relevant semantic information from the source data\nfor TOC. Through experimental evaluation, we show GeNet's effectiveness in\nanti-noise TOC while decoupling the SNR dependency. We further evaluate GeNet's\nperformance by varying the number of nodes, revealing its versatility as a new\nparadigm for semantic communication. Additionally, we show GeNet's robustness\nto geometric transformations by testing it with different rotation angles,\nwithout resorting to data augmentation.\n","authors":["Chunhang Zheng","Kechao Cai"],"pdf_url":"https://arxiv.org/pdf/2403.18296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17421v2","updated":"2024-03-27T06:28:53Z","published":"2024-03-26T06:34:23Z","title":"MA4DIV: Multi-Agent Reinforcement Learning for Search Result\n  Diversification","summary":"  The objective of search result diversification (SRD) is to ensure that\nselected documents cover as many different subtopics as possible. Existing\nmethods primarily utilize a paradigm of \"greedy selection\", i.e., selecting one\ndocument with the highest diversity score at a time. These approaches tend to\nbe inefficient and are easily trapped in a suboptimal state. In addition, some\nother methods aim to approximately optimize the diversity metric, such as\n$\\alpha$-NDCG, but the results still remain suboptimal. To address these\nchallenges, we introduce Multi-Agent reinforcement learning (MARL) for search\nresult DIVersity, which called MA4DIV. In this approach, each document is an\nagent and the search result diversification is modeled as a cooperative task\namong multiple agents. This approach allows for directly optimizing the\ndiversity metrics, such as $\\alpha$-NDCG, while achieving high training\nefficiency. We conducted preliminary experiments on public TREC datasets to\ndemonstrate the effectiveness and potential of MA4DIV. Considering the limited\nnumber of queries in public TREC datasets, we construct a large-scale dataset\nfrom industry sources and show that MA4DIV achieves substantial improvements in\nboth effectiveness and efficiency than existing baselines on a industrial scale\ndataset.\n","authors":["Yiqun Chen","Jiaxin Mao","Yi Zhang","Dehong Ma","Long Xia","Jun Fan","Daiting Shi","Zhicong Cheng","Simiu Gu","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17421v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18286v1","updated":"2024-03-27T06:25:40Z","published":"2024-03-27T06:25:40Z","title":"Few-Shot Recalibration of Language Models","summary":"  Recent work has uncovered promising ways to extract well-calibrated\nconfidence estimates from language models (LMs), where the model's confidence\nscore reflects how likely it is to be correct. However, while LMs may appear\nwell-calibrated over broad distributions, this often hides significant\nmiscalibration within narrower slices (e.g., systemic over-confidence in math\ncan balance out systemic under-confidence in history, yielding perfect\ncalibration in aggregate). To attain well-calibrated confidence estimates for\nany slice of a distribution, we propose a new framework for few-shot\nslice-specific recalibration. Specifically, we train a recalibration model that\ntakes in a few unlabeled examples from any given slice and predicts a curve\nthat remaps confidence scores to be more accurate for that slice. Our trained\nmodel can recalibrate for arbitrary new slices, without using any labeled data\nfrom that slice. This enables us to identify domain-specific confidence\nthresholds above which the LM's predictions can be trusted, and below which it\nshould abstain. Experiments show that our few-shot recalibrator consistently\noutperforms existing calibration methods, for instance improving calibration\nerror for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.\n","authors":["Xiang Lisa Li","Urvashi Khandelwal","Kelvin Guu"],"pdf_url":"https://arxiv.org/pdf/2403.18286v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2403.16512v2","updated":"2024-03-27T06:25:10Z","published":"2024-03-25T07:55:29Z","title":"LLMs Are Few-Shot In-Context Low-Resource Language Learners","summary":"  In-context learning (ICL) empowers large language models (LLMs) to perform\ndiverse tasks in underrepresented languages using only short in-context\ninformation, offering a crucial avenue for narrowing the gap between\nhigh-resource and low-resource languages. Nonetheless, there is only a handful\nof works explored ICL for low-resource languages with most of them focusing on\nrelatively high-resource languages, such as French and Spanish. In this work,\nwe extensively study ICL and its cross-lingual variation (X-ICL) on 25\nlow-resource and 7 relatively higher-resource languages. Our study not only\nassesses the effectiveness of ICL with LLMs in low-resource languages but also\nidentifies the shortcomings of in-context label alignment, and introduces a\nmore effective alternative: query alignment. Moreover, we provide valuable\ninsights into various facets of ICL for low-resource languages. Our study\nconcludes the significance of few-shot in-context information on enhancing the\nlow-resource understanding quality of LLMs through semantically relevant\ninformation by closing the language gap in the target language and aligning the\nsemantics between the targeted low-resource and the high-resource language that\nthe model is proficient in. Our work highlights the importance of advancing ICL\nresearch, particularly for low-resource languages.\n","authors":["Samuel Cahyawijaya","Holy Lovenia","Pascale Fung"],"pdf_url":"https://arxiv.org/pdf/2403.16512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15472v2","updated":"2024-03-27T06:22:41Z","published":"2024-03-20T15:47:28Z","title":"Enhancing Programming Education with ChatGPT: A Case Study on Student\n  Perceptions and Interactions in a Python Course","summary":"  The integration of ChatGPT as a supportive tool in education, notably in\nprogramming courses, addresses the unique challenges of programming education\nby providing assistance with debugging, code generation, and explanations.\nDespite existing research validating ChatGPT's effectiveness, its application\nin university-level programming education and a detailed understanding of\nstudent interactions and perspectives remain limited. This paper explores\nChatGPT's impact on learning in a Python programming course tailored for\nfirst-year students over eight weeks. By analyzing responses from surveys,\nopen-ended questions, and student-ChatGPT dialog data, we aim to provide a\ncomprehensive view of ChatGPT's utility and identify both its advantages and\nlimitations as perceived by students. Our study uncovers a generally positive\nreception toward ChatGPT and offers insights into its role in enhancing the\nprogramming education experience. These findings contribute to the broader\ndiscourse on AI's potential in education, suggesting paths for future research\nand application.\n","authors":["Boxaun Ma","Li Chen","Shin'ichi Konomi"],"pdf_url":"https://arxiv.org/pdf/2403.15472v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18278v1","updated":"2024-03-27T06:13:39Z","published":"2024-03-27T06:13:39Z","title":"Identification and Uses of Deep Learning Backbones via Pattern Mining","summary":"  Deep learning is extensively used in many areas of data mining as a black-box\nmethod with impressive results. However, understanding the core mechanism of\nhow deep learning makes predictions is a relatively understudied problem. Here\nwe explore the notion of identifying a backbone of deep learning for a given\ngroup of instances. A group here can be instances of the same class or even\nmisclassified instances of the same class. We view each instance for a given\ngroup as activating a subset of neurons and attempt to find a subgraph of\nneurons associated with a given concept/group. We formulate this problem as a\nset cover style problem and show it is intractable and presents a highly\nconstrained integer linear programming (ILP) formulation. As an alternative, we\nexplore a coverage-based heuristic approach related to pattern mining, and show\nit converges to a Pareto equilibrium point of the ILP formulation.\nExperimentally we explore these backbones to identify mistakes and improve\nperformance, explanation, and visualization. We demonstrate application-based\nresults using several challenging data sets, including Bird Audio Detection\n(BAD) Challenge and Labeled Faces in the Wild (LFW), as well as the classic\nMNIST data.\n","authors":["Michael Livanos","Ian Davidson"],"pdf_url":"https://arxiv.org/pdf/2403.18278v1.pdf","comment":"9 pages, 6 figures, published SIAM SDM24"},{"id":"http://arxiv.org/abs/2403.07711v2","updated":"2024-03-27T06:02:38Z","published":"2024-03-12T14:53:56Z","title":"SSM Meets Video Diffusion Models: Efficient Video Generation with\n  Structured State Spaces","summary":"  Given the remarkable achievements in image generation through diffusion\nmodels, the research community has shown increasing interest in extending these\nmodels to video generation. Recent diffusion models for video generation have\npredominantly utilized attention layers to extract temporal features. However,\nattention layers are limited by their memory consumption, which increases\nquadratically with the length of the sequence. This limitation presents\nsignificant challenges when attempting to generate longer video sequences using\ndiffusion models. To overcome this challenge, we propose leveraging state-space\nmodels (SSMs). SSMs have recently gained attention as viable alternatives due\nto their linear memory consumption relative to sequence length. In the\nexperiments, we first evaluate our SSM-based model with UCF101, a standard\nbenchmark of video generation. In addition, to investigate the potential of\nSSMs for longer video generation, we perform an experiment using the MineRL\nNavigate dataset, varying the number of frames to 64, 200, and 400. In these\nsettings, our SSM-based model can considerably save memory consumption for\nlonger sequences, while maintaining competitive FVD scores to the\nattention-based models. Our codes are available at\nhttps://github.com/shim0114/SSM-Meets-Video-Diffusion-Models.\n","authors":["Yuta Oshima","Shohei Taniguchi","Masahiro Suzuki","Yutaka Matsuo"],"pdf_url":"https://arxiv.org/pdf/2403.07711v2.pdf","comment":"Accepted as workshop paper at ICLR 2024"},{"id":"http://arxiv.org/abs/2305.14258v2","updated":"2024-03-27T05:45:37Z","published":"2023-05-23T17:11:33Z","title":"Weakly Supervised AUC Optimization: A Unified Partial AUC Approach","summary":"  Since acquiring perfect supervision is usually difficult, real-world machine\nlearning tasks often confront inaccurate, incomplete, or inexact supervision,\ncollectively referred to as weak supervision. In this work, we present WSAUC, a\nunified framework for weakly supervised AUC optimization problems, which covers\nnoisy label learning, positive-unlabeled learning, multi-instance learning, and\nsemi-supervised learning scenarios. Within the WSAUC framework, we first frame\nthe AUC optimization problems in various weakly supervised scenarios as a\ncommon formulation of minimizing the AUC risk on contaminated sets, and\ndemonstrate that the empirical risk minimization problems are consistent with\nthe true AUC. Then, we introduce a new type of partial AUC, specifically, the\nreversed partial AUC (rpAUC), which serves as a robust training objective for\nAUC maximization in the presence of contaminated labels. WSAUC offers a\nuniversal solution for AUC optimization in various weakly supervised scenarios\nby maximizing the empirical rpAUC. Theoretical and experimental results under\nmultiple settings support the effectiveness of WSAUC on a range of weakly\nsupervised AUC optimization tasks.\n","authors":["Zheng Xie","Yu Liu","Hao-Yuan He","Ming Li","Zhi-Hua Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.14258v2.pdf","comment":"Accepted by IEEE TPAMI"},{"id":"http://arxiv.org/abs/2403.18267v1","updated":"2024-03-27T05:41:50Z","published":"2024-03-27T05:41:50Z","title":"DSF-GAN: DownStream Feedback Generative Adversarial Network","summary":"  Utility and privacy are two crucial measurements of the quality of synthetic\ntabular data. While significant advancements have been made in privacy\nmeasures, generating synthetic samples with high utility remains challenging.\nTo enhance the utility of synthetic samples, we propose a novel architecture\ncalled the DownStream Feedback Generative Adversarial Network (DSF-GAN). This\napproach incorporates feedback from a downstream prediction model during\ntraining to augment the generator's loss function with valuable information.\nThus, DSF-GAN utilizes a downstream prediction task to enhance the utility of\nsynthetic samples. To evaluate our method, we tested it using two popular\ndatasets. Our experiments demonstrate improved model performance when training\non synthetic samples generated by DSF-GAN, compared to those generated by the\nsame GAN architecture without feedback. The evaluation was conducted on the\nsame validation set comprising real samples. All code and datasets used in this\nresearch will be made openly available for ease of reproduction.\n","authors":["Oriel Perets","Nadav Rappoport"],"pdf_url":"https://arxiv.org/pdf/2403.18267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18258v1","updated":"2024-03-27T05:10:38Z","published":"2024-03-27T05:10:38Z","title":"Enhancing Generative Class Incremental Learning Performance with Model\n  Forgetting Approach","summary":"  This study presents a novel approach to Generative Class Incremental Learning\n(GCIL) by introducing the forgetting mechanism, aimed at dynamically managing\nclass information for better adaptation to streaming data. GCIL is one of the\nhot topics in the field of computer vision, and this is considered one of the\ncrucial tasks in society, specifically the continual learning of generative\nmodels. The ability to forget is a crucial brain function that facilitates\ncontinual learning by selectively discarding less relevant information for\nhumans. However, in the field of machine learning models, the concept of\nintentionally forgetting has not been extensively investigated. In this study\nwe aim to bridge this gap by incorporating the forgetting mechanisms into GCIL,\nthereby examining their impact on the models' ability to learn in continual\nlearning. Through our experiments, we have found that integrating the\nforgetting mechanisms significantly enhances the models' performance in\nacquiring new knowledge, underscoring the positive role that strategic\nforgetting plays in the process of continual learning.\n","authors":["Taro Togo","Ren Togo","Keisuke Maeda","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2403.18258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09131v2","updated":"2024-03-27T05:02:55Z","published":"2024-03-14T06:49:16Z","title":"ProSwitch: Knowledge-Guided Language Model Fine-Tuning to Generate\n  Professional and Non-Professional Styled Text","summary":"  Large Language Models (LLMs) have demonstrated efficacy in various linguistic\napplications, including text summarization and controlled text generation.\nHowever, studies into their capacity of switching between styles via\nfine-tuning remain underexplored. This study concentrates on textual\nprofessionalism and introduces a novel methodology, named ProSwitch, which\nequips a language model with the ability to produce both professional and\nnon-professional responses through knowledge-guided instruction tuning.\nProSwitch unfolds across three phases: data preparation for gathering domain\nknowledge and training corpus; instruction tuning for optimizing language\nmodels with multiple levels of instruction formats; and comprehensive\nevaluation for assessing the professionalism discrimination and reference-based\nquality of generated text. Comparative analysis of ProSwitch against both\ngeneral and specialized language models reveals that our approach outperforms\nbaselines in switching between professional and non-professional text\ngeneration.\n","authors":["Chang Zong","Yuyan Chen","Weiming Lu","Jian Shao","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2403.09131v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2403.18256v1","updated":"2024-03-27T04:56:48Z","published":"2024-03-27T04:56:48Z","title":"Manipulating Neural Path Planners via Slight Perturbations","summary":"  Data-driven neural path planners are attracting increasing interest in the\nrobotics community. However, their neural network components typically come as\nblack boxes, obscuring their underlying decision-making processes. Their\nblack-box nature exposes them to the risk of being compromised via the\ninsertion of hidden malicious behaviors. For example, an attacker may hide\nbehaviors that, when triggered, hijack a delivery robot by guiding it to a\nspecific (albeit wrong) destination, trapping it in a predefined region, or\ninducing unnecessary energy expenditure by causing the robot to repeatedly\ncircle a region. In this paper, we propose a novel approach to specify and\ninject a range of hidden malicious behaviors, known as backdoors, into neural\npath planners. Our approach provides a concise but flexible way to define these\nbehaviors, and we show that hidden behaviors can be triggered by slight\nperturbations (e.g., inserting a tiny unnoticeable object), that can\nnonetheless significantly compromise their integrity. We also discuss potential\ntechniques to identify these backdoors aimed at alleviating such risks. We\ndemonstrate our approach on both sampling-based and search-based neural path\nplanners.\n","authors":["Zikang Xiong","Suresh Jagannathan"],"pdf_url":"https://arxiv.org/pdf/2403.18256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18252v1","updated":"2024-03-27T04:49:23Z","published":"2024-03-27T04:49:23Z","title":"Beyond Embeddings: The Promise of Visual Table in Multi-Modal Models","summary":"  Visual representation learning has been a cornerstone in computer vision,\nevolving from supervised learning with human-annotated labels to aligning\nimage-text pairs from the Internet. Despite recent advancements in multi-modal\nlarge language models (MLLMs), the visual representations they rely on, such as\nCLIP embeddings, often lack access to external world knowledge critical for\nreal-world visual reasoning. In this work, we propose Visual Table, a novel\nvisual representation tailored for MLLMs. It provides hierarchical text\ndescriptions of holistic visual scenes, consisting of a scene description and\nmultiple object-centric descriptions that encompass categories, attributes, and\nknowledge at instance level. We further develop a scalable generator for visual\ntable generation and train it on small-scale annotations from GPT4V. Extensive\nevaluations demonstrate that, with generated visual tables as additional visual\nrepresentations, our model can consistently outperform the state-of-the-art\n(SOTA) MLLMs across diverse benchmarks. When visual tables serve as standalone\nvisual representations, our model can closely match or even beat the SOTA MLLMs\nthat are built on CLIP visual embeddings. Our code is available at\nhttps://github.com/LaVi-Lab/Visual-Table.\n","authors":["Yiwu Zhong","Zi-Yuan Hu","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18252v1.pdf","comment":"Project page: https://github.com/LaVi-Lab/Visual-Table"},{"id":"http://arxiv.org/abs/2403.18243v1","updated":"2024-03-27T04:20:18Z","published":"2024-03-27T04:20:18Z","title":"Boosting Conversational Question Answering with Fine-Grained\n  Retrieval-Augmentation and Self-Check","summary":"  Retrieval-Augmented Generation (RAG) aims to generate more reliable and\naccurate responses, by augmenting large language models (LLMs) with the\nexternal vast and dynamic knowledge. Most previous work focuses on using RAG\nfor single-round question answering, while how to adapt RAG to the complex\nconversational setting wherein the question is interdependent on the preceding\ncontext is not well studied. In this paper, we propose a conversation-level RAG\napproach, which incorporates fine-grained retrieval augmentation and self-check\nfor conversational question answering (CQA). In particular, our approach\nconsists of three components, namely conversational question refiner,\nfine-grained retriever and self-check based response generator, which work\ncollaboratively for question understanding and relevant information acquisition\nin conversational settings. Extensive experiments demonstrate the great\nadvantages of our approach over the state-of-the-art baselines. Moreover, we\nalso release a Chinese CQA dataset with new features including reformulated\nquestion, extracted keyword, retrieved paragraphs and their helpfulness, which\nfacilitates further researches in RAG enhanced CQA.\n","authors":["Linhao Ye","Zhikai Lei","Jianghao Yin","Qin Chen","Jie Zhou","Liang He"],"pdf_url":"https://arxiv.org/pdf/2403.18243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18241v1","updated":"2024-03-27T04:09:34Z","published":"2024-03-27T04:09:34Z","title":"NeuSDFusion: A Spatial-Aware Generative Model for 3D Shape Completion,\n  Reconstruction, and Generation","summary":"  3D shape generation aims to produce innovative 3D content adhering to\nspecific conditions and constraints. Existing methods often decompose 3D shapes\ninto a sequence of localized components, treating each element in isolation\nwithout considering spatial consistency. As a result, these approaches exhibit\nlimited versatility in 3D data representation and shape generation, hindering\ntheir ability to generate highly diverse 3D shapes that comply with the\nspecified constraints. In this paper, we introduce a novel spatial-aware 3D\nshape generation framework that leverages 2D plane representations for enhanced\n3D shape modeling. To ensure spatial coherence and reduce memory usage, we\nincorporate a hybrid shape representation technique that directly learns a\ncontinuous signed distance field representation of the 3D shape using\northogonal 2D planes. Additionally, we meticulously enforce spatial\ncorrespondences across distinct planes using a transformer-based autoencoder\nstructure, promoting the preservation of spatial relationships in the generated\n3D shapes. This yields an algorithm that consistently outperforms\nstate-of-the-art 3D shape generation methods on various tasks, including\nunconditional shape generation, multi-modal shape completion, single-view\nreconstruction, and text-to-shape synthesis.\n","authors":["Ruikai Cui","Weizhe Liu","Weixuan Sun","Senbo Wang","Taizhang Shang","Yang Li","Xibin Song","Han Yan","Zhennan Wu","Shenzhou Chen","Hongdong Li","Pan Ji"],"pdf_url":"https://arxiv.org/pdf/2403.18241v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18230v1","updated":"2024-03-27T03:33:32Z","published":"2024-03-27T03:33:32Z","title":"Large Language Models Need Consultants for Reasoning: Becoming an Expert\n  in a Complex Human System Through Behavior Simulation","summary":"  Large language models (LLMs), in conjunction with various reasoning\nreinforcement methodologies, have demonstrated remarkable capabilities\ncomparable to humans in fields such as mathematics, law, coding, common sense,\nand world knowledge. In this paper, we delve into the reasoning abilities of\nLLMs within complex human systems. We propose a novel reasoning framework,\ntermed ``Mosaic Expert Observation Wall'' (MEOW) exploiting\ngenerative-agents-based simulation technique. In the MEOW framework, simulated\ndata are utilized to train an expert model concentrating ``experience'' about a\nspecific task in each independent time of simulation. It is the accumulated\n``experience'' through the simulation that makes for an expert on a task in a\ncomplex human system. We conduct the experiments within a communication game\nthat mirrors real-world security scenarios. The results indicate that our\nproposed methodology can cooperate with existing methodologies to enhance the\nreasoning abilities of LLMs in complex human systems.\n","authors":["Chuwen Wang","Shirong Zeng","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16427v3","updated":"2024-03-27T03:27:24Z","published":"2024-03-25T05:12:18Z","title":"Re2LLM: Reflective Reinforcement Large Language Model for Session-based\n  Recommendation","summary":"  Large Language Models (LLMs) are emerging as promising approaches to enhance\nsession-based recommendation (SBR), where both prompt-based and\nfine-tuning-based methods have been widely investigated to align LLMs with SBR.\nHowever, the former methods struggle with optimal prompts to elicit the correct\nreasoning of LLMs due to the lack of task-specific feedback, leading to\nunsatisfactory recommendations. Although the latter methods attempt to\nfine-tune LLMs with domain-specific knowledge, they face limitations such as\nhigh computational costs and reliance on open-source backbones. To address such\nissues, we propose a Reflective Reinforcement Large Language Model (Re2LLM) for\nSBR, guiding LLMs to focus on specialized knowledge essential for more accurate\nrecommendations effectively and efficiently. In particular, we first design the\nReflective Exploration Module to effectively extract knowledge that is readily\nunderstandable and digestible by LLMs. To be specific, we direct LLMs to\nexamine recommendation errors through self-reflection and construct a knowledge\nbase (KB) comprising hints capable of rectifying these errors. To efficiently\nelicit the correct reasoning of LLMs, we further devise the Reinforcement\nUtilization Module to train a lightweight retrieval agent. It learns to select\nhints from the constructed KB based on the task-specific feedback, where the\nhints can serve as guidance to help correct LLMs reasoning for better\nrecommendations. Extensive experiments on multiple real-world datasets\ndemonstrate that our method consistently outperforms state-of-the-art methods.\n","authors":["Ziyan Wang","Yingpeng Du","Zhu Sun","Haoyan Chua","Kaidong Feng","Wenya Wang","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.16427v3.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.18223v1","updated":"2024-03-27T03:25:45Z","published":"2024-03-27T03:25:45Z","title":"A Transformer-Based Framework for Payload Malware Detection and\n  Classification","summary":"  As malicious cyber threats become more sophisticated in breaching computer\nnetworks, the need for effective intrusion detection systems (IDSs) becomes\ncrucial. Techniques such as Deep Packet Inspection (DPI) have been introduced\nto allow IDSs analyze the content of network packets, providing more context\nfor identifying potential threats. IDSs traditionally rely on using\nanomaly-based and signature-based detection techniques to detect unrecognized\nand suspicious activity. Deep learning techniques have shown great potential in\nDPI for IDSs due to their efficiency in learning intricate patterns from the\npacket content being transmitted through the network. In this paper, we propose\na revolutionary DPI algorithm based on transformers adapted for the purpose of\ndetecting malicious traffic with a classifier head. Transformers learn the\ncomplex content of sequence data and generalize them well to similar scenarios\nthanks to their self-attention mechanism. Our proposed method uses the raw\npayload bytes that represent the packet contents and is deployed as\nman-in-the-middle. The payload bytes are used to detect malicious packets and\nclassify their types. Experimental results on the UNSW-NB15 and CIC-IOT23\ndatasets demonstrate that our transformer-based model is effective in\ndistinguishing malicious from benign traffic in the test dataset, attaining an\naverage accuracy of 79\\% using binary classification and 72\\% on the\nmulti-classification experiment, both using solely payload bytes.\n","authors":["Kyle Stein","Arash Mahyari","Guillermo Francia III","Eman El-Sheikh"],"pdf_url":"https://arxiv.org/pdf/2403.18223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18219v1","updated":"2024-03-27T03:07:18Z","published":"2024-03-27T03:07:18Z","title":"From Two-Dimensional to Three-Dimensional Environment with Q-Learning:\n  Modeling Autonomous Navigation with Reinforcement Learning and no Libraries","summary":"  Reinforcement learning (RL) algorithms have become indispensable tools in\nartificial intelligence, empowering agents to acquire optimal decision-making\npolicies through interactions with their environment and feedback mechanisms.\nThis study explores the performance of RL agents in both two-dimensional (2D)\nand three-dimensional (3D) environments, aiming to research the dynamics of\nlearning across different spatial dimensions. A key aspect of this\ninvestigation is the absence of pre-made libraries for learning, with the\nalgorithm developed exclusively through computational mathematics. The\nmethodological framework centers on RL principles, employing a Q-learning agent\nclass and distinct environment classes tailored to each spatial dimension. The\nresearch aims to address the question: How do reinforcement learning agents\nadapt and perform in environments of varying spatial dimensions, particularly\nin 2D and 3D settings? Through empirical analysis, the study evaluates agents'\nlearning trajectories and adaptation processes, revealing insights into the\nefficacy of RL algorithms in navigating complex, multi-dimensional spaces.\nReflections on the findings prompt considerations for future research,\nparticularly in understanding the dynamics of learning in higher-dimensional\nenvironments.\n","authors":["Ergon Cugler de Moraes Silva"],"pdf_url":"https://arxiv.org/pdf/2403.18219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04357v5","updated":"2024-03-27T03:06:13Z","published":"2023-06-07T11:40:07Z","title":"Dial-MAE: ConTextual Masked Auto-Encoder for Retrieval-based Dialogue\n  Systems","summary":"  Dialogue response selection aims to select an appropriate response from\nseveral candidates based on a given user and system utterance history. Most\nexisting works primarily focus on post-training and fine-tuning tailored for\ncross-encoders. However, there are no post-training methods tailored for dense\nencoders in dialogue response selection. We argue that when the current\nlanguage model, based on dense dialogue systems (such as BERT), is employed as\na dense encoder, it separately encodes dialogue context and response, leading\nto a struggle to achieve the alignment of both representations. Thus, we\npropose Dial-MAE (Dialogue Contextual Masking Auto-Encoder), a straightforward\nyet effective post-training technique tailored for dense encoders in dialogue\nresponse selection. Dial-MAE uses an asymmetric encoder-decoder architecture to\ncompress the dialogue semantics into dense vectors, which achieves better\nalignment between the features of the dialogue context and response. Our\nexperiments have demonstrated that Dial-MAE is highly effective, achieving\nstate-of-the-art performance on two commonly evaluated benchmarks.\n","authors":["Zhenpeng Su","Xing Wu","Wei Zhou","Guangyuan Ma","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04357v5.pdf","comment":"This paper has been accepted by NAACL 2024"},{"id":"http://arxiv.org/abs/2403.18218v1","updated":"2024-03-27T03:04:21Z","published":"2024-03-27T03:04:21Z","title":"Leveraging Large Language Models for Fuzzy String Matching in Political\n  Science","summary":"  Fuzzy string matching remains a key issue when political scientists combine\ndata from different sources. Existing matching methods invariably rely on\nstring distances, such as Levenshtein distance and cosine similarity. As such,\nthey are inherently incapable of matching strings that refer to the same entity\nwith different names such as ''JP Morgan'' and ''Chase Bank'', ''DPRK'' and\n''North Korea'', ''Chuck Fleischmann (R)'' and ''Charles Fleischmann (R)''. In\nthis letter, we propose to use large language models to entirely sidestep this\nproblem in an easy and intuitive manner. Extensive experiments show that our\nproposed methods can improve the state of the art by as much as 39% in terms of\naverage precision while being substantially easier and more intuitive to use by\npolitical scientists. Moreover, our results are robust against various\ntemperatures. We further note that enhanced prompting can lead to additional\nperformance improvements.\n","authors":["Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18218v1.pdf","comment":"7 pages, 2 figures, 1 table;"},{"id":"http://arxiv.org/abs/2403.00868v2","updated":"2024-03-27T03:03:00Z","published":"2024-03-01T04:39:16Z","title":"SoftTiger: A Clinical Foundation Model for Healthcare Workflows","summary":"  We introduce SoftTiger, a clinical large language model (CLaM) designed as a\nfoundation model for healthcare workflows. The narrative and unstructured\nnature of clinical notes is a major obstacle for healthcare intelligentization.\nWe address a critical problem of structuring clinical notes into clinical data,\naccording to international interoperability standards. We collect and annotate\ndata for three subtasks, namely, international patient summary, clinical\nimpression and medical encounter. We then supervised fine-tuned a\nstate-of-the-art LLM using public and credentialed clinical data. The training\nis orchestrated in a way that the target model can first support basic clinical\ntasks such as abbreviation expansion and temporal information extraction, and\nthen learn to perform more complex downstream clinical tasks. Moreover, we\naddress several modeling challenges in the healthcare context, e.g., extra long\ncontext window. Our blind pairwise evaluation shows that SoftTiger outperforms\nother popular open-source models and GPT-3.5, comparable to Gemini-pro, with a\nmild gap from GPT-4. We believe that LLMs may become a step-stone towards\nhealthcare digitalization and democratization. Therefore, we publicly release\nSoftTiger models at scales of 13 billion and 70 billion parameters, as well as\ndatasets and code for our innovative scalable evaluation, hopefully, making a\nsignificant contribution to the healthcare industry.\n","authors":["Ye Chen","Igor Couto","Wei Cai","Cong Fu","Bruno Dorneles"],"pdf_url":"https://arxiv.org/pdf/2403.00868v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17304v2","updated":"2024-03-27T02:59:57Z","published":"2024-02-27T08:27:15Z","title":"Probing Multimodal Large Language Models for Global and Local Semantic\n  Representations","summary":"  The advancement of Multimodal Large Language Models (MLLMs) has greatly\naccelerated the development of applications in understanding integrated texts\nand images. Recent works leverage image-caption datasets to train MLLMs,\nachieving state-of-the-art performance on image-to-text tasks. However, there\nare few studies exploring which layers of MLLMs make the most effort to the\nglobal image information, which plays vital roles in multimodal comprehension\nand generation. In this study, we find that the intermediate layers of models\ncan encode more global semantic information, whose representation vectors\nperform better on visual-language entailment tasks, rather than the topmost\nlayers. We further probe models regarding local semantic representations\nthrough object recognition tasks. We find that the topmost layers may\nexcessively focus on local information, leading to a diminished ability to\nencode global information. Our code and data are released via\nhttps://github.com/kobayashikanna01/probing_MLLM_rep.\n","authors":["Mingxu Tao","Quzhe Huang","Kun Xu","Liwei Chen","Yansong Feng","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.17304v2.pdf","comment":"Accepted by LREC-COLING 2024 as a short paper (Camera Ready)"},{"id":"http://arxiv.org/abs/2304.06427v2","updated":"2024-03-27T02:58:26Z","published":"2023-04-13T11:46:32Z","title":"In-Distribution and Out-of-Distribution Self-supervised ECG\n  Representation Learning for Arrhythmia Detection","summary":"  This paper presents a systematic investigation into the effectiveness of\nSelf-Supervised Learning (SSL) methods for Electrocardiogram (ECG) arrhythmia\ndetection. We begin by conducting a novel analysis of the data distributions on\nthree popular ECG-based arrhythmia datasets: PTB-XL, Chapman, and Ribeiro. To\nthe best of our knowledge, our study is the first to quantitatively explore and\ncharacterize these distributions in the area. We then perform a comprehensive\nset of experiments using different augmentations and parameters to evaluate the\neffectiveness of various SSL methods, namely SimCRL, BYOL, and SwAV, for ECG\nrepresentation learning, where we observe the best performance achieved by\nSwAV. Furthermore, our analysis shows that SSL methods achieve highly\ncompetitive results to those achieved by supervised state-of-the-art methods.\nTo further assess the performance of these methods on both In-Distribution (ID)\nand Out-of-Distribution (OOD) ECG data, we conduct cross-dataset training and\ntesting experiments. Our comprehensive experiments show almost identical\nresults when comparing ID and OOD schemes, indicating that SSL techniques can\nlearn highly effective representations that generalize well across different\nOOD datasets. This finding can have major implications for ECG-based arrhythmia\ndetection. Lastly, to further analyze our results, we perform detailed\nper-disease studies on the performance of the SSL methods on the three\ndatasets.\n","authors":["Sahar Soltanieh","Javad Hashemi","Ali Etemad"],"pdf_url":"https://arxiv.org/pdf/2304.06427v2.pdf","comment":"This paper has been published in the IEEE Journal of Biomedical and\n  Health Informatics (JBHI). Copyright IEEE. Please cite as: S. Soltanieh, J.\n  Hashemi and A. Etemad, \"In-Distribution and Out-of-Distribution\n  Self-Supervised ECG Representation Learning for Arrhythmia Detection,\" in\n  IEEE Journal of Biomedical and Health Informatics, vol. 28, no. 2, pp.\n  789-800, Feb. 2024"},{"id":"http://arxiv.org/abs/2403.18212v1","updated":"2024-03-27T02:46:09Z","published":"2024-03-27T02:46:09Z","title":"Preference-Based Planning in Stochastic Environments: From\n  Partially-Ordered Temporal Goals to Most Preferred Policies","summary":"  Human preferences are not always represented via complete linear orders: It\nis natural to employ partially-ordered preferences for expressing incomparable\noutcomes. In this work, we consider decision-making and probabilistic planning\nin stochastic systems modeled as Markov decision processes (MDPs), given a\npartially ordered preference over a set of temporally extended goals.\nSpecifically, each temporally extended goal is expressed using a formula in\nLinear Temporal Logic on Finite Traces (LTL$_f$). To plan with the partially\nordered preference, we introduce order theory to map a preference over temporal\ngoals to a preference over policies for the MDP. Accordingly, a most preferred\npolicy under a stochastic ordering induces a stochastic nondominated\nprobability distribution over the finite paths in the MDP. To synthesize a most\npreferred policy, our technical approach includes two key steps. In the first\nstep, we develop a procedure to transform a partially ordered preference over\ntemporal goals into a computational model, called preference automaton, which\nis a semi-automaton with a partial order over acceptance conditions. In the\nsecond step, we prove that finding a most preferred policy is equivalent to\ncomputing a Pareto-optimal policy in a multi-objective MDP that is constructed\nfrom the original MDP, the preference automaton, and the chosen stochastic\nordering relation. Throughout the paper, we employ running examples to\nillustrate the proposed preference specification and solution approaches. We\ndemonstrate the efficacy of our algorithm using these examples, providing\ndetailed analysis, and then discuss several potential future directions.\n","authors":["Hazhar Rahmani","Abhishek N. Kulkarni","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2403.18212v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2209.12267"},{"id":"http://arxiv.org/abs/2403.18209v1","updated":"2024-03-27T02:41:52Z","published":"2024-03-27T02:41:52Z","title":"Long and Short-Term Constraints Driven Safe Reinforcement Learning for\n  Autonomous Driving","summary":"  Reinforcement learning (RL) has been widely used in decision-making tasks,\nbut it cannot guarantee the agent's safety in the training process due to the\nrequirements of interaction with the environment, which seriously limits its\nindustrial applications such as autonomous driving. Safe RL methods are\ndeveloped to handle this issue by constraining the expected safety violation\ncosts as a training objective, but they still permit unsafe state occurrence,\nwhich is unacceptable in autonomous driving tasks. Moreover, these methods are\ndifficult to achieve a balance between the cost and return expectations, which\nleads to learning performance degradation for the algorithms. In this paper, we\npropose a novel algorithm based on the long and short-term constraints (LSTC)\nfor safe RL. The short-term constraint aims to guarantee the short-term state\nsafety that the vehicle explores, while the long-term constraint ensures the\noverall safety of the vehicle throughout the decision-making process. In\naddition, we develop a safe RL method with dual-constraint optimization based\non the Lagrange multiplier to optimize the training process for end-to-end\nautonomous driving. Comprehensive experiments were conducted on the MetaDrive\nsimulator. Experimental results demonstrate that the proposed method achieves\nhigher safety in continuous state and action tasks, and exhibits higher\nexploration performance in long-distance decision-making tasks compared with\nstate-of-the-art methods.\n","authors":["Xuemin Hu","Pan Chen","Yijun Wen","Bo Tang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2403.18209v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17456v2","updated":"2024-03-27T02:39:26Z","published":"2024-03-26T07:41:54Z","title":"Imitating Cost-Constrained Behaviors in Reinforcement Learning","summary":"  Complex planning and scheduling problems have long been solved using various\noptimization or heuristic approaches. In recent years, imitation learning that\naims to learn from expert demonstrations has been proposed as a viable\nalternative to solving these problems. Generally speaking, imitation learning\nis designed to learn either the reward (or preference) model or directly the\nbehavioral policy by observing the behavior of an expert. Existing work in\nimitation learning and inverse reinforcement learning has focused on imitation\nprimarily in unconstrained settings (e.g., no limit on fuel consumed by the\nvehicle). However, in many real-world domains, the behavior of an expert is\ngoverned not only by reward (or preference) but also by constraints. For\ninstance, decisions on self-driving delivery vehicles are dependent not only on\nthe route preferences/rewards (depending on past demand data) but also on the\nfuel in the vehicle and the time available. In such problems, imitation\nlearning is challenging as decisions are not only dictated by the reward model\nbut are also dependent on a cost-constrained model. In this paper, we provide\nmultiple methods that match expert distributions in the presence of trajectory\ncost constraints through (a) Lagrangian-based method; (b) Meta-gradients to\nfind a good trade-off between expected return and minimizing constraint\nviolation; and (c) Cost-violation-based alternating gradient. We empirically\nshow that leading imitation learning approaches imitate cost-constrained\nbehaviors poorly and our meta-gradient-based approach achieves the best\nperformance.\n","authors":["Qian Shao","Pradeep Varakantham","Shih-Fen Cheng"],"pdf_url":"https://arxiv.org/pdf/2403.17456v2.pdf","comment":"Accepted to the 34th International Conference on Automated Planning\n  and Scheduling (ICAPS-24)"},{"id":"http://arxiv.org/abs/2403.18208v1","updated":"2024-03-27T02:39:23Z","published":"2024-03-27T02:39:23Z","title":"An Evolutionary Network Architecture Search Framework with Adaptive\n  Multimodal Fusion for Hand Gesture Recognition","summary":"  Hand gesture recognition (HGR) based on multimodal data has attracted\nconsiderable attention owing to its great potential in applications. Various\nmanually designed multimodal deep networks have performed well in multimodal\nHGR (MHGR), but most of existing algorithms require a lot of expert experience\nand time-consuming manual trials. To address these issues, we propose an\nevolutionary network architecture search framework with the adaptive multimodel\nfusion (AMF-ENAS). Specifically, we design an encoding space that\nsimultaneously considers fusion positions and ratios of the multimodal data,\nallowing for the automatic construction of multimodal networks with different\narchitectures through decoding. Additionally, we consider three input streams\ncorresponding to intra-modal surface electromyography (sEMG), intra-modal\naccelerometer (ACC), and inter-modal sEMG-ACC. To automatically adapt to\nvarious datasets, the ENAS framework is designed to automatically search a MHGR\nnetwork with appropriate fusion positions and ratios. To the best of our\nknowledge, this is the first time that ENAS has been utilized in MHGR to tackle\nissues related to the fusion position and ratio of multimodal data.\nExperimental results demonstrate that AMF-ENAS achieves state-of-the-art\nperformance on the Ninapro DB2, DB3, and DB7 datasets.\n","authors":["Yizhang Xia","Shihao Song","Zhanglu Hou","Junwen Xu","Juan Zou","Yuan Liu","Shengxiang Yang"],"pdf_url":"https://arxiv.org/pdf/2403.18208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18205v1","updated":"2024-03-27T02:31:54Z","published":"2024-03-27T02:31:54Z","title":"Exploring the Privacy Protection Capabilities of Chinese Large Language\n  Models","summary":"  Large language models (LLMs), renowned for their impressive capabilities in\nvarious tasks, have significantly advanced artificial intelligence. Yet, these\nadvancements have raised growing concerns about privacy and security\nimplications. To address these issues and explain the risks inherent in these\nmodels, we have devised a three-tiered progressive framework tailored for\nevaluating privacy in language systems. This framework consists of\nprogressively complex and in-depth privacy test tasks at each tier. Our primary\nobjective is to comprehensively evaluate the sensitivity of large language\nmodels to private information, examining how effectively they discern, manage,\nand safeguard sensitive data in diverse scenarios. This systematic evaluation\nhelps us understand the degree to which these models comply with privacy\nprotection guidelines and the effectiveness of their inherent safeguards\nagainst privacy breaches. Our observations indicate that existing Chinese large\nlanguage models universally show privacy protection shortcomings. It seems that\nat the moment this widespread issue is unavoidable and may pose corresponding\nprivacy risks in applications based on these models.\n","authors":["Yuqi Yang","Xiaowen Huang","Jitao Sang"],"pdf_url":"https://arxiv.org/pdf/2403.18205v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2403.18203v1","updated":"2024-03-27T02:24:38Z","published":"2024-03-27T02:24:38Z","title":"EndToEndML: An Open-Source End-to-End Pipeline for Machine Learning\n  Applications","summary":"  Artificial intelligence (AI) techniques are widely applied in the life\nsciences. However, applying innovative AI techniques to understand and\ndeconvolute biological complexity is hindered by the learning curve for life\nscience scientists to understand and use computing languages. An open-source,\nuser-friendly interface for AI models, that does not require programming skills\nto analyze complex biological data will be extremely valuable to the\nbioinformatics community. With easy access to different sequencing technologies\nand increased interest in different 'omics' studies, the number of biological\ndatasets being generated has increased and analyzing these high-throughput\ndatasets is computationally demanding. The majority of AI libraries today\nrequire advanced programming skills as well as machine learning, data\npreprocessing, and visualization skills. In this research, we propose a\nweb-based end-to-end pipeline that is capable of preprocessing, training,\nevaluating, and visualizing machine learning (ML) models without manual\nintervention or coding expertise. By integrating traditional machine learning\nand deep neural network models with visualizations, our library assists in\nrecognizing, classifying, clustering, and predicting a wide range of\nmulti-modal, multi-sensor datasets, including images, languages, and\none-dimensional numerical data, for drug discovery, pathogen classification,\nand medical diagnostics.\n","authors":["Nisha Pillai","Athish Ram Das","Moses Ayoola","Ganga Gireesan","Bindu Nanduri","Mahalingam Ramkumar"],"pdf_url":"https://arxiv.org/pdf/2403.18203v1.pdf","comment":"2024 7th International Conference on Information and Computer\n  Technologies (ICICT)"},{"id":"http://arxiv.org/abs/2303.13300v3","updated":"2024-03-27T02:23:29Z","published":"2023-03-23T14:37:35Z","title":"The Innovation Paradox: Concept Space Expansion with Diminishing\n  Originality and the Promise of Creative AI","summary":"  Innovation, typically spurred by reusing, recombining, and synthesizing\nexisting concepts, is expected to result in an exponential growth of the\nconcept space over time. However, our statistical analysis of TechNet, which is\na comprehensive technology semantic network encompassing over four million\nconcepts derived from patent texts, reveals a linear rather than exponential\nexpansion of the overall technological concept space. Moreover, there is a\nnotable decline in the originality of newly created concepts. These trends can\nbe attributed to the constraints of human cognitive abilities to innovate\nbeyond an ever-growing space of prior art, among other factors. Integrating\ncreative artificial intelligence (CAI) into the innovation process holds the\npotential to overcome these limitations and alter the observed trends in the\nfuture.\n","authors":["Serhad Sarica","Jianxi Luo"],"pdf_url":"https://arxiv.org/pdf/2303.13300v3.pdf","comment":"Forthcoming on the Design Science"},{"id":"http://arxiv.org/abs/2403.00154v2","updated":"2024-03-27T02:21:03Z","published":"2024-02-29T22:11:20Z","title":"LLMs in Political Science: Heralding a New Era of Visual Analysis","summary":"  Interest is increasing among political scientists in leveraging the extensive\ninformation available in images. However, the challenge of interpreting these\nimages lies in the need for specialized knowledge in computer vision and access\nto specialized hardware. As a result, image analysis has been limited to a\nrelatively small group within the political science community. This landscape\ncould potentially change thanks to the rise of large language models (LLMs).\nThis paper aims to raise awareness of the feasibility of using Gemini for image\ncontent analysis. A retrospective analysis was conducted on a corpus of 688\nimages. Content reports were elicited from Gemini for each image and then\nmanually evaluated by the authors. We find that Gemini is highly accurate in\nperforming object detection, which is arguably the most common and fundamental\ntask in image analysis for political scientists. Equally important, we show\nthat it is easy to implement as the entire command consists of a single prompt\nin natural language; it is fast to run and should meet the time budget of most\nresearchers; and it is free to use and does not require any specialized\nhardware. In addition, we illustrate how political scientists can leverage\nGemini for other image understanding tasks, including face identification,\nsentiment analysis, and caption generation. Our findings suggest that Gemini\nand other similar LLMs have the potential to drastically stimulate and\naccelerate image research in political science and social sciences more\nbroadly.\n","authors":["Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2403.00154v2.pdf","comment":"7 pages, 3 tables"},{"id":"http://arxiv.org/abs/2403.18196v1","updated":"2024-03-27T02:13:20Z","published":"2024-03-27T02:13:20Z","title":"Looking Beyond What You See: An Empirical Analysis on Subgroup\n  Intersectional Fairness for Multi-label Chest X-ray Classification Using\n  Social Determinants of Racial Health Inequities","summary":"  There has been significant progress in implementing deep learning models in\ndisease diagnosis using chest X- rays. Despite these advancements, inherent\nbiases in these models can lead to disparities in prediction accuracy across\nprotected groups. In this study, we propose a framework to achieve accurate\ndiagnostic outcomes and ensure fairness across intersectional groups in\nhigh-dimensional chest X- ray multi-label classification. Transcending\ntraditional protected attributes, we consider complex interactions within\nsocial determinants, enabling a more granular benchmark and evaluation of\nfairness. We present a simple and robust method that involves retraining the\nlast classification layer of pre-trained models using a balanced dataset across\ngroups. Additionally, we account for fairness constraints and integrate\nclass-balanced fine-tuning for multi-label settings. The evaluation of our\nmethod on the MIMIC-CXR dataset demonstrates that our framework achieves an\noptimal tradeoff between accuracy and fairness compared to baseline methods.\n","authors":["Dana Moukheiber","Saurabh Mahindre","Lama Moukheiber","Mira Moukheiber","Mingchen Gao"],"pdf_url":"https://arxiv.org/pdf/2403.18196v1.pdf","comment":"ICCV CVAMD 2023"},{"id":"http://arxiv.org/abs/2403.18195v1","updated":"2024-03-27T02:08:12Z","published":"2024-03-27T02:08:12Z","title":"SCANet: Correcting LEGO Assembly Errors with Self-Correct Assembly\n  Network","summary":"  Autonomous assembly in robotics and 3D vision presents significant\nchallenges, particularly in ensuring assembly correctness. Presently,\npredominant methods such as MEPNet focus on assembling components based on\nmanually provided images. However, these approaches often fall short in\nachieving satisfactory results for tasks requiring long-term planning.\nConcurrently, we observe that integrating a self-correction module can\npartially alleviate such issues. Motivated by this concern, we introduce the\nsingle-step assembly error correction task, which involves identifying and\nrectifying misassembled components. To support research in this area, we\npresent the LEGO Error Correction Assembly Dataset (LEGO-ECA), comprising\nmanual images for assembly steps and instances of assembly failures.\nAdditionally, we propose the Self-Correct Assembly Network (SCANet), a novel\nmethod to address this task. SCANet treats assembled components as queries,\ndetermining their correctness in manual images and providing corrections when\nnecessary. Finally, we utilize SCANet to correct the assembly results of\nMEPNet. Experimental results demonstrate that SCANet can identify and correct\nMEPNet's misassembled results, significantly improving the correctness of\nassembly. Our code and dataset are available at\nhttps://github.com/Yaser-wyx/SCANet.\n","authors":["Yuxuan Wan","Kaichen Zhou","jinhong Chen","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16915v3","updated":"2024-03-27T01:53:36Z","published":"2024-03-25T16:32:50Z","title":"Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language\n  Models","summary":"  Fine-tuning in information retrieval systems using pre-trained language\nmodels (PLM-based IR) requires learning query representations and\nquery-document relations, in addition to downstream task-specific learning.\nThis study introduces coarse-tuning as an intermediate learning stage that\nbridges pre-training and fine-tuning. By learning query representations and\nquery-document relations in coarse-tuning, we aim to reduce the load of\nfine-tuning and improve the learning effect of downstream IR tasks. We propose\nQuery-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the\nappropriateness of query-document pairs. Evaluation experiments show that the\nproposed method significantly improves MRR and/or nDCG@5 in four ad-hoc\ndocument retrieval datasets. Furthermore, the results of the query prediction\ntask suggested that coarse-tuning facilitated learning of query representation\nand query-document relations.\n","authors":["Atsushi Keyaki","Ribeka Keyaki"],"pdf_url":"https://arxiv.org/pdf/2403.16915v3.pdf","comment":"Accepted at LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2402.15764v2","updated":"2024-03-27T01:23:58Z","published":"2024-02-24T08:40:30Z","title":"Look Before You Leap: Problem Elaboration Prompting Improves\n  Mathematical Reasoning in Large Language Models","summary":"  Large language models (LLMs) still grapple with complex tasks like\nmathematical reasoning. Despite significant efforts invested in improving\nprefix prompts or reasoning process, the crucial role of problem context might\nhave been neglected. Accurate recognition of inputs is fundamental for solving\nmathematical tasks, as ill-formed problems could potentially mislead LLM's\nreasoning. In this study, we propose a new approach named Problem Elaboration\nPrompting (PEP) to enhance the mathematical capacities of LLMs. Specifically,\nPEP decomposes and elucidates the problem context before reasoning, therefore\nenhancing the context modeling and parsing efficiency. Experiments across\ndatasets and models demonstrate promising performances: (1) PEP demonstrates an\noverall enhancement in various mathematical tasks. For instance, with the\nGPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through\ngreedy decoding and self-consistency, respectively. (2) PEP can be easily\nimplemented and integrated with other prompting methods. (3) PEP shows\nparticular strength in handling distraction problems.\n","authors":["Haoran Liao","Jidong Tian","Shaohua Hu","Hao He","Yaohui Jin"],"pdf_url":"https://arxiv.org/pdf/2402.15764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18183v1","updated":"2024-03-27T01:21:48Z","published":"2024-03-27T01:21:48Z","title":"Can AI Models Appreciate Document Aesthetics? An Exploration of\n  Legibility and Layout Quality in Relation to Prediction Confidence","summary":"  A well-designed document communicates not only through its words but also\nthrough its visual eloquence. Authors utilize aesthetic elements such as\ncolors, fonts, graphics, and layouts to shape the perception of information.\nThoughtful document design, informed by psychological insights, enhances both\nthe visual appeal and the comprehension of the content. While state-of-the-art\ndocument AI models demonstrate the benefits of incorporating layout and image\ndata, it remains unclear whether the nuances of document aesthetics are\neffectively captured. To bridge the gap between human cognition and AI\ninterpretation of aesthetic elements, we formulated hypotheses concerning AI\nbehavior in document understanding tasks, specifically anchored in document\ndesign principles. With a focus on legibility and layout quality, we tested\nfour aspects of aesthetic effects: noise, font-size contrast, alignment, and\ncomplexity, on model confidence using correlational analysis. The results and\nobservations highlight the value of model analysis rooted in document design\ntheories. Our work serves as a trailhead for further studies and we advocate\nfor continued research in this topic to deepen our understanding of how AI\ninterprets document aesthetics.\n","authors":["Hsiu-Wei Yang","Abhinav Agrawal","Pavlos Fragkogiannis","Shubham Nitin Mulay"],"pdf_url":"https://arxiv.org/pdf/2403.18183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01421v3","updated":"2024-03-27T00:38:33Z","published":"2023-02-02T21:21:14Z","title":"Follower Agnostic Methods for Stackelberg Games","summary":"  In this paper, we present an efficient algorithm to solve online Stackelberg\ngames, featuring multiple followers, in a follower-agnostic manner. Unlike\nprevious works, our approach works even when leader has no knowledge about the\nfollowers' utility functions or strategy space. Our algorithm introduces a\nunique gradient estimator, leveraging specially designed strategies to probe\nfollowers. In a departure from traditional assumptions of optimal play, we\nmodel followers' responses using a convergent adaptation rule, allowing for\nrealistic and dynamic interactions. The leader constructs the gradient\nestimator solely based on observations of followers' actions. We provide both\nnon-asymptotic convergence rates to stationary points of the leader's objective\nand demonstrate asymptotic convergence to a \\emph{local Stackelberg\nequilibrium}. To validate the effectiveness of our algorithm, we use this\nalgorithm to solve the problem of incentive design on a large-scale\ntransportation network, showcasing its robustness even when the leader lacks\naccess to followers' demand.\n","authors":["Chinmay Maheshwari","James Cheng","S. Shankar Sasty","Lillian Ratliff","Eric Mazumdar"],"pdf_url":"https://arxiv.org/pdf/2302.01421v3.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2403.18167v1","updated":"2024-03-27T00:23:03Z","published":"2024-03-27T00:23:03Z","title":"Mechanisms of non-factual hallucinations in language models","summary":"  State-of-the-art language models (LMs) sometimes generate non-factual\nhallucinations that misalign with world knowledge. Despite extensive efforts to\ndetect and mitigate hallucinations, understanding their internal mechanisms\nremains elusive. Our study investigates the mechanistic causes of\nhallucination, specifically non-factual ones where the LM incorrectly predicts\nobject attributes in response to subject-relation queries. With causal\nmediation analysis and embedding space projection, we identify two general\nmechanistic causes of hallucinations shared across LMs of various scales and\ndesigns: 1) insufficient subject attribute knowledge in lower layer MLPs, and\n2) failing to select the correct object attribute in upper layer attention\nheads and MLPs. These two mechanisms exhibit varying degrees of subject-object\nassociation, predictive uncertainty and perturbation robustness. Additionally,\nwe scrutinize LM pre-training checkpoints, revealing distinct learning dynamics\nfor the two mechanistic causes of hallucinations. We also highlight how\nattribution features from our causal analysis can effectively construct\nhallucination detectors. Our work proposes a mechanistic understanding of LM\nfactual errors.\n","authors":["Lei Yu","Meng Cao","Jackie Chi Kit Cheung","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2403.18167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10812v2","updated":"2024-03-27T00:15:16Z","published":"2023-12-17T20:39:54Z","title":"Learning to Act without Actions","summary":"  Pre-training large models on vast amounts of web data has proven to be an\neffective approach for obtaining powerful, general models in domains such as\nlanguage and vision. However, this paradigm has not yet taken hold in\nreinforcement learning. This is because videos, the most abundant form of\nembodied behavioral data on the web, lack the action labels required by\nexisting methods for imitating behavior from demonstrations. We introduce\nLatent Action Policies (LAPO), a method for recovering latent action\ninformation, and thereby latent-action policies, world models, and inverse\ndynamics models, purely from videos. LAPO is the first method able to recover\nthe structure of the true action space just from observed dynamics, even in\nchallenging procedurally-generated environments. LAPO enables training\nlatent-action policies that can be rapidly fine-tuned into expert-level\npolicies, either offline using a small action-labeled dataset, or online with\nrewards. LAPO takes a first step towards pre-training powerful, generalist\npolicies and world models on the vast amounts of videos readily available on\nthe web.\n","authors":["Dominik Schmidt","Minqi Jiang"],"pdf_url":"https://arxiv.org/pdf/2312.10812v2.pdf","comment":"Accepted at ICLR 2024 (spotlight). The code can be found at\n  http://github.com/schmidtdominik/LAPO"}],"Computational Engineering":[{"id":"http://arxiv.org/abs/2310.05649v2","updated":"2024-03-27T16:13:46Z","published":"2023-10-09T12:01:15Z","title":"Context, Composition, Automation, and Communication -- The C2AC Roadmap\n  for Modeling and Simulation","summary":"  Simulation has become, in many application areas, a sine-qua-non. Most\nrecently, COVID-19 has underlined the importance of simulation studies and\nlimitations in current practices and methods. We identify four goals of\nmethodological work for addressing these limitations. The first is to provide\nbetter support for capturing, representing, and evaluating the context of\nsimulation studies, including research questions, assumptions, requirements,\nand activities contributing to a simulation study. In addition, the composition\nof simulation models and other simulation studies' products must be supported\nbeyond syntactical coherence, including aspects of semantics and purpose,\nenabling their effective reuse. A higher degree of automating simulation\nstudies will contribute to more systematic, standardized simulation studies and\ntheir efficiency. Finally, it is essential to invest increased effort into\neffectively communicating results and the processes involved in simulation\nstudies to enable their use in research and decision-making. These goals are\nnot pursued independently of each other, but they will benefit from and\nsometimes even rely on advances in other subfields. In the present paper, we\nexplore the basis and interdependencies evident in current research and\npractice and delineate future research directions based on these\nconsiderations.\n","authors":["Adelinde Uhrmacher","Peter Frazier","Reiner Hähnle","Franziska Klügl","Fabian Lorig","Bertram Ludäscher","Laura Nenzi","Cristina Ruiz-Martin","Bernhard Rumpe","Claudia Szabo","Gabriel A. Wainer","Pia Wilsdorf"],"pdf_url":"https://arxiv.org/pdf/2310.05649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07494v3","updated":"2024-03-27T16:06:34Z","published":"2024-01-15T06:26:53Z","title":"Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering\n  Tasks","summary":"  Computational efficiency and non-adversarial robustness are critical factors\nin real-world engineering applications. Yet, conventional neural networks often\nfall short in addressing both simultaneously, or even separately. Drawing\ninsights from natural physical systems and existing literature, it is known\nthat an input convex architecture enhances computational efficiency, while a\nLipschitz-constrained architecture bolsters non-adversarial robustness. By\nleveraging the strengths of convexity and Lipschitz continuity, we develop a\nnovel network architecture, termed Input Convex Lipschitz Recurrent Neural\nNetworks. This model is explicitly designed for fast and robust\noptimization-based tasks and outperforms existing recurrent units across a\nspectrum of engineering tasks in terms of computational efficiency and\nnon-adversarial robustness, including real-world solar irradiance prediction\nfor Solar PV system planning at LHT Holdings in Singapore and real-time Model\nPredictive Control optimization for a nonlinear chemical reactor.\n","authors":["Zihao Wang","P S Pravin","Zhe Wu"],"pdf_url":"https://arxiv.org/pdf/2401.07494v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18416v1","updated":"2024-03-27T10:08:48Z","published":"2024-03-27T10:08:48Z","title":"A Delaunay Refinement Algorithm for the Particle Finite Element Method\n  applied to Free Surface Flows","summary":"  This paper proposes two contributions to the calculation of free surface\nflows using the particle finite element method (PFEM). The PFEM is based on a\nLagrangian approach: a set of particles defines the fluid. Then, unlike a pure\nLagrangian method, all the particles are connected by a triangular mesh. The\ndifficulty lies in locating the free surface from this mesh. It is a matter of\ndeciding which of the elements in the mesh are part of the fluid domain, and to\ndefine a boundary - the free surface. Then, the incompressible Navier-Stokes\nequations are solved on the fluid domain and the particles' position is updated\nusing the resulting velocity vector. Our first contribution is to propose an\napproach to adapt the mesh with theoretical guarantees of quality: the mesh\ngeneration community has acquired a lot of experience and understanding about\nmesh adaptation approaches with guarantees of quality on the final mesh. We use\nhere a Delaunay refinement strategy, allowing to insert and remove nodes while\ngradually improving mesh quality. We show that this allows to create stable and\nsmooth free surface geometries. Our PFEM approach models the topological\nevolution of one fluid. It is nevertheless necessary to apply conditions on the\ndomain boundaries. When a boundary is a free surface, the flow on the other\nside is not modelled, it is represented by an external pressure. On the\nexternal free surface boundary, atmospheric pressure can be imposed.\nNevertheless, there may be internal free surfaces: the fluid can fully\nencapsulate cavities to form bubbles. The pressure required to maintain the\nvolume of those bubbles is a priori unknown. We propose a multi-point\nconstraint approach to enforce global incompressibility of those empty bubbles.\nThis approach allows to accurately model bubbly flows that involve two fluids\nwith large density differences, while only modelling the heavier fluid.\n","authors":["Thomas Leyssens","Michel Henry","Jonathan Lambrechts","Jean-Francois Remacle"],"pdf_url":"https://arxiv.org/pdf/2403.18416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18369v1","updated":"2024-03-27T09:00:02Z","published":"2024-03-27T09:00:02Z","title":"Damage Mechanics Challenge: Predictions based on the phase field\n  fracture model","summary":"  In this work, we describe our contribution to the Purdue-SANDIA-LLNL\n\\emph{Damage Mechanics Challenge}. The phase field fracture model is adopted to\nblindly estimate the failure characteristics of the challenge test, an\nunconventional three-point bending experiment on an additively manufactured\nrock resembling a type of gypsum. The model is formulated in a variationally\nconsistent fashion, incorporating a volumetric-deviatoric strain energy\ndecomposition, and the numerical implementation adopts a monolithic\nunconditionally stable solution scheme. Our focus is on providing an efficient\nand simple yet rigorous approach capable of delivering accurate predictions\nbased solely on physical parameters. Model inputs are Young's modulus $E$,\nPoisson's ratio $\\nu$, toughness $G_c$ and strength $\\sigma_c$ (as determined\nby the choice of phase field length scale $\\ell$). We show that a single mode I\nthree-point bending test is sufficient to calibrate the model, and that the\ncalibrated model can then reliably predict the force versus displacement\nresponses, crack paths and surface crack morphologies of more intricate\nthree-point bending experiments that are inherently mixed-mode. Importantly,\nour peak load, crack trajectory and crack surface morphology predictions for\nthe challenge test, submitted before the experimental data was released, show a\nremarkable agreement with experiments. The characteristics of the challenge,\nand how changes in these can impact the predictive abilities of phase field\nfracture models, are also discussed.\n","authors":["Y. Navidtehrani","R. Duddu","E. Martínez-Pañeda"],"pdf_url":"https://arxiv.org/pdf/2403.18369v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18310v1","updated":"2024-03-27T07:22:32Z","published":"2024-03-27T07:22:32Z","title":"A thermodynamically consistent physics-informed deep learning material\n  model for short fiber/polymer nanocomposites","summary":"  This work proposes a physics-informed deep learning (PIDL)-based constitutive\nmodel for investigating the viscoelastic-viscoplastic behavior of short\nfiber-reinforced nanoparticle-filled epoxies under various ambient conditions.\nThe deep-learning model is trained to enforce thermodynamic principles, leading\nto a thermodynamically consistent constitutive model. To accomplish this, a\nlong short-term memory network is combined with a feed-forward neural network\nto predict internal variables required for characterizing the internal\ndissipation of the nanocomposite materials. In addition, another feed-forward\nneural network is used to indicate the free-energy function, which enables\ndefining the thermodynamic state of the entire system. The PIDL model is\ninitially developed for the three-dimensional case by generating synthetic data\nfrom a classical constitutive model. The model is then trained by extracting\nthe data directly from cyclic loading-unloading experimental tests. Numerical\nexamples show that the PIDL model can accurately predict the mechanical\nbehavior of epoxy-based nanocomposites for different volume fractions of fibers\nand nanoparticles under various hygrothermal conditions.\n","authors":["Betim Bahtiri","Behrouz Arash","Sven Scheffler","Maximilian Jux","Raimund Rolfes"],"pdf_url":"https://arxiv.org/pdf/2403.18310v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.08102"},{"id":"http://arxiv.org/abs/2403.18308v1","updated":"2024-03-27T07:16:05Z","published":"2024-03-27T07:16:05Z","title":"Comparison of different methods for identification of dominant\n  oscillation mode","summary":"  This paper introduces and compares the various techniques for identification\nand analysis of low frequency oscillations in a power system. Inter-area\nelectromechanical oscillations are the focus of this paper. After\nmultiresolution decomposition of characteristic signals, physical\ncharacteristics of system oscillations in signal components are identified and\npresented using the Fourier transform, Prony method, Matrix Pencil Analysis\nMethod, S-transform, Global Wavelet Spectrum and Hilbert Huang transform\n(Hilbert Marginal Spectrum) in time-frequency domain representation. The\nanalyses were performed on real frequency signals obtained from FNET GridEye\nsystem during the earthquake that triggered the shutdown of the North Anna\nNuclear Generating Station in the east coast of the United States. In addition,\naccording to the obtained results the proposed methods have proven to be\nreliable for identification of the model parameters of low-frequency\noscillation in power systems. The relevant analyses are carried out in MATLAB\ncoding environment.\n","authors":["Maja Muftic Dedovic","Samir Avdakovic","Adnan Mujezinovic","Nedis Dautbasic"],"pdf_url":"https://arxiv.org/pdf/2403.18308v1.pdf","comment":null}]},"2024-03-26T00:00:00Z":{"Systems and Control":[{"id":"http://arxiv.org/abs/2403.18149v1","updated":"2024-03-26T23:17:05Z","published":"2024-03-26T23:17:05Z","title":"Code Generation for Conic Model-Predictive Control on Microcontrollers\n  with TinyMPC","summary":"  Conic constraints appear in many important control applications like legged\nlocomotion, robotic manipulation, and autonomous rocket landing. However,\ncurrent solvers for conic optimization problems have relatively heavy\ncomputational demands in terms of both floating-point operations and memory\nfootprint, making them impractical for use on small embedded devices. We extend\nTinyMPC, an open-source, high-speed solver targeting low-power embedded control\napplications, to handle second-order cone constraints. We also present\ncode-generation software to enable deployment of TinyMPC on a variety of\nmicrocontrollers. We benchmark our generated code against state-of-the-art\nembedded QP and SOCP solvers, demonstrating a two-order-of-magnitude speed\nincrease over ECOS while consuming less memory. Finally, we demonstrate\nTinyMPC's efficacy on the Crazyflie, a lightweight, resource-constrained\nquadrotor with fast dynamics. TinyMPC and its code-generation tools are\npublicly available at https://tinympc.org.\n","authors":["Sam Schoedel","Khai Nguyen","Elakhya Nedumaran","Brian Plancher","Zachary Manchester"],"pdf_url":"https://arxiv.org/pdf/2403.18149v1.pdf","comment":"Submitted to CDC, 2024. First two authors contributed equally"},{"id":"http://arxiv.org/abs/2403.18119v1","updated":"2024-03-26T21:50:09Z","published":"2024-03-26T21:50:09Z","title":"Multiple Model Reference Adaptive Control with Blending for Non-Square\n  Multivariable Systems","summary":"  In this paper we develop a multiple model reference adaptive controller\n(MMRAC) with blending. The systems under consideration are non-square, i.e.,\nthe number of inputs is not equal to the number of states; multi-input, linear,\ntime-invariant with uncertain parameters that lie inside of a known, compact,\nand convex set. Moreover, the full state of the plant is available for\nfeedback. A multiple model online identification scheme for the plant's state\nand input matrices is developed that guarantees the estimated parameters\nconverge to the underlying plant model under the assumption of persistence of\nexcitation. Using an exact matching condition, the parameter estimates are used\nin a control law such that the plant's states asymptotically track the\nreference signal generated by a state-space model reference. The control\narchitecture is proven to provide boundedness of all closed-loop signals and to\nasymptotically drive the state tracking error to zero. Numerical simulations\nillustrate the stability and efficacy of the proposed MMRAC scheme.\n","authors":["Alex Lovi","Baris Fidan","Christopher Nielsen"],"pdf_url":"https://arxiv.org/pdf/2403.18119v1.pdf","comment":"10 pages, 7 figures, IEEE Journal Submission"},{"id":"http://arxiv.org/abs/2403.18085v1","updated":"2024-03-26T20:15:30Z","published":"2024-03-26T20:15:30Z","title":"ANOCA: AC Network-aware Optimal Curtailment Approach for Dynamic Hosting\n  Capacity","summary":"  With exponential growth in distributed energy resources (DERs) coupled with\nat-capacity distribution grid infrastructure, prosumers cannot always export\nall extra power to the grid without violating technical limits. Consequently, a\nslew of dynamic hosting capacity (DHC) algorithms have emerged for optimal\nutilization of grid infrastructure while maximizing export from DERs. Most of\nthese DHC algorithms utilize the concept of operating envelopes (OE)}, where\nthe utility gives prosumers technical power export limits, and they are free to\nexport power within these limits. Recent studies have shown that OE-based\nframeworks have drawbacks, as most develop power export limits based on convex\nor linear grid models. As OEs must capture extreme operating conditions, both\nconvex and linear models can violate technical limits in practice because they\napproximate grid physics. However, AC models are unsuitable because they may\nnot be feasible within the whole region of OE. We propose a new two-stage\noptimization framework for DHC built on three-phase AC models to address the\ncurrent gaps. In this approach, the prosumers first run a receding horizon\nmulti-period optimization to identify optimal export power setpoints to\ncommunicate with the utility. The utility then performs an infeasibility-based\noptimization to either accept the prosumer's request or dispatch an optimal\ncurtail signal such that overall system technical constraints are not violated.\nTo explore various curtailment strategies, we develop an L1, L2, and Linf\nnorm-based dispatch algorithm with an exact three-phase AC model. We test our\nframework on a 1420 three-phase node meshed distribution network and show that\nthe proposed algorithm optimally curtails DERs while guaranteeing the AC\nfeasibility of the network.\n","authors":["Emmanuel O. Badmus","Amritanshu Pandey"],"pdf_url":"https://arxiv.org/pdf/2403.18085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18071v1","updated":"2024-03-26T19:49:40Z","published":"2024-03-26T19:49:40Z","title":"From Sontag s to Cardano-Lyapunov Formula for Systems Not Affine in the\n  Control: Convection-Enabled PDE Stabilization","summary":"  We propose the first generalization of Sontag s universal controller to\nsystems not affine in the control, particularly, to PDEs with boundary\nactuation. We assume that the system admits a control Lyapunov function (CLF)\nwhose derivative, rather than being affine in the control, has either a\ndepressed cubic, quadratic, or depressed quartic dependence on the control. For\neach case, a continuous universal controller that vanishes at the origin and\nachieves global exponential stability is derived. We prove our result in the\ncontext of convectionreaction-diffusion PDEs with Dirichlet actuation. We show\nthat if the convection has a certain structure, then the L2 norm of the state\nis a CLF. In addition to generalizing Sontag s formula to some non-affine\nsystems, we present the first general Lyapunov approach for boundary control of\nnonlinear PDEs. We illustrate our results via a numerical example.\n","authors":["Mohamed Camil Belhadjoudja","Miroslav Krstic","Mohamed Maghenem","Emmanuel Witrant"],"pdf_url":"https://arxiv.org/pdf/2403.18071v1.pdf","comment":"To be presented at the 2024 American Control Conference"},{"id":"http://arxiv.org/abs/2312.10842v2","updated":"2024-03-26T19:45:15Z","published":"2023-12-17T23:20:51Z","title":"Compositional Inductive Invariant Based Verification of Neural Network\n  Controlled Systems","summary":"  The integration of neural networks into safety-critical systems has shown\ngreat potential in recent years. However, the challenge of effectively\nverifying the safety of Neural Network Controlled Systems (NNCS) persists. This\npaper introduces a novel approach to NNCS safety verification, leveraging the\ninductive invariant method. Verifying the inductiveness of a candidate\ninductive invariant in the context of NNCS is hard because of the scale and\nnonlinearity of neural networks. Our compositional method makes this\nverification process manageable by decomposing the inductiveness proof\nobligation into smaller, more tractable subproblems. Alongside the high-level\nmethod, we present an algorithm capable of automatically verifying the\ninductiveness of given candidates by automatically inferring the necessary\ndecomposition predicates. The algorithm significantly outperforms the baseline\nmethod and shows remarkable reductions in execution time in our case studies,\nshortening the verification time from hours (or timeout) to seconds.\n","authors":["Yuhao Zhou","Stavros Tripakis"],"pdf_url":"https://arxiv.org/pdf/2312.10842v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18066v1","updated":"2024-03-26T19:35:22Z","published":"2024-03-26T19:35:22Z","title":"Path Integral Control with Rollout Clustering and Dynamic Obstacles","summary":"  Model Predictive Path Integral (MPPI) control has proven to be a powerful\ntool for the control of uncertain systems (such as systems subject to\ndisturbances and systems with unmodeled dynamics). One important limitation of\nthe baseline MPPI algorithm is that it does not utilize simulated trajectories\nto their fullest extent. For one, it assumes that the average of all\ntrajectories weighted by their performance index will be a safe trajectory. In\nthis paper, multiple examples are shown where the previous assumption does not\nhold, and a trajectory clustering technique is presented that reduces the\nchances of the weighted average crossing in an unsafe region. Secondly, MPPI\ndoes not account for dynamic obstacles, so the authors put forward a novel cost\nfunction that accounts for dynamic obstacles without adding significant\ncomputation time to the overall algorithm. The novel contributions proposed in\nthis paper were evaluated with extensive simulations to demonstrate\nimprovements upon the state-of-the-art MPPI techniques.\n","authors":["Steven Patrick","Efstathios Bakolas"],"pdf_url":"https://arxiv.org/pdf/2403.18066v1.pdf","comment":"8 pages, 5 figures, extended version of ACC 2024 submission"},{"id":"http://arxiv.org/abs/2403.18055v1","updated":"2024-03-26T19:17:03Z","published":"2024-03-26T19:17:03Z","title":"Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under\n  Intermittent Sensing","summary":"  We study in this paper boundary stabilization, in the L2 sense, of the\none-dimensional Kuramoto-Sivashinsky equation subject to intermittent sensing.\nWe assume that we measure the state of this spatio-temporal equation on a given\nspatial subdomain during certain intervals of time, while we measure the state\non the remaining spatial subdomain during the remaining intervals of time. As a\nresult, we assign a feedback law at the boundary of the spatial domain and\nforce to zero the value of the state at the junction of the two subdomains.\nThroughout the study, the destabilizing coefficient is assumed to be\nspace-dependent and bounded but unknown. Adaptive boundary controllers are\ndesigned under different assumptions on the forcing term. In particular, when\nthe forcing term is null, we guarantee global exponential stability of the\norigin. Furthermore, when the forcing term is bounded and admits a known upper\nbound, we guarantee input-to-state stability, and only global uniform ultimate\nboundedness is guaranteed when the upper bound is unknown. Numerical\nsimulations are performed to illustrate our results\n","authors":["Mohamed Camil Belhadjoudja","Mohamed Maghenem","Emmanuel Witrant","Christophe Prieur"],"pdf_url":"https://arxiv.org/pdf/2403.18055v1.pdf","comment":"Submitted to Automatica"},{"id":"http://arxiv.org/abs/2403.18041v1","updated":"2024-03-26T18:52:50Z","published":"2024-03-26T18:52:50Z","title":"Learning Piecewise Residuals of Control Barrier Functions for Safety of\n  Switching Systems using Multi-Output Gaussian Processes","summary":"  Control barrier functions (CBFs) have recently been introduced as a\nsystematic tool to ensure safety by establishing set invariance. When combined\nwith a control Lyapunov function (CLF), they form a safety-critical control\nmechanism. However, the effectiveness of CBFs and CLFs is closely tied to the\nsystem model. In practice, model uncertainty can jeopardize safety and\nstability guarantees and may lead to undesirable performance. In this paper, we\ndevelop a safe learning-based control strategy for switching systems in the\nface of uncertainty. We focus on the case that a nominal model is available for\na true underlying switching system. This uncertainty results in piecewise\nresiduals for each switching surface, impacting the CLF and CBF constraints. We\nintroduce a batch multi-output Gaussian process (MOGP) framework to approximate\nthese piecewise residuals, thereby mitigating the adverse effects of\nuncertainty. A particular structure of the covariance function enables us to\nconvert the MOGP-based chance constraints CLF and CBF into second-order cone\nconstraints, which leads to a convex optimization. We analyze the feasibility\nof the resulting optimization and provide the necessary and sufficient\nconditions for feasibility. The effectiveness of the proposed strategy is\nvalidated through a simulation of a switching adaptive cruise control system.\n","authors":["Mohammad Aali","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2403.18041v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.09573"},{"id":"http://arxiv.org/abs/2403.18015v1","updated":"2024-03-26T18:04:10Z","published":"2024-03-26T18:04:10Z","title":"A Constructive Method for Designing Safe Multirate Controllers for\n  Differentially-Flat Systems","summary":"  We present a multi-rate control architecture that leverages fundamental\nproperties of differential flatness to synthesize controllers for\nsafety-critical nonlinear dynamical systems. We propose a two-layer\narchitecture, where the high-level generates reference trajectories using a\nlinear Model Predictive Controller, and the low-level tracks this reference\nusing a feedback controller. The novelty lies in how we couple these layers, to\nachieve formal guarantees on recursive feasibility of the MPC problem, and\nsafety of the nonlinear system. Furthermore, using differential flatness, we\nprovide a constructive means to synthesize the multi-rate controller, thereby\nremoving the need to search for suitable Lyapunov or barrier functions, or to\napproximately linearize/discretize nonlinear dynamics. We show the synthesized\ncontroller is a convex optimization problem, making it amenable to real-time\nimplementations. The method is demonstrated experimentally on a ground rover\nand a quadruped robotic system.\n","authors":["Devansh R. Agrawal","Hardik Parwana","Ryan K. Cosner","Ugo Rosolia","Aaron D. Ames","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2403.18015v1.pdf","comment":"6 pages, 3 figures, accepted at IEEE Control Systems Letters 2021"},{"id":"http://arxiv.org/abs/2403.17917v1","updated":"2024-03-26T17:54:05Z","published":"2024-03-26T17:54:05Z","title":"Multi-Agent Clarity-Aware Dynamic Coverage with Gaussian Processes","summary":"  This paper presents two algorithms for multi-agent dynamic coverage in\nspatiotemporal environments, where the coverage algorithms are informed by the\nmethod of data assimilation. In particular, we show that by considering the\ninformation assimilation algorithm, here a Numerical Gaussian Process Kalman\nFilter, the influence of measurements taken at one position on the uncertainty\nof the estimate at another location can be computed. We use this relationship\nto propose new coverage algorithms. Furthermore, we show that the controllers\nnaturally extend to the multi-agent context, allowing for a distributed-control\ncentral-information paradigm for multi-agent coverage. Finally, we demonstrate\nthe algorithms through a realistic simulation of a team of UAVs collecting wind\ndata over a region in Austria.\n","authors":["Devansh R. Agrawal","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2403.17917v1.pdf","comment":"8 pages, 2 figures, submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.17907v1","updated":"2024-03-26T17:45:48Z","published":"2024-03-26T17:45:48Z","title":"Multi-Agent Resilient Consensus under Intermittent Faulty and Malicious\n  Transmissions (Extended Version)","summary":"  In this work, we consider the consensus problem in which legitimate agents\nshare their values over an undirected communication network in the presence of\nmalicious or faulty agents. Different from the previous works, we characterize\nthe conditions that generalize to several scenarios such as intermittent faulty\nor malicious transmissions, based on trust observations. As the standard trust\naggregation approach based on a constant threshold fails to distinguish\nintermittent malicious/faulty activity, we propose a new detection algorithm\nutilizing time-varying thresholds and the random trust values available to\nlegitimate agents. Under these conditions, legitimate agents almost surely\ndetermine their trusted neighborhood correctly with geometrically decaying\nmisclassification probabilities. We further prove that the consensus process\nconverges almost surely even in the presence of malicious agents. We also\nderive the probabilistic bounds on the deviation from the nominal consensus\nvalue that would have been achieved with no malicious agents in the system.\nNumerical results verify the convergence among agents and exemplify the\ndeviation under different scenarios.\n","authors":["Sarper Aydın","Orhan Eren Akgün","Stephanie Gil","Angelia Nedić"],"pdf_url":"https://arxiv.org/pdf/2403.17907v1.pdf","comment":"Extended Version of CDC '24 submission"},{"id":"http://arxiv.org/abs/2310.00473v2","updated":"2024-03-26T17:43:01Z","published":"2023-09-30T19:32:56Z","title":"Optimal Control of Grid-Interfacing Inverters With Current Magnitude\n  Limits","summary":"  Grid-interfacing inverters act as the interface between renewable resources\nand the electric grid, and have the potential to offer fast and programmable\ncontrols compared to synchronous generators. With this flexibility there has\nbeen significant research efforts into determining the best way to control\nthese inverters. Inverters are limited in their maximum current output in order\nto protect semiconductor devices, presenting a nonlinear constraint that needs\nto be accounted for in their control algorithms. Existing approaches either\nsimply saturate a controller that is designed for unconstrained systems, or\nassume small perturbations and linearize a saturated system. These approaches\ncan lead to stability issues or limiting the control actions to be too\nconservative.\n  In this paper, we directly focus on a nonlinear system that explicitly\naccounts for the saturation of the current magnitude. We use a Lyapunov\nstability approach to determine a stability condition for the system,\nguaranteeing that a class of controllers would be stabilizing if they satisfy a\nsimple SDP condition. With this condition we fit a linear-feedback controller\nby sampling the output (offline) model predictive control problems. This\nlearned controller has improved performances with existing designs.\n","authors":["Trager Joswig-Jones","Baosen Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.00473v2.pdf","comment":"6 pages, 6 figures, 1 table. Submitted to CDC'2024"},{"id":"http://arxiv.org/abs/2403.17861v1","updated":"2024-03-26T16:49:31Z","published":"2024-03-26T16:49:31Z","title":"Stealthy Deactivation of Safety Filters","summary":"  Safety filters ensure that only safe control actions are executed. We propose\na simple and stealthy false-data injection attack for deactivating such safety\nfilters; in particular, we focus on deactivating safety filters that are based\non control-barrier functions. The attack injects false sensor measurements to\nbias state estimates to the interior of a safety region, which makes the safety\nfilter accept unsafe control actions. To detect such attacks, we also propose a\ndetector that detects biases manufactured by the proposed attack policy, which\ncomplements conventional detectors when safety filters are used. The proposed\nattack policy and detector are illustrated on a double integrator example.\n","authors":["Daniel Arnström","André M. H. Teixeira"],"pdf_url":"https://arxiv.org/pdf/2403.17861v1.pdf","comment":"ECC24"},{"id":"http://arxiv.org/abs/2403.17831v1","updated":"2024-03-26T16:13:55Z","published":"2024-03-26T16:13:55Z","title":"Learning the Optimal Power Flow: Environment Design Matters","summary":"  To solve the optimal power flow (OPF) problem, reinforcement learning (RL)\nemerges as a promising new approach. However, the RL-OPF literature is strongly\ndivided regarding the exact formulation of the OPF problem as an RL\nenvironment. In this work, we collect and implement diverse environment design\ndecisions from the literature regarding training data, observation space,\nepisode definition, and reward function choice. In an experimental analysis, we\nshow the significant impact of these environment design options on RL-OPF\ntraining performance. Further, we derive some first recommendations regarding\nthe choice of these design decisions. The created environment framework is\nfully open-source and can serve as a benchmark for future research in the\nRL-OPF field.\n","authors":["Thomas Wolgast","Astrid Nieße"],"pdf_url":"https://arxiv.org/pdf/2403.17831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03300v4","updated":"2024-03-26T16:09:26Z","published":"2023-11-06T17:47:40Z","title":"Faster Run-to-Run Feedforward Control of Electromechanical Switching\n  Devices: a Sensitivity-Based Approach","summary":"  Electromechanical switching devices, such as solenoid valves, contactors, and\nrelays, suffer from undesirable phenomena like clicking, mechanical wear, and\ncontact bounce. Despite that, they are still widely used in industry due to\ntheir various economic and technical advantages. This has encouraged the\ndevelopment of controllers aimed at reducing the collisions that occur at the\nend of the switching operations. One of the most successful approaches has been\nthe use of iterative techniques. However, these algorithms typically require a\nlarge number of operations to converge, which is definitely a clear drawback.\nThis paper presents a strategy to improve the convergence rate of such\ncontrollers. Our proposal, which is based on the sensitivity of the control law\nwith respect to the parameters, assumes that the performance of the system is\nmore heavily affected by some parameters than others. Thus, by avoiding\nmovements in the directions that have less impact, the search algorithm is\nexpected to drive the system to near-optimal behaviors using fewer operations.\nResults obtained by simulation show significant improvement in the convergence\nrate of a state-of-the-art run-to-run feedforward controller, which\ndemonstrates the high potential of the proposal.\n","authors":["Edgar Ramirez-Laboreo","Eduardo Moya-Lasheras","Eloy Serrano-Seco"],"pdf_url":"https://arxiv.org/pdf/2311.03300v4.pdf","comment":"6 pages, 4 figures. Final version, after peer review and acceptance,\n  submitted to the 22nd European Control Conference (ECC)"},{"id":"http://arxiv.org/abs/2309.02937v2","updated":"2024-03-26T15:51:49Z","published":"2023-09-06T12:04:24Z","title":"Resilient source seeking with robot swarms","summary":"  We present a solution for locating the source, or maximum, of an unknown\nscalar field using a swarm of mobile robots. Unlike relying on the traditional\ngradient information, the swarm determines an ascending direction to approach\nthe source with arbitrary precision. The ascending direction is calculated from\nmeasurements of the field strength at the robot locations and their relative\npositions concerning the centroid. Rather than focusing on individual robots,\nwe focus the analysis on the density of robots per unit area to guarantee a\nmore resilient swarm, i.e., the functionality remains even if individuals go\nmissing or are misplaced during the mission. We reinforce the robustness of the\nalgorithm by providing sufficient conditions for the swarm shape so that the\nascending direction is almost parallel to the gradient. The swarm can respond\nto an unexpected environment by morphing its shape and exploiting the existence\nof multiple ascending directions. Finally, we validate our approach numerically\nwith hundreds of robots. The fact that a large number of robots always\ncalculate an ascending direction compensates for the loss of individuals and\nmitigates issues arising from the actuator and sensor noises.\n","authors":["Antonio Acuaviva","Jesus Bautista","Weijia Yao","Juan Jimenez","Hector Garcia de Marina"],"pdf_url":"https://arxiv.org/pdf/2309.02937v2.pdf","comment":"7 pages, submitted to CDC 2024"},{"id":"http://arxiv.org/abs/2403.17800v1","updated":"2024-03-26T15:39:59Z","published":"2024-03-26T15:39:59Z","title":"Steering Feedback in Dynamic Driving Simulators: The Influence of\n  Steering Wheel Vibration and Vehicle Motion Frequency","summary":"  The validity of the subjective evaluation of steering feedback in driving\nsimulators is crucial for modern vehicle development. Although there are\nestablished objective steering characteristics for the assessment of both\nstationary and dynamic feedback behaviour, factors such as steering wheel\nvibrations and vehicle body motion, particularly in high-frequency ranges,\npresent challenges in simulator fidelity. This work investigates the influence\nof steering wheel vibration and vehicle body motion frequency content on the\nsubjective evaluation of steering feedback during closed-loop driving in a\ndynamic driving simulator. A controlled subject study with 30 participants\nconsisting of a back-to-back comparison of a reference vehicle with an\nelectrical power steering system and three variants of its virtual\nrepresentation on a dynamic driving simulator was performed. Subjective\nevaluation focused on the representation of road feedback in comparison to the\nreference vehicle. The statistical analysis of subjective results show that\nthere is a significant influence of the frequency content of both steering\nwheel torque and vehicle motion on the subjective evaluation of steering\nfeedback in a dynamic driving simulator. The results suggest an influence of\nfrequency content on the subjective evaluation quality of steering feedback\ncharacteristics that are not associated with the dynamic feedback behaviour in\nthe context of established performance indicators.\n","authors":["Maximilian Böhle","Bernhard Schick","Steffen Müller"],"pdf_url":"https://arxiv.org/pdf/2403.17800v1.pdf","comment":"12 pages, 7 figures, 9 tables, submitted to the IEEE Transactions on\n  Intelligent Vehicles"},{"id":"http://arxiv.org/abs/2403.17793v1","updated":"2024-03-26T15:27:24Z","published":"2024-03-26T15:27:24Z","title":"Neural Exponential Stabilization of Control-affine Nonlinear Systems","summary":"  This paper proposes a novel learning-based approach for achieving exponential\nstabilization of nonlinear control-affine systems. We leverage the Control\nContraction Metrics (CCMs) framework to co-synthesize Neural Contraction\nMetrics (NCMs) and Neural Network (NN) controllers. First, we transform the\ninfinite-dimensional semi-definite program (SDP) for CCM computation into a\ntractable inequality feasibility problem using element-wise bounds of\nmatrix-valued functions. The terms in the inequality can be efficiently\ncomputed by our novel algorithms. Second, we propose a free parametrization of\nNCMs guaranteeing positive definiteness and the satisfaction of a partial\ndifferential equation, regardless of trainable parameters. Third, this\nparametrization and the inequality condition enable the design of\ncontractivity-enforcing regularizers, which can be incorporated while designing\nthe NN controller for exponential stabilization of the underlying nonlinear\nsystems. Furthermore, when the training loss goes to zero, we provide formal\nguarantees on verification of the NCM and the exponentional stabilization under\nthe NN controller. Finally, we validate our method through benchmark\nexperiments on set-point stabilization and increasing the region of attraction\nof a locally pre-stabilized closed-loop system.\n","authors":["Muhammad Zakwan","Liang Xu","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2403.17793v1.pdf","comment":"This paper is submitted in CDC2024 for a possible publication"},{"id":"http://arxiv.org/abs/2403.17790v1","updated":"2024-03-26T15:21:18Z","published":"2024-03-26T15:21:18Z","title":"A PAC-Bayesian Framework for Optimal Control with Stability Guarantees","summary":"  Stochastic Nonlinear Optimal Control (SNOC) involves minimizing a cost\nfunction that averages out the random uncertainties affecting the dynamics of\nnonlinear systems. For tractability reasons, this problem is typically\naddressed by minimizing an empirical cost, which represents the average cost\nacross a finite dataset of sampled disturbances. However, this approach raises\nthe challenge of quantifying the control performance against out-of-sample\nuncertainties. Particularly, in scenarios where the training dataset is small,\nSNOC policies are prone to overfitting, resulting in significant discrepancies\nbetween the empirical cost and the true cost, i.e., the average SNOC cost\nincurred during control deployment. Therefore, establishing generalization\nbounds on the true cost is crucial for ensuring reliability in real-world\napplications. In this paper, we introduce a novel approach that leverages\nPAC-Bayes theory to provide rigorous generalization bounds for SNOC. Based on\nthese bounds, we propose a new method for designing optimal controllers,\noffering a principled way to incorporate prior knowledge into the synthesis\nprocess, which aids in improving the control policy and mitigating overfitting.\nFurthermore, by leveraging recent parametrizations of stabilizing controllers\nfor nonlinear systems, our framework inherently ensures closed-loop stability.\nThe effectiveness of our proposed method in incorporating prior knowledge and\ncombating overfitting is shown by designing neural network controllers for\ntasks in cooperative robotics.\n","authors":["Mahrokh Ghoddousi Boroujeni","Clara Lucía Galimberti","Andreas Krause","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2403.17790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17785v1","updated":"2024-03-26T15:17:55Z","published":"2024-03-26T15:17:55Z","title":"Neural Distributed Controllers with Port-Hamiltonian Structures","summary":"  Controlling large-scale cyber-physical systems necessitates optimal\ndistributed policies, relying solely on local real-time data and limited\ncommunication with neighboring agents. However, finding optimal controllers\nremains challenging, even in seemingly simple scenarios. Parameterizing these\npolicies using Neural Networks (NNs) can deliver good performance, but their\nsensitivity to small input changes can destabilize the closed-loop system. This\npaper addresses this issue for a network of nonlinear dissipative systems.\nSpecifically, we leverage well-established port-Hamiltonian structures to\ncharacterize deep distributed control policies with closed-loop stability\nguarantees and a finite $\\mathcal{L}_2$ gain, regardless of specific NN\nparameters. This eliminates the need to constrain the parameters during\noptimization and enables training with standard methods like stochastic\ngradient descent. A numerical study on the consensus control of Kuramoto\noscillators demonstrates the effectiveness of the proposed controllers.\n","authors":["Muhammad Zakwan","Giancarlo Ferrari-Trecate"],"pdf_url":"https://arxiv.org/pdf/2403.17785v1.pdf","comment":"This paper is submitted in CDC2024 for a possible publication"},{"id":"http://arxiv.org/abs/2403.17779v1","updated":"2024-03-26T15:12:46Z","published":"2024-03-26T15:12:46Z","title":"Optical Flow Based Detection and Tracking of Moving Objects for\n  Autonomous Vehicles","summary":"  Accurate velocity estimation of surrounding moving objects and their\ntrajectories are critical elements of perception systems in\nAutomated/Autonomous Vehicles (AVs) with a direct impact on their safety. These\nare non-trivial problems due to the diverse types and sizes of such objects and\ntheir dynamic and random behaviour. Recent point cloud based solutions often\nuse Iterative Closest Point (ICP) techniques, which are known to have certain\nlimitations. For example, their computational costs are high due to their\niterative nature, and their estimation error often deteriorates as the relative\nvelocities of the target objects increase (>2 m/sec). Motivated by such\nshortcomings, this paper first proposes a novel Detection and Tracking of\nMoving Objects (DATMO) for AVs based on an optical flow technique, which is\nproven to be computationally efficient and highly accurate for such problems.\n\\textcolor{black}{This is achieved by representing the driving scenario as a\nvector field and applying vector calculus theories to ensure spatiotemporal\ncontinuity.} We also report the results of a comprehensive performance\nevaluation of the proposed DATMO technique, carried out in this study using\nsynthetic and real-world data. The results of this study demonstrate the\nsuperiority of the proposed technique, compared to the DATMO techniques in the\nliterature, in terms of estimation accuracy and processing time in a wide range\nof relative velocities of moving objects. Finally, we evaluate and discuss the\nsensitivity of the estimation error of the proposed DATMO technique to various\nsystem and environmental parameters, as well as the relative velocities of the\nmoving objects.\n","authors":["MReza Alipour Sormoli","Mehrdad Dianati","Sajjad Mozaffari","Roger woodman"],"pdf_url":"https://arxiv.org/pdf/2403.17779v1.pdf","comment":"This manuscript has been accepted as a regular paper in Transactions\n  on Intelligent Transportation Systems (DOI: 10.1109/TITS.2024.3382495)"},{"id":"http://arxiv.org/abs/2306.02987v2","updated":"2024-03-26T14:38:44Z","published":"2023-06-05T16:00:22Z","title":"Frequency Regulation with Storage: On Losses and Profits","summary":"  Low-carbon societies will need to store vast amounts of electricity to\nbalance intermittent generation from wind and solar energy, for example,\nthrough frequency regulation. Here, we derive an analytical solution to the\ndecision-making problem of storage operators who sell frequency regulation\npower to grid operators and trade electricity on day-ahead markets.\nMathematically, we treat future frequency deviation trajectories as functional\nuncertainties in a receding horizon robust optimization problem. We constrain\nthe expected terminal state-of-charge to be equal to some target to allow\nstorage operators to make good decisions not only for the present but also the\nfuture. Thanks to this constraint, the amount of electricity traded on\nday-ahead markets is an implicit function of the regulation power sold to grid\noperators. The implicit function quantifies the amount of power that needs to\nbe purchased to cover the expected energy loss that results from providing\nfrequency regulation. We show how the marginal cost associated with the\nexpected energy loss decreases with roundtrip efficiency and increases with\nfrequency deviation dispersion. We find that the profits from frequency\nregulation over the lifetime of energy-constrained storage devices are roughly\ninversely proportional to the length of time for which regulation power must be\ncommitted.\n","authors":["Dirk Lauinger","François Vuille","Daniel Kuhn"],"pdf_url":"https://arxiv.org/pdf/2306.02987v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.02099v4","updated":"2024-03-26T14:25:52Z","published":"2023-10-30T21:52:37Z","title":"A Safe Preference Learning Approach for Personalization with\n  Applications to Autonomous Vehicles","summary":"  This work introduces a preference learning method that ensures adherence to\ngiven specifications, with an application to autonomous vehicles. Our approach\nincorporates the priority ordering of Signal Temporal Logic (STL) formulas\ndescribing traffic rules into a learning framework. By leveraging Parametric\nWeighted Signal Temporal Logic (PWSTL), we formulate the problem of\nsafety-guaranteed preference learning based on pairwise comparisons and propose\nan approach to solve this learning problem. Our approach finds a feasible\nvaluation for the weights of the given PWSTL formula such that, with these\nweights, preferred signals have weighted quantitative satisfaction measures\ngreater than their non-preferred counterparts. The feasible valuation of\nweights given by our approach leads to a weighted STL formula that can be used\nin correct-and-custom-by-construction controller synthesis. We demonstrate the\nperformance of our method with a pilot human subject study in two different\nsimulated driving scenarios involving a stop sign and a pedestrian crossing.\nOur approach yields competitive results compared to existing preference\nlearning methods in terms of capturing preferences and notably outperforms them\nwhen safety is considered.\n","authors":["Ruya Karagulle","Nikos Arechiga","Andrew Best","Jonathan DeCastro","Necmiye Ozay"],"pdf_url":"https://arxiv.org/pdf/2311.02099v4.pdf","comment":"9 pages, 3 figures, 2 tables. This work has been published at IEEE\n  Robotics and Automation Letters. Copyright may be transferred without notice,\n  after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2403.17730v1","updated":"2024-03-26T14:19:22Z","published":"2024-03-26T14:19:22Z","title":"On Structural Non-commutativity in Affine Feedback of SISO Nonlinear\n  Systems","summary":"  The affine feedback connection of SISO nonlinear systems modeled by\nChen--Fliess series is shown to be a group action on the plant which is\nisomorphic to the semi-direct product of shuffle and additive group of\nnon-commutative formal power series. The additive and multiplicative feedback\nloops in an affine feedback connection are thus proven to be structurally\nnon-commutative. A flip in the order of these loops results in a net additive\nfeedback loop.\n","authors":["Venkatesh G. S."],"pdf_url":"https://arxiv.org/pdf/2403.17730v1.pdf","comment":"submitted to $26^{th}$ International Symposium on Mathematical Theory\n  of Networks and Systems, 2024"},{"id":"http://arxiv.org/abs/2403.17711v1","updated":"2024-03-26T13:58:21Z","published":"2024-03-26T13:58:21Z","title":"Using quantum computers in control: interval matrix properties","summary":"  Quantum computing provides a powerful framework for tackling computational\nproblems that are classically intractable. The goal of this paper is to explore\nthe use of quantum computers for solving relevant problems in systems and\ncontrol theory. In the recent literature, different quantum algorithms have\nbeen developed to tackle binary optimization, which plays an important role in\nvarious control-theoretic problems. As a prototypical example, we consider the\nverification of interval matrix properties such as non-singularity and\nstability on a quantum computer. We present a quantum algorithm solving these\nproblems and we study its performance in simulation. Our results demonstrate\nthat quantum computers provide a promising tool for control whose applicability\nto further computationally complex problems remains to be explored.\n","authors":["Jan Schneider","Julian Berberich"],"pdf_url":"https://arxiv.org/pdf/2403.17711v1.pdf","comment":"Final version, accepted for publication in Proc. European Control\n  Conference (ECC), 2024"},{"id":"http://arxiv.org/abs/2311.02068v2","updated":"2024-03-26T13:53:28Z","published":"2023-11-03T17:52:37Z","title":"Closing the Gap to Quadratic Invariance: a Regret Minimization Approach\n  to Optimal Distributed Control","summary":"  In this work, we focus on the design of optimal controllers that must comply\nwith an information structure. State-of-the-art approaches do so based on the\nH2 or Hinfty norm to minimize the expected or worst-case cost in the presence\nof stochastic or adversarial disturbances. Large-scale systems often experience\na combination of stochastic and deterministic disruptions (e.g., sensor\nfailures, environmental fluctuations) that spread across the system and are\ndifficult to model precisely, leading to sub-optimal closed-loop behaviors.\nHence, we propose improving performance for these scenarios by minimizing the\nregret with respect to an ideal policy that complies with less stringent\nsensor-information constraints. This endows our controller with the ability to\napproach the improved behavior of a more informed policy, which would detect\nand counteract heterogeneous and localized disturbances more promptly.\nSpecifically, we derive convex relaxations of the resulting regret minimization\nproblem that are compatible with any desired controller sparsity, while we\nreveal a renewed role of the Quadratic Invariance (QI) condition in designing\ninformative benchmarks to measure regret. Last, we validate our proposed method\nthrough numerical simulations on controlling a multi-agent distributed system,\ncomparing its performance with traditional H2 and Hinfty policies.\n","authors":["Daniele Martinelli","Andrea Martin","Giancarlo Ferrari-Trecate","Luca Furieri"],"pdf_url":"https://arxiv.org/pdf/2311.02068v2.pdf","comment":"Accepted for presentation and publication in the proceedings of the\n  2024 European Control Conference (ECC)"},{"id":"http://arxiv.org/abs/2403.17704v1","updated":"2024-03-26T13:49:48Z","published":"2024-03-26T13:49:48Z","title":"Prioritize Team Actions: Multi-Agent Temporal Logic Task Planning with\n  Ordering Constraints","summary":"  In this paper, we investigate the problem of linear temporal logic (LTL) path\nplanning for multi-agent systems, introducing the new concept of \\emph{ordering\nconstraints}. Specifically, we consider a generic objective function that is\ndefined for the path of each individual agent. The primary objective is to find\na global plan for the team of agents, ensuring they collectively meet the\nspecified LTL requirements. Simultaneously, we aim to maintain a pre-determined\norder in the values of the objective function for each agent, which we refer to\nas the ordering constraints. This new requirement stems from scenarios like\nsecurity-aware planning, where relative orders outweigh absolute values in\nimportance. We present an efficient algorithm to solve this problem, supported\nby proofs of correctness that demonstrate the optimality of our solution.\nAdditionally, we provide a case study in security-aware path planning to\nillustrate the practicality and effectiveness of our proposed approach.\n","authors":["Bowen Ye","Jianing Zhao","Shaoyuan Li","Xiang Yin"],"pdf_url":"https://arxiv.org/pdf/2403.17704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07039v3","updated":"2024-03-26T13:34:43Z","published":"2023-11-13T02:42:43Z","title":"Time-Optimal Control for High-Order Chain-of-Integrators Systems with\n  Full State Constraints and Arbitrary Terminal States (Extended Version)","summary":"  Time-optimal control for high-order chain-of-integrators systems with full\nstate constraints and arbitrary given terminal states remains a challenging\nproblem in the optimal control theory domain, yet to be resolved. To enhance\nfurther comprehension of the problem, this paper establishes a novel notation\nsystem and theoretical framework, providing the switching manifold for\nhigh-order problems in the form of switching laws. Through deriving properties\nof switching laws on signs and dimension, this paper proposes a definite\ncondition for time-optimal control. Guided by the developed theory, a\ntrajectory planning method named the manifold-intercept method (MIM) is\ndeveloped. The proposed MIM can plan time-optimal jerk-limited trajectories\nwith full state constraints, and can also plan near-optimal non-chattering\nhigher-order trajectories with negligible extra motion time compared to optimal\nprofiles. Numerical results indicate that the proposed MIM outperforms all\nbaselines in computational time, computational accuracy, and trajectory quality\nby a large gap.\n","authors":["Yunan Wang","Chuxiong Hu","Zeyang Li","Shize Lin","Suqin He","Ze Wang","Yu Zhu"],"pdf_url":"https://arxiv.org/pdf/2311.07039v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17675v1","updated":"2024-03-26T13:02:48Z","published":"2024-03-26T13:02:48Z","title":"Chattering Phenomena in Time-Optimal Control for High-Order\n  Chain-of-Integrators Systems with Full State Constraints","summary":"  Time-optimal control for high-order chain-of-integrators systems with full\nstate constraints and arbitrary given terminal states remains an open and\nchallenging problem in optimal control theory domain. However, optimal\ncontrol's behaviors in high-order problems lack of precision characterization,\neven where the existence of chattering phenomena remain unknown and overlooked.\nThis paper establishes a theoretical framework of chattering phenomena in the\nproblem, focusing on the uniqueness of state constraints inducing chattering,\nthe upper bound of switching times in an unconstrained arc during chattering,\nand the convergence of states and costates to the chattering limit point. For\nthe first time, this paper proves the existence of chattering phenomena in the\nproblems. The chattering optimal control for 4th order problems with velocity\nconstraints is precisely solved, providing an approach to plan strictly\ntime-optimal snap-limited trajectories, while other cases of order $n\\leq4$ are\nproved to not allow chattering. The conclusions correct the longstanding\nmisconception in the industry regarding the time-optimality of S-shaped\ntrajectories with minimal switching times.\n","authors":["Yunan Wang","Chuxiong Hu","Zeyang Li","Yujie Lin","Shize Lin","Suqin He","Yu Zhu"],"pdf_url":"https://arxiv.org/pdf/2403.17675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07939v2","updated":"2024-03-26T11:54:27Z","published":"2023-11-14T06:33:41Z","title":"Discretized Distributed Optimization over Dynamic Digraphs","summary":"  We consider a discrete-time model of continuous-time distributed optimization\nover dynamic directed-graphs (digraphs) with applications to distributed\nlearning. Our optimization algorithm works over general strongly connected\ndynamic networks under switching topologies, e.g., in mobile multi-agent\nsystems and volatile networks due to link failures. Compared to many existing\nlines of work, there is no need for bi-stochastic weight designs on the links.\nThe existing literature mostly needs the link weights to be stochastic using\nspecific weight-design algorithms needed both at the initialization and at all\ntimes when the topology of the network changes. This paper eliminates the need\nfor such algorithms and paves the way for distributed optimization over\ntime-varying digraphs. We derive the bound on the gradient-tracking step-size\nand discrete time-step for convergence and prove dynamic stability using\narguments from consensus algorithms, matrix perturbation theory, and Lyapunov\ntheory. This work, particularly, is an improvement over existing\nstochastic-weight undirected networks in case of link removal or packet drops.\nThis is because the existing literature may need to rerun time-consuming and\ncomputationally complex algorithms for stochastic design, while the proposed\nstrategy works as long as the underlying network is weight-symmetric and\nbalanced. The proposed optimization framework finds applications to distributed\nclassification and learning.\n","authors":["Mohammadreza Doostmohammadian","Wei Jiang","Muwahida Liaquat","Alireza Aghasi","Houman Zarrabi"],"pdf_url":"https://arxiv.org/pdf/2311.07939v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.02021v5","updated":"2024-03-26T11:41:36Z","published":"2022-09-05T15:41:13Z","title":"When Robotics Meets Wireless Communications: An Introductory Tutorial","summary":"  The importance of ground Mobile Robots (MRs) and Unmanned Aerial Vehicles\n(UAVs) within the research community, industry, and society is growing fast.\nMany of these agents are nowadays equipped with communication systems that are,\nin some cases, essential to successfully achieve certain tasks. In this\ncontext, we have begun to witness the development of a new interdisciplinary\nresearch field at the intersection of robotics and communications. This\nresearch field has been boosted by the intention of integrating UAVs within the\n5G and 6G communication networks. This research will undoubtedly lead to many\nimportant applications in the near future. Nevertheless, one of the main\nobstacles to the development of this research area is that most researchers\naddress these problems by oversimplifying either the robotics or the\ncommunications aspect. This impedes the ability of reaching the full potential\nof this new interdisciplinary research area. In this tutorial, we present some\nof the modelling tools necessary to address problems involving both robotics\nand communication from an interdisciplinary perspective. As an illustrative\nexample of such problems, we focus in this tutorial on the issue of\ncommunication-aware trajectory planning.\n","authors":["Daniel Bonilla Licea","Mounir Ghogho","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2209.02021v5.pdf","comment":"35 pages, 192 references"},{"id":"http://arxiv.org/abs/2311.03197v4","updated":"2024-03-26T11:37:38Z","published":"2023-11-06T15:39:05Z","title":"Stable Linear Subspace Identification: A Machine Learning Approach","summary":"  Machine Learning (ML) and linear System Identification (SI) have been\nhistorically developed independently. In this paper, we leverage\nwell-established ML tools - especially the automatic differentiation framework\n- to introduce SIMBa, a family of discrete linear multi-step-ahead state-space\nSI methods using backpropagation. SIMBa relies on a novel\nLinear-Matrix-Inequality-based free parametrization of Schur matrices to ensure\nthe stability of the identified model.\n  We show how SIMBa generally outperforms traditional linear state-space SI\nmethods, and sometimes significantly, although at the price of a higher\ncomputational burden. This performance gap is particularly remarkable compared\nto other SI methods with stability guarantees, where the gain is frequently\nabove 25% in our investigations, hinting at SIMBa's ability to simultaneously\nachieve state-of-the-art fitting performance and enforce stability.\nInterestingly, these observations hold for a wide variety of input-output\nsystems and on both simulated and real-world data, showcasing the flexibility\nof the proposed approach. We postulate that this new SI paradigm presents a\ngreat extension potential to identify structured nonlinear models from data,\nand we hence open-source SIMBa on https://github.com/Cemempamoi/simba.\n","authors":["Loris Di Natale","Muhammad Zakwan","Bratislav Svetozarevic","Philipp Heer","Giancarlo Ferrari-Trecate","Colin N. Jones"],"pdf_url":"https://arxiv.org/pdf/2311.03197v4.pdf","comment":"Accepted at ECC 2024"},{"id":"http://arxiv.org/abs/2403.17598v1","updated":"2024-03-26T11:09:27Z","published":"2024-03-26T11:09:27Z","title":"Ultrafast Adaptive Primary Frequency Tuning and Secondary Frequency\n  Identification for S/S WPT system","summary":"  Magnetic resonance wireless power transfer (WPT) technology is increasingly\nbeing adopted across diverse applications. However, its effectiveness can be\nsignificantly compromised by parameter shifts within the resonance network,\nowing to its high system quality factor. Such shifts are inherent and\nchallenging to mitigate during the manufacturing process. In response, this\narticle introduces a rapid frequency tuning approach. Leveraging\nswitch-controlled capacitors (SCC) to adjust the resonance network and the\nprimary side's operating frequency, alongside a current zero-crossing detection\n(ZCD) circuit for voltage-current phase determination, this method circumvents\nthe need for intricate knowledge of WPT system parameters. Moreover, it\nobviates the necessity for inter-side communication for real-time\nidentification of the secondary side resonance frequency. The swift response of\nSCC and two-step perturb-and-observe algorithm mitigate output disturbances,\nthereby expediting the frequency tuning process. Experimental validation on a\n200W Series-Series compensated WPT (SS-WPT) system demonstrates that the\nproposed method achieves frequency recognition accuracy within 0.7kHz in less\nthan 1ms, increasing system efficiency up to 9%.\n","authors":["Chang Liu","Wei Han","Guangyu Yan","Bowang Zhang","Chunlin Li"],"pdf_url":"https://arxiv.org/pdf/2403.17598v1.pdf","comment":"11 pages,16 figures,to be published in IEEE Transactions on\n  Industrial Electronics"},{"id":"http://arxiv.org/abs/2309.09012v2","updated":"2024-03-26T10:51:08Z","published":"2023-09-16T14:50:31Z","title":"Modelling Irrational Behaviour of Residential End Users using\n  Non-Stationary Gaussian Processes","summary":"  Demand response (DR) plays a critical role in ensuring efficient electricity\nconsumption and optimal use of network assets. Yet, existing DR models often\noverlook a crucial element, the irrational behaviour of electricity end users.\nIn this work, we propose a price-responsive model that incorporates key aspects\nof end-user irrationality, specifically loss aversion, time inconsistency, and\nbounded rationality. To this end, we first develop a framework that uses\nMultiple Seasonal-Trend decomposition using Loess (MSTL) and non-stationary\nGaussian processes to model the randomness in the electricity consumption by\nresidential consumers. The impact of this model is then evaluated through a\ncommunity battery storage (CBS) business model. Additionally, we apply a\nchance-constrained optimisation model for CBS operation that deals with the\nunpredictability of the end-user irrationality. Our simulations using\nreal-world data show that the proposed DR model provides a more realistic\nestimate of end-user price-responsive behaviour when considering irrationality.\nCompared to a deterministic model that cannot fully take into account the\nirrational behaviour of end users, the chance-constrained CBS operation model\nyields an additional 19% revenue. Lastly, the business model reduces the\nelectricity costs of solar end users by 11%.\n","authors":["Nam Trong Dinh","Sahand Karimi-Arpanahi","Rui Yuan","S. Ali Pourmousavi","Mingyu Guo","Jon A. R. Liisberg","Julian Lemos-Vinasco"],"pdf_url":"https://arxiv.org/pdf/2309.09012v2.pdf","comment":"This manuscript has been accepted for publication in IEEE\n  Transactions on Smart Grid"},{"id":"http://arxiv.org/abs/2403.17565v1","updated":"2024-03-26T10:19:04Z","published":"2024-03-26T10:19:04Z","title":"Aerial Robots Carrying Flexible Cables: Dynamic Shape Optimal Control\n  via Spectral Method Model","summary":"  In this work, we present a model-based optimal boundary control design for an\naerial robotic system composed of a quadrotor carrying a flexible cable. The\nwhole system is modeled by partial differential equations (PDEs) combined with\nboundary conditions described by ordinary differential equations (ODEs). The\nproper orthogonal decomposition (POD) method is adopted to project the original\ninfinite-dimensional system on a subspace spanned by orthogonal basis\nfunctions. Based on the reduced order model, nonlinear model predictive control\n(NMPC) is implemented online to realize shape trajectory tracking of the\nflexible cable in an optimal predictive fashion. The proposed reduced modeling\nand optimal control paradigms are numerically verified against an accurate\nhigh-dimensional FDM-based model in different scenarios and the controller's\nsuperior performance is shown compared to an optimally tuned PID controller.\n","authors":["Yaolei Shen","Chiara Gabellieri","Antonio Franchi"],"pdf_url":"https://arxiv.org/pdf/2403.17565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17554v1","updated":"2024-03-26T10:01:13Z","published":"2024-03-26T10:01:13Z","title":"Robust Stability for Multiagent Systems with Spatio-Temporally\n  Correlated Packet Loss","summary":"  A problem with considering correlations in the analysis of multiagent system\nwith stochastic packet loss is that they induce dependencies between agents\nthat are otherwise decoupled, preventing the application of decomposition\nmethods required for efficient evaluation. To circumvent that issue, this paper\nis proposing an approach based on analysing sets of networks with independent\ncommunication links, only considering the correlations in an implicit fashion.\nCombining ideas from the robust stabilization of Markov jump linear systems\nwith recently proposed techniques for analysing packet loss in multiagent\nsystems, we obtain a linear matrix inequality based stability condition which\nis independent of the number of agents. The main result is that the set of\nstabilized probability distributions has non-empty interior such that small\ncorrelations cannot lead to instability, even though only distributions of\nindependent links were analysed. Moreover, two examples are provided to\ndemonstrate the applicability of the results to practically relevant scenarios.\n","authors":["Christian Hespe","Adwait Datar","Herbert Werner"],"pdf_url":"https://arxiv.org/pdf/2403.17554v1.pdf","comment":"7 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2011.07529v3","updated":"2024-03-26T08:54:31Z","published":"2020-11-15T13:40:21Z","title":"Full Attitude Intelligent Controller Design of a Heliquad under Complete\n  Failure of an Actuator","summary":"  In this paper, we design a reliable Heliquad and develop an intelligent\ncontroller to handle one actuators complete failure. Heliquad is a multi-copter\nsimilar to Quadcopter, with four actuators diagonally symmetric from the\ncenter. Each actuator has two control inputs; the first input changes the\npropeller blades collective pitch (also called variable pitch), and the other\ninput changes the rotation speed. For reliable operation and high torque\ncharacteristic requirement for yaw control, a cambered airfoil is used to\ndesign propeller blades. A neural network-based control allocation is designed\nto provide complete control authority even under a complete loss of one\nactuator. Nonlinear quaternion based outer loop position control, with\nproportional-derivative inner loop for attitude control and neural\nnetwork-based control allocation is used in controller design. The proposed\ncontroller and Heliquad designs performance is evaluated using a\nsoftware-in-loop simulation to track the position reference command under\nfailure. The results clearly indicate that the Heliquad with an intelligent\ncontroller provides necessary tracking performance even under a complete loss\nof one actuator.\n","authors":["Eeshan Kulkarni","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2011.07529v3.pdf","comment":"7 pages, For video go to\n  https://indianinstituteofscience-my.sharepoint.com/:v:/g/personal/eeshank_iisc_ac_in/EcMg2uTtE91AsHDejNkb6YMBNckaXGjeh_YMzDV6sAHZAQ?e=DrRqmN"},{"id":"http://arxiv.org/abs/2304.02444v2","updated":"2024-03-26T08:13:01Z","published":"2023-04-05T14:02:53Z","title":"Autonomous Hook-Based Grasping and Transportation with Quadcopters","summary":"  Payload grasping and transportation with quadcopters is an active research\narea that has rapidly developed over the last decade. To grasp a payload\nwithout human interaction, most state-of-the-art approaches apply robotic arms\nthat are attached to the quadcopter body. However, due to the large weight and\npower consumption of these aerial manipulators, their agility and flight time\nare limited. This paper proposes a motion control and planning method for\ntransportation with a lightweight, passive manipulator structure that consists\nof a hook attached to a quadrotor using a 1 DoF revolute joint. To perform\npayload grasping, transportation, and release, first, time-optimal reference\ntrajectories are designed through specific waypoints to ensure the fast and\nreliable execution of the tasks. Then, a two-stage motion control approach is\ndeveloped based on a robust geometric controller for precise and reliable\nreference tracking and a linear--quadratic payload regulator for rapid setpoint\nstabilization of the payload swing. Furthermore, stability of the closed-loop\nsystem is mathematically proven to give safety guarantee for its operation. The\nproposed control architecture and design are evaluated in a high-fidelity\nphysical simulator, and also in real flight experiments, using a custom-made\nquadrotor--hook manipulator platform.\n","authors":["Péter Antal","Tamás Péni","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2304.02444v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07658v3","updated":"2024-03-26T08:01:29Z","published":"2024-01-15T13:00:35Z","title":"Robustness Evaluation of Localization Techniques for Autonomous Racing","summary":"  This work introduces SynPF, an MCL-based algorithm tailored for high-speed\nracing environments. Benchmarked against Cartographer, a state-of-the-art\npose-graph SLAM algorithm, SynPF leverages synergies from previous\nparticle-filtering methods and synthesizes them for the high-performance racing\ndomain. Our extensive in-field evaluations reveal that while Cartographer\nexcels under nominal conditions, it struggles when subjected to wheel-slip, a\ncommon phenomenon in a racing scenario due to varying grip levels and\naggressive driving behaviour. Conversely, SynPF demonstrates robustness in\nthese challenging conditions and a low-latency computation time of 1.25 ms on\non-board computers without a GPU. Using the F1TENTH platform, a 1:10 scaled\nautonomous racing vehicle, this work not only highlights the vulnerabilities of\nexisting algorithms in high-speed scenarios, tested up until 7.6 m/s, but also\nemphasizes the potential of SynPF as a viable alternative, especially in\ndeteriorating odometry conditions.\n","authors":["Tian Yi Lim","Edoardo Ghignone","Nicolas Baumann","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2401.07658v3.pdf","comment":"Accepted at the Design, Automation and Test in Europe Conference 2024\n  as an extended abstract"},{"id":"http://arxiv.org/abs/2403.17417v1","updated":"2024-03-26T06:14:58Z","published":"2024-03-26T06:14:58Z","title":"Cyclic pursuit formation control for arbitrary desired shapes","summary":"  A multi-agent system comprises numerous agents that autonomously make\ndecisions to collectively accomplish tasks, drawing significant attention for\ntheir wide-ranging applications. Within this context, formation control emerges\nas a prominent task, wherein agents collaboratively shape and maneuver while\npreserving formation integrity. Our focus centers on cyclic pursuit, a method\nfacilitating the formation of circles, ellipses, and figure-eights under the\nassumption that agents can only perceive the relative positions of those\npreceding them. However, this method's scope has been restricted to these\nspecific shapes, leaving the feasibility of forming other shapes uncertain. In\nresponse, our study proposes a novel method based on cyclic pursuit capable of\nforming a broader array of shapes, enabling agents to individually shape while\npursuing preceding agents, thereby extending the repertoire of achievable\nformations. We present two scenarios concerning the information available to\nagents and devise formation control methods tailored to each scenario. Through\nextensive simulations, we demonstrate the efficacy of our proposed method in\nforming multiple shapes, including those represented as Fourier series, thereby\nunderscoring the versatility and effectiveness of our approach.\n","authors":["Anna Fujioka","Masaki Ogura","Naoki Wakamiya"],"pdf_url":"https://arxiv.org/pdf/2403.17417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17391v1","updated":"2024-03-26T05:21:17Z","published":"2024-03-26T05:21:17Z","title":"Reverse Kron reduction of Multi-phase Radial Network","summary":"  We consider the problem of identifying the admittance matrix of a three-phase\nradial network from voltage and current measurements at a subset of nodes.\nThese measurements are used to estimate a virtual network represented by the\nKron reduction (Schur complement) of the full admittance matrix. We focus on\nrecovering exactly the full admittance matrix from its Kron reduction, i.e.,\ncomputing the inverse of Schur complement. The key idea is to decompose Kron\nreduction into a sequence of iterations that maintains an invariance structure,\nand exploit this structure to reverse each step of the iterative Kron\nreduction.\n","authors":["Steven H. Low"],"pdf_url":"https://arxiv.org/pdf/2403.17391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13923v3","updated":"2024-03-26T04:36:59Z","published":"2024-03-20T18:53:22Z","title":"Credit vs. Discount-Based Congestion Pricing: A Comparison Study","summary":"  Tolling, or congestion pricing, offers a promising traffic management policy\nfor regulating congestion, but has also attracted criticism for placing\noutsized financial burdens on low-income users. Credit-based congestion pricing\n(CBCP) and discount-based congestion pricing (DBCP) policies, which\nrespectively provide travel credits and toll discounts to low-income users on\ntolled roads, have emerged as promising mechanisms for reducing traffic\ncongestion without worsening societal inequities. However, the optimal design\nof CBCP and DBCP policies, as well as their relative advantages and\ndisadvantages, remain poorly understood. To address this, we study the effects\nof implementing CBCP and DBCP policies to route users on a network of\nmulti-lane highways with tolled express lanes. We formulate a non-atomic\nrouting game framework in which a subset of eligible users is granted toll\nrelief in the form of a fixed budget or toll discount, while the remaining\nineligible users must pay out-of-pocket. We prove the existence of Nash\nequilibrium traffic flow patterns corresponding to any given CBCP or DBCP\npolicy. Under the additional assumption that eligible users have time-invariant\nVoTs, we provide a convex program to efficiently compute these equilibria. For\nnetworks consisting of a single edge, we identify conditions under which CBCP\npolicies outperform DBCP policies (and vice versa), in the sense of improving\neligible users' access to the express lane. Finally, we present empirical\nresults from a CBCP pilot study of the San Mateo 101 Express Lane Project in\nCalifornia. Our empirical results corroborate our theoretical analysis of the\nimpact of deploying credit-based and discount-based policies, and lend insights\ninto the sensitivity of their impact with respect to the travel demand and\nusers' VoTs.\n","authors":["Chih-Yuan Chiu","Devansh Jalota","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2403.13923v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17364v1","updated":"2024-03-26T04:02:09Z","published":"2024-03-26T04:02:09Z","title":"A Moreau Envelope Approach for LQR Meta-Policy Estimation","summary":"  We study the problem of policy estimation for the Linear Quadratic Regulator\n(LQR) in discrete-time linear time-invariant uncertain dynamical systems. We\npropose a Moreau Envelope-based surrogate LQR cost, built from a finite set of\nrealizations of the uncertain system, to define a meta-policy efficiently\nadjustable to new realizations. Moreover, we design an algorithm to find an\napproximate first-order stationary point of the meta-LQR cost function.\nNumerical results show that the proposed approach outperforms naive averaging\nof controllers on new realizations of the linear system. We also provide\nempirical evidence that our method has better sample complexity than\nModel-Agnostic Meta-Learning (MAML) approaches.\n","authors":["Ashwin Aravind","Mohammad Taha Toghani","César A. Uribe"],"pdf_url":"https://arxiv.org/pdf/2403.17364v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2306.02107v2","updated":"2024-03-26T03:45:05Z","published":"2023-06-03T13:14:36Z","title":"Achievable Sum Rate Optimization on NOMA-aided Cell-Free Massive MIMO\n  with Finite Blocklength Coding","summary":"  Non-orthogonal multiple access (NOMA)-aided cell-free massive multiple-input\nmultiple-output (CFmMIMO) has been considered as a promising technology to\nfulfill strict quality of service requirements for ultra-reliable low-latency\ncommunications (URLLC). However, finite blocklength coding (FBC) in URLLC makes\nit challenging to achieve the optimal performance in the NOMA-aided CFmMIMO\nsystem. In this paper, we investigate the performance of the NOMA-aided CFmMIMO\nsystem with FBC in terms of achievable sum rate (ASR). Firstly, we derive a\nlower bound (LB) on the ergodic data rate. Then, we formulate an ASR\nmaximization problem by jointly considering power allocation and user equipment\n(UE) clustering. To tackle such an intractable problem, we decompose it into\ntwo sub-problems, i.e., the power allocation problem and the UE clustering\nproblem. A successive convex approximation (SCA) algorithm is proposed to solve\nthe power allocation problem by transforming it into a series of geometric\nprogramming problems. Meanwhile, two algorithms based on graph theory are\nproposed to solve the UE clustering problem by identifying negative loops.\nFinally, alternative optimization is performed to find the maximum ASR of the\nNOMA-aided CFmMIMO system with FBC. The simulation results demonstrate that the\nproposed algorithms significantly outperform the benchmark algorithms in terms\nof ASR under various scenarios.\n","authors":["Baolin Chong","Hancheng Lu","Yuang Chen","Langtian Qin","Fengqian Guo"],"pdf_url":"https://arxiv.org/pdf/2306.02107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17338v1","updated":"2024-03-26T02:49:08Z","published":"2024-03-26T02:49:08Z","title":"Reinforcement Learning-based Receding Horizon Control using Adaptive\n  Control Barrier Functions for Safety-Critical Systems","summary":"  Optimal control methods provide solutions to safety-critical problems but\neasily become intractable. Control Barrier Functions (CBFs) have emerged as a\npopular technique that facilitates their solution by provably guaranteeing\nsafety, through their forward invariance property, at the expense of some\nperformance loss. This approach involves defining a performance objective\nalongside CBF-based safety constraints that must always be enforced.\nUnfortunately, both performance and solution feasibility can be significantly\nimpacted by two key factors: (i) the selection of the cost function and\nassociated parameters, and (ii) the calibration of parameters within the\nCBF-based constraints, which capture the trade-off between performance and\nconservativeness. %as well as infeasibility. To address these challenges, we\npropose a Reinforcement Learning (RL)-based Receding Horizon Control (RHC)\napproach leveraging Model Predictive Control (MPC) with CBFs (MPC-CBF). In\nparticular, we parameterize our controller and use bilevel optimization, where\nRL is used to learn the optimal parameters while MPC computes the optimal\ncontrol input. We validate our method by applying it to the challenging\nautomated merging control problem for Connected and Automated Vehicles (CAVs)\nat conflicting roadways. Results demonstrate improved performance and a\nsignificant reduction in the number of infeasible cases compared to traditional\nheuristic approaches used for tuning CBF-based controllers, showcasing the\neffectiveness of the proposed method.\n","authors":["Ehsan Sabouni","H. M. Sabbir Ahmad","Vittorio Giammarino","Christos G. Cassandras","Ioannis Ch. Paschalidis","Wenchao Li"],"pdf_url":"https://arxiv.org/pdf/2403.17338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17337v1","updated":"2024-03-26T02:48:52Z","published":"2024-03-26T02:48:52Z","title":"Destination-Constrained Linear Dynamical System Modeling in Set-Valued\n  Frameworks","summary":"  Directional motion towards a specified destination is a common occurrence in\nphysical processes and human societal activities. Utilizing this prior\ninformation can significantly improve the control and predictive performance of\nsystem models. This paper primarily focuses on reconstructing linear dynamic\nsystem models based on destination constraints in the set-valued framework. We\ntreat destination constraints as inherent information in the state evolution\nprocess and employ convex optimization techniques to construct a coherent and\nrobust state model. This refined model effectively captures the impact of\ndestination constraints on the state evolution at each time step. Furthermore,\nwe design an optimal weight matrix for the reconstructed model to ensure\nsmoother and more natural trajectories of state evolution. We also analyze the\ntheoretical guarantee of optimality for this weight matrix and the properties\nof the reconstructed model. Finally, simulation experiments verify that the\nreconstructed model has significant advantages over the unconstrained and\nunoptimized weighted models and constrains the evolution of state trajectories\nwith different starting and ending points.\n","authors":["Xiaowei Yang","Haiqi Liu","Fanqin Meng","Xiaojing Shen"],"pdf_url":"https://arxiv.org/pdf/2403.17337v1.pdf","comment":"15 pages, 11 figures"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2403.09887v2","updated":"2024-03-26T23:52:35Z","published":"2024-03-14T21:44:48Z","title":"Sabiá-2: A New Generation of Portuguese Large Language Models","summary":"  We introduce Sabi\\'a-2, a family of large language models trained on\nPortuguese texts. The models are evaluated on a diverse range of exams,\nincluding entry-level tests for Brazilian universities, professional\ncertification exams, and graduate-level exams for various disciplines such as\naccounting, economics, engineering, law and medicine. Our results reveal that\nour best model so far, Sabi\\'a-2 Medium, matches or surpasses GPT-4's\nperformance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64\nexams. Notably, specialization has a significant impact on a model's\nperformance without the need to increase its size, allowing us to offer\nSabi\\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.\nFinally, we identified that math and coding are key abilities that need\nimprovement.\n","authors":["Thales Sales Almeida","Hugo Abonizio","Rodrigo Nogueira","Ramon Pires"],"pdf_url":"https://arxiv.org/pdf/2403.09887v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18159v1","updated":"2024-03-26T23:51:44Z","published":"2024-03-26T23:51:44Z","title":"Oh! We Freeze: Improving Quantized Knowledge Distillation via Signal\n  Propagation Analysis for Large Language Models","summary":"  Large generative models, such as large language models (LLMs) and diffusion\nmodels have as revolutionized the fields of NLP and computer vision\nrespectively. However, their slow inference, high computation and memory\nrequirement makes it challenging to deploy them on edge devices. In this study,\nwe propose a light-weight quantization aware fine tuning technique using\nknowledge distillation (KD-QAT) to improve the performance of 4-bit weight\nquantized LLMs using commonly available datasets to realize a popular language\nuse case, on device chat applications. To improve this paradigm of finetuning,\nas main contributions, we provide insights into stability of KD-QAT by\nempirically studying the gradient propagation during training to better\nunderstand the vulnerabilities of KD-QAT based approaches to low-bit\nquantization errors. Based on our insights, we propose ov-freeze, a simple\ntechnique to stabilize the KD-QAT process. Finally, we experiment with the\npopular 7B LLaMAv2-Chat model at 4-bit quantization level and demonstrate that\nov-freeze results in near float-point precision performance, i.e., less than\n0.7% loss of accuracy on Commonsense Reasoning benchmarks.\n","authors":["Kartikeya Bhardwaj","Nilesh Prasad Pandey","Sweta Priyadarshi","Kyunggeun Lee","Jun Ma","Harris Teague"],"pdf_url":"https://arxiv.org/pdf/2403.18159v1.pdf","comment":"Accepted at Practical ML for Low Resource Settings Workshop at ICLR\n  2024"},{"id":"http://arxiv.org/abs/2403.18148v1","updated":"2024-03-26T23:14:34Z","published":"2024-03-26T23:14:34Z","title":"Large Language Models Produce Responses Perceived to be Empathic","summary":"  Large Language Models (LLMs) have demonstrated surprising performance on many\ntasks, including writing supportive messages that display empathy. Here, we had\nthese models generate empathic messages in response to posts describing common\nlife experiences, such as workplace situations, parenting, relationships, and\nother anxiety- and anger-eliciting situations. Across two studies (N=192, 202),\nwe showed human raters a variety of responses written by several models (GPT4\nTurbo, Llama2, and Mistral), and had people rate these responses on how\nempathic they seemed to be. We found that LLM-generated responses were\nconsistently rated as more empathic than human-written responses. Linguistic\nanalyses also show that these models write in distinct, predictable ``styles\",\nin terms of their use of punctuation, emojis, and certain words. These results\nhighlight the potential of using LLMs to enhance human peer support in contexts\nwhere empathy is important.\n","authors":["Yoon Kyung Lee","Jina Suh","Hongli Zhan","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2403.18148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18145v1","updated":"2024-03-26T23:10:41Z","published":"2024-03-26T23:10:41Z","title":"A Real-Time Rescheduling Algorithm for Multi-robot Plan Execution","summary":"  One area of research in multi-agent path finding is to determine how\nreplanning can be efficiently achieved in the case of agents being delayed\nduring execution. One option is to reschedule the passing order of agents,\ni.e., the sequence in which agents visit the same location. In response, we\npropose Switchable-Edge Search (SES), an A*-style algorithm designed to find\noptimal passing orders. We prove the optimality of SES and evaluate its\nefficiency via simulations. The best variant of SES takes less than 1 second\nfor small- and medium-sized problems and runs up to 4 times faster than\nbaselines for large-sized problems.\n","authors":["Ying Feng","Adittyo Paul","Zhe Chen","Jiaoyang Li"],"pdf_url":"https://arxiv.org/pdf/2403.18145v1.pdf","comment":"ICAPS 2024"},{"id":"http://arxiv.org/abs/2303.09618v2","updated":"2024-03-26T22:59:52Z","published":"2023-03-16T19:47:41Z","title":"HIVE: Harnessing Human Feedback for Instructional Visual Editing","summary":"  Incorporating human feedback has been shown to be crucial to align text\ngenerated by large language models to human preferences. We hypothesize that\nstate-of-the-art instructional image editing models, where outputs are\ngenerated based on an input image and an editing instruction, could similarly\nbenefit from human feedback, as their outputs may not adhere to the correct\ninstructions and preferences of users. In this paper, we present a novel\nframework to harness human feedback for instructional visual editing (HIVE).\nSpecifically, we collect human feedback on the edited images and learn a reward\nfunction to capture the underlying user preferences. We then introduce scalable\ndiffusion model fine-tuning methods that can incorporate human preferences\nbased on the estimated reward. Besides, to mitigate the bias brought by the\nlimitation of data, we contribute a new 1M training dataset, a 3.6K reward\ndataset for rewards learning, and a 1K evaluation dataset to boost the\nperformance of instructional image editing. We conduct extensive empirical\nexperiments quantitatively and qualitatively, showing that HIVE is favored over\nprevious state-of-the-art instructional image editing approaches by a large\nmargin.\n","authors":["Shu Zhang","Xinyi Yang","Yihao Feng","Can Qin","Chia-Chih Chen","Ning Yu","Zeyuan Chen","Huan Wang","Silvio Savarese","Stefano Ermon","Caiming Xiong","Ran Xu"],"pdf_url":"https://arxiv.org/pdf/2303.09618v2.pdf","comment":"In CVPR, 2024"},{"id":"http://arxiv.org/abs/2403.18140v1","updated":"2024-03-26T22:54:12Z","published":"2024-03-26T22:54:12Z","title":"Juru: Legal Brazilian Large Language Model from Reputable Sources","summary":"  The high computational cost associated with pretraining large language models\nlimits their research. Two strategies have emerged to address this issue:\ndomain specialization and pretraining with high-quality data. To explore these\nstrategies, we specialized the Sabi\\'a-2 Small model with 1.9 billion unique\ntokens from reputable Brazilian legal sources and conducted few-shot\nevaluations on legal and general knowledge exams. Our model, Juru, demonstrates\nthe benefits of domain specialization with a reduced amount of pretraining\ndata. However, this specialization comes at the expense of degrading\nperformance in other knowledge areas within the same language. This study\ncontributes to the growing body of scientific evidence showing that pretraining\ndata selection may enhance the performance of large language models, enabling\nthe exploration of these models at a lower cost.\n","authors":["Roseval Malaquias Junior","Ramon Pires","Roseli Romero","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2403.18140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05677v2","updated":"2024-03-26T22:53:56Z","published":"2023-12-09T20:51:48Z","title":"Batched Low-Rank Adaptation of Foundation Models","summary":"  Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning\nfoundation models by incorporating trainable low-rank matrices, thereby\nreducing the number of trainable parameters. While LoRA offers numerous\nadvantages, its applicability for real-time serving to a diverse and global\nuser base is constrained by its incapability to handle multiple task-specific\nadapters efficiently. This imposes a performance bottleneck in scenarios\nrequiring personalized, task-specific adaptations for each incoming request. To\nmitigate this constraint, we introduce Fast LoRA (FLoRA), a framework in which\neach input example in a minibatch can be associated with its unique low-rank\nadaptation weights, allowing for efficient batching of heterogeneous requests.\nWe empirically demonstrate that FLoRA retains the performance merits of LoRA,\nshowcasing competitive results on the MultiPL-E code generation benchmark\nspanning over 8 languages and a multilingual speech recognition task across 6\nlanguages.\n","authors":["Yeming Wen","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2312.05677v2.pdf","comment":"16 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.18136v1","updated":"2024-03-26T22:41:41Z","published":"2024-03-26T22:41:41Z","title":"Securing GNNs: Explanation-Based Identification of Backdoored Training\n  Graphs","summary":"  Graph Neural Networks (GNNs) have gained popularity in numerous domains, yet\nthey are vulnerable to backdoor attacks that can compromise their performance\nand ethical application. The detection of these attacks is crucial for\nmaintaining the reliability and security of GNN classification tasks, but\neffective detection techniques are lacking. Following an initial investigation,\nwe observed that while graph-level explanations can offer limited insights,\ntheir effectiveness in detecting backdoor triggers is inconsistent and\nincomplete. To bridge this gap, we extract and transform secondary outputs of\nGNN explanation mechanisms, designing seven novel metrics that more effectively\ndetect backdoor attacks. Additionally, we develop an adaptive attack to\nrigorously evaluate our approach. We test our method on multiple benchmark\ndatasets and examine its efficacy against various attack models. Our results\nshow that our method can achieve high detection performance, marking a\nsignificant advancement in safeguarding GNNs against backdoor attacks.\n","authors":["Jane Downer","Ren Wang","Binghui Wang"],"pdf_url":"https://arxiv.org/pdf/2403.18136v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18133v1","updated":"2024-03-26T22:28:43Z","published":"2024-03-26T22:28:43Z","title":"AE SemRL: Learning Semantic Association Rules with Autoencoders","summary":"  Association Rule Mining (ARM) is the task of learning associations among data\nfeatures in the form of logical rules. Mining association rules from\nhigh-dimensional numerical data, for example, time series data from a large\nnumber of sensors in a smart environment, is a computationally intensive task.\nIn this study, we propose an Autoencoder-based approach to learn and extract\nassociation rules from time series data (AE SemRL). Moreover, we argue that in\nthe presence of semantic information related to time series data sources,\nsemantics can facilitate learning generalizable and explainable association\nrules. Despite enriching time series data with additional semantic features, AE\nSemRL makes learning association rules from high-dimensional data feasible. Our\nexperiments show that semantic association rules can be extracted from a latent\nrepresentation created by an Autoencoder and this method has in the order of\nhundreds of times faster execution time than state-of-the-art ARM approaches in\nmany scenarios. We believe that this study advances a new way of extracting\nassociations from representations and has the potential to inspire more\nresearch in this field.\n","authors":["Erkan Karabulut","Victoria Degeler","Paul Groth"],"pdf_url":"https://arxiv.org/pdf/2403.18133v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18132v1","updated":"2024-03-26T22:26:39Z","published":"2024-03-26T22:26:39Z","title":"Recommendation of data-free class-incremental learning algorithms by\n  simulating future data","summary":"  Class-incremental learning deals with sequential data streams composed of\nbatches of classes. Various algorithms have been proposed to address the\nchallenging case where samples from past classes cannot be stored. However,\nselecting an appropriate algorithm for a user-defined setting is an open\nproblem, as the relative performance of these algorithms depends on the\nincremental settings. To solve this problem, we introduce an algorithm\nrecommendation method that simulates the future data stream. Given an initial\nset of classes, it leverages generative models to simulate future classes from\nthe same visual domain. We evaluate recent algorithms on the simulated stream\nand recommend the one which performs best in the user-defined incremental\nsetting. We illustrate the effectiveness of our method on three large datasets\nusing six algorithms and six incremental settings. Our method outperforms\ncompetitive baselines, and performance is close to that of an oracle choosing\nthe best algorithm in each setting. This work contributes to facilitate the\npractical deployment of incremental learning.\n","authors":["Eva Feillet","Adrian Popescu","Céline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2403.18132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18120v1","updated":"2024-03-26T22:01:13Z","published":"2024-03-26T22:01:13Z","title":"Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with\n  Autoformalization","summary":"  Large language models (LLM), such as Google's Minerva and OpenAI's GPT\nfamilies, are becoming increasingly capable of solving mathematical\nquantitative reasoning problems. However, they still make unjustified logical\nand computational errors in their reasoning steps and answers. In this paper,\nwe leverage the fact that if the training corpus of LLMs contained sufficiently\nmany examples of formal mathematics (e.g. in Isabelle, a formal theorem proving\nenvironment), they can be prompted to translate i.e. autoformalize informal\nmathematical statements into formal Isabelle code -- which can be verified\nautomatically for internal consistency. This provides a mechanism to\nautomatically reject solutions whose formalized versions are inconsistent\nwithin themselves or with the formalized problem statement. We evaluate our\nmethod on GSM8K, MATH and MultiArith datasets and demonstrate that our approach\nprovides a consistently better heuristic than vanilla majority voting -- the\npreviously best method to identify correct answers, by more than 12% on GSM8K.\nIn our experiments it improves results consistently across all datasets and LLM\nmodel sizes. The code can be found at https://github.com/jinpz/dtv.\n","authors":["Jin Peng Zhou","Charles Staats","Wenda Li","Christian Szegedy","Kilian Q. Weinberger","Yuhuai Wu"],"pdf_url":"https://arxiv.org/pdf/2403.18120v1.pdf","comment":"ICLR 2024"}],"Computational Engineering":[{"id":"http://arxiv.org/abs/2403.17679v1","updated":"2024-03-26T13:08:56Z","published":"2024-03-26T13:08:56Z","title":"Shape Optimization of Geometrically Nonlinear Modal Coupling\n  Coefficients: An Application to MEMS Gyroscopes","summary":"  Micro- and nanoelectromechanical system (MEMS and NEMS) resonators can\nexhibit rich nonlinear dynamics as they are often operated at large amplitudes\nwith high quality factors and possess a high mode density with a variety of\nnonlinear modal couplings. Their impact is strongly influenced by internal\nresonance conditions and by the strength of the modal coupling coefficients. On\none hand, strong nonlinear couplings are of academic interest and promise novel\ndevice concepts. On the other hand, however, they have the potential to disturb\nthe linear system behavior on which industrial devices such as gyroscopes and\nmicro mirrors are based on. In either case, being able to optimize the coupling\ncoefficients by design is certainly beneficial. A main source of nonlinear\nmodal couplings are geometric nonlinearities. In this work, we apply node-based\nshape optimization to tune the geometrically nonlinear 3-wave coupling\ncoefficients of a MEMS gyroscope. We demonstrate that individual coupling\ncoefficients can be tuned over several orders of magnitude by shape\noptimization, while satisfying typical constraints on manufacturability and\noperability of the devices. The optimized designs contain unintuitive\ngeometrical features far away from any solution an experienced human MEMS or\nNEMS designer could have thought of. Thus, this work demonstrates the power of\nshape optimization for tailoring the complex nonlinear dynamic properties of\nMEMS and NEMS resonators.\n","authors":["Daniel Schiwietz","Marian Hörsting","Eva Maria Weig","Matthias Wenzel","Peter Degenfeld-Schonburg"],"pdf_url":"https://arxiv.org/pdf/2403.17679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17597v1","updated":"2024-03-26T11:08:48Z","published":"2024-03-26T11:08:48Z","title":"An Exact Solution for Allocating Car Parking Spaces on Campus","summary":"  All over the world, especially in the university environment, planning\nmanagers and traffic engineers are constantly faced with the problem of\ninadequate allocation of car parking spaces to demanded users. Users could\neither prefer reserved parking spaces to unreserved parking spaces or vice\nversa. This makes the campus parking manager to be faced with two basic problem\nwhich are: the problem of allocating the actual number of available reserved\nspaces to users without any conflict over the same parking space, and the\nproblem of determining the number of parking permit to be issued for parking\nlot with unreserved spaces. Hence, an optimal or available solution to the\nproblem is required. This paper investigates a model for allocating car parking\nspaces, adds a constraint to address the reserved parking policy in a\nuniversity environment and solves the parking allocation problem using an exact\nsolution method. The result obtained gives the value of the objective function\nand the optimal allocation of users to each parking lot.\n","authors":["Luke Oluwaseye Joel","Sawyerr A. Babatunde","Adewumi O. Aderemi"],"pdf_url":"https://arxiv.org/pdf/2403.17597v1.pdf","comment":"An International Multidiscinary Conference on Research, Development\n  and Practices in Science, Technology, Education, Arts, Management & the\n  Social Science (iSTEAMS). Conference Centre, University of Ibandan, Nigeria.\n  30 May - 01 June 2013"},{"id":"http://arxiv.org/abs/2312.12467v3","updated":"2024-03-26T01:50:54Z","published":"2023-12-19T05:30:08Z","title":"Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh\n  Transformer","summary":"  Recently, many mesh-based graph neural network (GNN) models have been\nproposed for modeling complex high-dimensional physical systems. Remarkable\nachievements have been made in significantly reducing the solving time compared\nto traditional numerical solvers. These methods are typically designed to i)\nreduce the computational cost in solving physical dynamics and/or ii) propose\ntechniques to enhance the solution accuracy in fluid and rigid body dynamics.\nHowever, it remains under-explored whether they are effective in addressing the\nchallenges of flexible body dynamics, where instantaneous collisions occur\nwithin a very short timeframe. In this paper, we present Hierarchical Contact\nMesh Transformer (HCMT), which uses hierarchical mesh structures and can learn\nlong-range dependencies (occurred by collisions) among spatially distant\npositions of a body -- two close positions in a higher-level mesh correspond to\ntwo distant positions in a lower-level mesh. HCMT enables long-range\ninteractions, and the hierarchical mesh structure quickly propagates collision\neffects to faraway positions. To this end, it consists of a contact mesh\nTransformer and a hierarchical mesh Transformer (CMT and HMT, respectively).\nLastly, we propose a flexible body dynamics dataset, consisting of trajectories\nthat reflect experimental settings frequently used in the display industry for\nproduct designs. We also compare the performance of several baselines using\nwell-known benchmark datasets. Our results show that HCMT provides significant\nperformance improvements over existing methods. Our code is available at\nhttps://github.com/yuyudeep/hcmt.\n","authors":["Youn-Yeol Yu","Jeongwhan Choi","Woojin Cho","Kookjin Lee","Nayong Kim","Kiseok Chang","Chang-Seung Woo","Ilho Kim","Seok-Woo Lee","Joon-Young Yang","Sooyoung Yoon","Noseong Park"],"pdf_url":"https://arxiv.org/pdf/2312.12467v3.pdf","comment":"Accepted at ICLR 2024"}]},"2024-03-25T00:00:00Z":{"Systems and Control":[{"id":"http://arxiv.org/abs/2403.17272v1","updated":"2024-03-25T23:34:22Z","published":"2024-03-25T23:34:22Z","title":"Optimal Operation of Reconfigurable Active Distribution Networks Aiming\n  at Resiliency Improvement","summary":"  As natural disasters bring about power outage and financial losses, network\nresiliency is an important challenge for distribution network operators (DNOs).\nOn the other side, power loss reduction during normal operating condition is a\nmajor concern of DNOs. In this paper, optimal scheduling of active distribution\nnetwork (ADN) is addressed through simultaneous minimization of power loss in\nnormal condition and load shedding in critical condition after natural\ndisasters. A new formulation is developed for the network reconfiguration to\noptimize the system operation in both normal and emergency conditions in the\npresence of conventional and renewable-energy-based distributed generation (DG)\nas well as energy storage systems (ESSs). The line flow based (LFB) algorithm\nis used for the AC power flow calculations, and all the developed relations\nhave been convexified to construct a mixed-integer quadratically-constrained\nprogramming (MIQCP) optimization model. The simulations have been implemented\non the IEEE 33-bus system in GAMS, and the results are investigated.\n","authors":["Saeed Behzadi","Amir Bagheri","Abbas Rabiee"],"pdf_url":"https://arxiv.org/pdf/2403.17272v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17249v1","updated":"2024-03-25T22:51:27Z","published":"2024-03-25T22:51:27Z","title":"Impact-Aware Bimanual Catching of Large-Momentum Objects","summary":"  This paper investigates one of the most challenging tasks in dynamic\nmanipulation -- catching large-momentum moving objects. Beyond the realm of\nquasi-static manipulation, dealing with highly dynamic objects can\nsignificantly improve the robot's capability of interacting with its\nsurrounding environment. Yet, the inevitable motion mismatch between the fast\nmoving object and the approaching robot will result in large impulsive forces,\nwhich lead to the unstable contacts and irreversible damage to both the object\nand the robot. To address the above problems, we propose an online optimization\nframework to: 1) estimate and predict the linear and angular motion of the\nobject; 2) search and select the optimal contact locations across every surface\nof the object to mitigate impact through sequential quadratic programming\n(SQP); 3) simultaneously optimize the end-effector motion, stiffness, and\ncontact force for both robots using multi-mode trajectory optimization (MMTO);\nand 4) realise the impact-aware catching motion on the compliant robotic system\nbased on indirect force controller. We validate the impulse distribution,\ncontact selection, and impact-aware MMTO algorithms in simulation and\ndemonstrate the benefits of the proposed framework in real-world experiments\nincluding catching large-momentum moving objects with well-defined motion,\nconstrained motion and free-flying motion.\n","authors":["Lei Yan","Theodoros Stouraitis","João Moura","Wenfu Xu","Michael Gienger","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2403.17249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17247v1","updated":"2024-03-25T22:49:56Z","published":"2024-03-25T22:49:56Z","title":"DASA: Delay-Adaptive Multi-Agent Stochastic Approximation","summary":"  We consider a setting in which $N$ agents aim to speedup a common Stochastic\nApproximation (SA) problem by acting in parallel and communicating with a\ncentral server. We assume that the up-link transmissions to the server are\nsubject to asynchronous and potentially unbounded time-varying delays. To\nmitigate the effect of delays and stragglers while reaping the benefits of\ndistributed computation, we propose \\texttt{DASA}, a Delay-Adaptive algorithm\nfor multi-agent Stochastic Approximation. We provide a finite-time analysis of\n\\texttt{DASA} assuming that the agents' stochastic observation processes are\nindependent Markov chains. Significantly advancing existing results,\n\\texttt{DASA} is the first algorithm whose convergence rate depends only on the\nmixing time $\\tmix$ and on the average delay $\\tau_{avg}$ while jointly\nachieving an $N$-fold convergence speedup under Markovian sampling. Our work is\nrelevant for various SA applications, including multi-agent and distributed\ntemporal difference (TD) learning, Q-learning and stochastic optimization with\ncorrelated data.\n","authors":["Nicolo Dal Fabbro","Arman Adibi","H. Vincent Poor","Sanjeev R. Kulkarni","Aritra Mitra","George J. Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.17247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17235v1","updated":"2024-03-25T22:25:03Z","published":"2024-03-25T22:25:03Z","title":"A Discrete-Time Least-Squares Adaptive State Tracking Control Scheme\n  with A Mobile-Robot System Study","summary":"  This paper develops an adaptive state tracking control scheme for\ndiscrete-time systems, using the least-squares algorithm, as the new solution\nto the long-standing discrete-time adaptive state tracking control problem to\nwhich the Lyapunov method (well-developed for the continuous-time adaptive\nstate tracking problem) is not applicable. The new adaptive state tracking\nscheme is based on a recently-developed new discrete-time error model which has\nbeen used for gradient algorithm based state tracking control schemes, and uses\nthe least-squares algorithm for parameter adaptation. The new least-squares\nalgorithm is derived to minimize an accumulative estimation error, to ensure\ncertain optimality for parameter estimation. The system stability and output\ntracking properties are studied. Technical results are presented in terms of\nplant-model matching, error model, adaptive law, optimality formulation, and\nstability and tracking analysis. The developed adaptive control scheme is\napplied to a discrete-time multiple mobile robot system to meet an adaptive\nstate tracking objective. In addition, a collision avoidance mechanism is\nproposed to prevent collisions in the whole tracking process. Simulation\nresults are presented, which verify the desired system state tracking\nproperties under the developed least-squares algorithm based adaptive control\nscheme.\n","authors":["Qianhong Zhao","Gang Tao"],"pdf_url":"https://arxiv.org/pdf/2403.17235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17233v1","updated":"2024-03-25T22:20:45Z","published":"2024-03-25T22:20:45Z","title":"Active Learning of Dynamics Using Prior Domain Knowledge in the Sampling\n  Process","summary":"  We present an active learning algorithm for learning dynamics that leverages\nside information by explicitly incorporating prior domain knowledge into the\nsampling process. Our proposed algorithm guides the exploration toward regions\nthat demonstrate high empirical discrepancy between the observed data and an\nimperfect prior model of the dynamics derived from side information. Through\nnumerical experiments, we demonstrate that this strategy explores regions of\nhigh discrepancy and accelerates learning while simultaneously reducing model\nuncertainty. We rigorously prove that our active learning algorithm yields a\nconsistent estimate of the underlying dynamics by providing an explicit rate of\nconvergence for the maximum predictive variance. We demonstrate the efficacy of\nour approach on an under-actuated pendulum system and on the half-cheetah\nMuJoCo environment.\n","authors":["Kevin S. Miller","Adam J. Thorpe","Ufuk Topcu"],"pdf_url":"https://arxiv.org/pdf/2403.17233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06157v5","updated":"2024-03-25T21:23:11Z","published":"2023-06-10T23:50:02Z","title":"Fault Localization for Buggy Deep Learning Framework Conversions in\n  Image Recognition","summary":"  When deploying Deep Neural Networks (DNNs), developers often convert models\nfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).\nHowever, this process is error-prone and can impact target model accuracy. To\nidentify the extent of such impact, we perform and briefly present a\ndifferential analysis against three DNNs widely used for image recognition\n(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep\nlearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which\nrevealed numerous model crashes and output label discrepancies of up to 100%.\nTo mitigate such errors, we present a novel approach towards fault localization\nand repair of buggy deep learning framework conversions, focusing on\npre-trained image recognition models. Our technique consists of four stages of\nanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,\nand 4) graph representation. In addition, we propose various strategies towards\nfault repair of the faults detected. We implement our technique on top of the\nApache TVM deep learning compiler, and we test it by conducting a preliminary\nfault localization analysis for the conversion of InceptionV3 from TF to\nTFLite. Our approach detected a fault in a common DNN converter tool, which\nintroduced precision errors in weights, reducing model accuracy. After our\nfault localization, we repaired the issue, reducing our conversion error to\nzero.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06157v5.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2306.06208v5","updated":"2024-03-25T21:08:25Z","published":"2023-06-05T23:07:01Z","title":"DeltaNN: Assessing the Impact of Computational Environment Parameters on\n  the Performance of Image Recognition Models","summary":"  Image recognition tasks typically use deep learning and require enormous\nprocessing power, thus relying on hardware accelerators like GPUs and TPUs for\nfast, timely processing. Failure in real-time image recognition tasks can occur\ndue to sub-optimal mapping on hardware accelerators during model deployment,\nwhich may lead to timing uncertainty and erroneous behavior. Mapping on\nhardware accelerators is done using multiple software components like deep\nlearning frameworks, compilers, and device libraries, that we refer to as the\ncomputational environment. Owing to the increased use of image recognition\ntasks in safety-critical applications like autonomous driving and medical\nimaging, it is imperative to assess their robustness to changes in the\ncomputational environment, as the impact of parameters like deep learning\nframeworks, compiler optimizations, and hardware devices on model performance\nand correctness is not yet well understood.\n  In this paper we present a differential testing framework, DeltaNN, that\nallows us to assess the impact of different computational environment\nparameters on the performance of image recognition models during deployment,\npost training. DeltaNN generates different implementations of a given image\nrecognition model for variations in environment parameters, namely, deep\nlearning frameworks, compiler optimizations and hardware devices and analyzes\ndifferences in model performance as a result. Using DeltaNN, we conduct an\nempirical study of robustness analysis of three popular image recognition\nmodels using the ImageNet dataset. We report the impact in terms of\nmisclassifications and inference time differences across different settings. In\ntotal, we observed up to 100% output label differences across deep learning\nframeworks, and up to 81% unexpected performance degradation in terms of\ninference time, when applying compiler optimizations.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06208v5.pdf","comment":"11 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2403.17191v1","updated":"2024-03-25T21:02:51Z","published":"2024-03-25T21:02:51Z","title":"High-dimensional continuification control of large-scale multi-agent\n  systems under limited sensing and perturbations","summary":"  This paper investigates the robustness of a novel high-dimensional\ncontinuification control method for complex multi-agent systems. We begin by\nformulating a partial differential equation describing the spatio-temporal\ndensity dynamics of swarming agents. A stable control action for the density is\nthen derived and validated under nominal conditions. Subsequently, we\ndiscretize this macroscopic strategy into actionable velocity inputs for the\nsystem's agents. Our analysis demonstrates the robustness of the approach\nbeyond idealized assumptions of unlimited sensing and absence of perturbations.\n","authors":["Gian Carlo Maffettone","Mario di Bernardo","Maurizio Porfiri"],"pdf_url":"https://arxiv.org/pdf/2403.17191v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.01573"},{"id":"http://arxiv.org/abs/2403.17184v1","updated":"2024-03-25T20:53:17Z","published":"2024-03-25T20:53:17Z","title":"Robust Finite-time Stabilization of Linear Systems with Limited State\n  Quantization","summary":"  This paper investigates the robust asymptotic stabilization of a linear\ntime-invariant (LTI) system by a static feedback with a static state\nquantization. It is shown that the controllable LTI system can be stabilized to\nzero in a finite time by means of a nonlinear feedback with a quantizer having\na limited (finite) number of values (quantization seeds) even when all\nparameters of the controller and the quantizer are time-invariant. The control\ndesign is based on generalized homogeneity. A homogeneous spherical quantizer\nis introduced. The static homogeneous feedback is shown to be local (or global)\nfinite-time stabilizer for the linear system (dependently of the system\nmatrix). The tuning rules for both the quantizer and the feedback law are\nobtained in the form of Linear Matrix Inequalities (LMIs). The closed-loop\nsystem is proven to be robust with respect to some bounded matched and\nvanishing mismatched perturbations. Theoretical results are supported by\nnumerical simulations. \\\n","authors":["Yu Zhou","Andrey Polyakov","Gang Zheng"],"pdf_url":"https://arxiv.org/pdf/2403.17184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17174v1","updated":"2024-03-25T20:43:17Z","published":"2024-03-25T20:43:17Z","title":"Belief Samples Are All You Need For Social Learning","summary":"  In this paper, we consider the problem of social learning, where a group of\nagents embedded in a social network are interested in learning an underlying\nstate of the world. Agents have incomplete, noisy, and heterogeneous sources of\ninformation, providing them with recurring private observations of the\nunderlying state of the world. Agents can share their learning experience with\ntheir peers by taking actions observable to them, with values from a finite\nfeasible set of states. Actions can be interpreted as samples from the beliefs\nwhich agents may form and update on what the true state of the world is.\nSharing samples, in place of full beliefs, is motivated by the limited\ncommunication, cognitive, and information-processing resources available to\nagents especially in large populations. Previous work (Salhab et al.) poses the\nquestion as to whether learning with probability one is still achievable if\nagents are only allowed to communicate samples from their beliefs. We provide a\ndefinite positive answer to this question, assuming a strongly connected\nnetwork and a ``collective distinguishability'' assumption, which are both\nrequired for learning even in full-belief-sharing settings. In our proposed\nbelief update mechanism, each agent's belief is a normalized weighted geometric\ninterpolation between a fully Bayesian private belief -- aggregating\ninformation from the private source -- and an ensemble of empirical\ndistributions of the samples shared by her neighbors over time. By carefully\nconstructing asymptotic almost-sure lower/upper bounds on the frequency of\nshared samples matching the true state/or not, we rigorously prove the\nconvergence of all the beliefs to the true state, with probability one.\n","authors":["Mahyar JafariNodeh","Amir Ajorlou","Ali Jadbabaie"],"pdf_url":"https://arxiv.org/pdf/2403.17174v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2403.17157v1","updated":"2024-03-25T20:16:05Z","published":"2024-03-25T20:16:05Z","title":"Output-feedback Synthesis Orbit Geometry: Quotient Manifolds and LQG\n  Direct Policy Optimization","summary":"  In this paper, we consider direct policy optimization for the\nlinear-quadratic Gaussian (LQG) setting. Over the past few years, it has been\nrecognized that the landscape of stabilizing output-feedback controllers of\nrelevance to LQG has an intricate geometry, particularly as it pertains to the\nexistence of spurious stationary points. In order to address such challenges,\nin this paper, we first adopt a Riemannian metric for the space of stabilizing\nfull-order minimal output-feedback controllers. We then proceed to prove that\nthe orbit of such controllers modulo coordinate transformation admits a\nRiemannian quotient manifold structure. This geometric structure is then used\nto develop a Riemannian gradient descent for the direct LQG policy\noptimization. We prove a local convergence guarantee with linear rate and show\nthe proposed approach exhibits significantly faster and more robust numerical\nperformance as compared with ordinary gradient descent for LQG. Subsequently,\nwe provide reasons for this observed behavior; in particular, we argue that\noptimizing over the orbit space of controllers is the right theoretical and\ncomputational setup for direct LQG policy optimization.\n","authors":["Spencer Kraisler","Mehran Mesbahi"],"pdf_url":"https://arxiv.org/pdf/2403.17157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17142v1","updated":"2024-03-25T19:39:17Z","published":"2024-03-25T19:39:17Z","title":"Approximation with Random Shallow ReLU Networks with Applications to\n  Model Reference Adaptive Control","summary":"  Neural networks are regularly employed in adaptive control of nonlinear\nsystems and related methods o reinforcement learning. A common architecture\nuses a neural network with a single hidden layer (i.e. a shallow network), in\nwhich the weights and biases are fixed in advance and only the output layer is\ntrained. While classical results show that there exist neural networks of this\ntype that can approximate arbitrary continuous functions over bounded regions,\nthey are non-constructive, and the networks used in practice have no\napproximation guarantees. Thus, the approximation properties required for\ncontrol with neural networks are assumed, rather than proved. In this paper, we\naim to fill this gap by showing that for sufficiently smooth functions, ReLU\nnetworks with randomly generated weights and biases achieve $L_{\\infty}$ error\nof $O(m^{-1/2})$ with high probability, where $m$ is the number of neurons. It\nsuffices to generate the weights uniformly over a sphere and the biases\nuniformly over an interval. We show how the result can be used to get\napproximations of required accuracy in a model reference adaptive control\napplication.\n","authors":["Andrew Lamperski","Tyler Lekang"],"pdf_url":"https://arxiv.org/pdf/2403.17142v1.pdf","comment":"Under Review for Conference on Decision and Control"},{"id":"http://arxiv.org/abs/2310.13843v2","updated":"2024-03-25T19:36:52Z","published":"2023-10-20T22:32:42Z","title":"Long Solution Times or Low Solution Quality: On Trade-Offs in Choosing a\n  Power Flow Formulation for the Optimal Power Shutoff Problem","summary":"  The Optimal Power Shutoff (OPS) problem is an optimization problem that makes\npower line de-energization decisions in order to reduce the risk of igniting a\nwildfire, while minimizing the load shed of customers. This problem, with DC\nlinear power flow equations, has been used in many studies in recent years.\nHowever, using linear approximations for power flow when making decisions on\nthe network topology is known to cause challenges with AC feasibility of the\nresulting network, as studied in the related contexts of optimal transmission\nswitching or grid restoration planning. This paper explores the accuracy of the\nDC OPS formulation and the ability to recover an AC-feasible power flow\nsolution after de-energization decisions are made. We also extend the OPS\nproblem to include variants with the AC, Second-Order-Cone, and Network-Flow\npower flow equations, and compare them to the DC approximation with respect to\nsolution quality and time. The results highlight that the DC approximation\noverestimates the amount of load that can be served, leading to poor\nde-energization decisions. The AC and SOC-based formulations are better, but\nprohibitively slow to solve for even modestly sized networks thus demonstrating\nthe need for new solution methods with better trade-offs between computational\ntime and solution quality.\n","authors":["Eric Haag","Noah Rhodes","Line Roald"],"pdf_url":"https://arxiv.org/pdf/2310.13843v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17136v1","updated":"2024-03-25T19:18:25Z","published":"2024-03-25T19:18:25Z","title":"Adaptive Step Duration for Precise Foot Placement: Achieving Robust\n  Bipedal Locomotion on Terrains with Restricted Footholds","summary":"  This paper introduces a novel multi-step preview foot placement planning\nalgorithm designed to enhance the robustness of bipedal robotic walking across\nchallenging terrains with restricted footholds. Traditional one-step preview\nplanning struggles to maintain stability when stepping areas are severely\nlimited, such as with random stepping stones. In this work, we developed a\ndiscrete-time Model Predictive Control (MPC) based on the step-to-step discrete\nevolution of the Divergent Component of Motion (DCM) of bipedal locomotion.\nThis approach adaptively changes the step duration for optimal foot placement\nunder constraints, thereby ensuring the robot's operational viability over\nmultiple future steps and significantly improving its ability to navigate\nthrough environments with tight constraints on possible footholds. The\neffectiveness of this planning algorithm is demonstrated through simulations\nthat include a variety of complex stepping-stone configurations and external\nperturbations. These tests underscore the algorithm's improved performance for\nnavigating foothold-restricted environments, even with the presence of external\ndisturbances.\n","authors":["Zhaoyang Xiang","Victor Paredes","Ayonga Hereid"],"pdf_url":"https://arxiv.org/pdf/2403.17136v1.pdf","comment":"8 pages, 8 figures, submitted to CDC 2024, for associated simulation\n  video, see https://youtu.be/2jhikPlZmbE"},{"id":"http://arxiv.org/abs/2311.07462v2","updated":"2024-03-25T19:02:47Z","published":"2023-11-13T16:44:43Z","title":"Investigating Robustness in Cyber-Physical Systems:\n  Specification-Centric Analysis in the face of System Deviations","summary":"  The adoption of cyber-physical systems (CPS) is on the rise in complex\nphysical environments, encompassing domains such as autonomous vehicles, the\nInternet of Things (IoT), and smart cities. A critical attribute of CPS is\nrobustness, denoting its capacity to operate safely despite potential\ndisruptions and uncertainties in the operating environment. This paper proposes\na novel specification-based robustness, which characterizes the effectiveness\nof a controller in meeting a specified system requirement, articulated through\nSignal Temporal Logic (STL) while accounting for possible deviations in the\nsystem. This paper also proposes the robustness falsification problem based on\nthe definition, which involves identifying minor deviations capable of\nviolating the specified requirement. We present an innovative two-layer\nsimulation-based analysis framework designed to identify subtle robustness\nviolations. To assess our methodology, we devise a series of benchmark problems\nwherein system parameters can be adjusted to emulate various forms of\nuncertainties and disturbances. Initial evaluations indicate that our\nfalsification approach proficiently identifies robustness violations, providing\nvaluable insights for comparing robustness between conventional and\nreinforcement learning (RL)-based controllers\n","authors":["Changjian Zhang","Parv Kapoor","Romulo Meira-Goes","David Garlan","Eunsuk Kang","Akila Ganlath","Shatadal Mishra","Nejib Ammar"],"pdf_url":"https://arxiv.org/pdf/2311.07462v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2004.01041v8","updated":"2024-03-25T18:21:39Z","published":"2020-04-01T06:37:54Z","title":"On the Feedback Law in Stochastic Optimal Nonlinear Control","summary":"  We consider the problem of nonlinear stochastic optimal control. This problem\nis thought to be fundamentally intractable owing to Bellman's ``curse of\ndimensionality\". We present a result that shows that repeatedly solving an\nopen-loop deterministic problem from the current state with progressively\nshorter horizons, similar to Model Predictive Control (MPC), results in a\nfeedback policy that is $O(\\epsilon^4)$ near to the true global stochastic\noptimal policy, \\nxx{where $\\epsilon$ is a perturbation parameter modulating\nthe noise.} We show that the optimal deterministic feedback problem has a\nperturbation structure in that higher-order terms of the feedback law do not\naffect lower-order terms, and that this structure is lost in the optimal\nstochastic feedback problem. Consequently, solving the Stochastic Dynamic\nProgramming problem is highly susceptible to noise, even when tractable, and in\npractice, the MPC-type feedback law offers superior performance even for\nstochastic systems.\n","authors":["Mohamed Naveed Gul Mohamed","Suman Chakravorty","Raman Goyal","Ran Wang"],"pdf_url":"https://arxiv.org/pdf/2004.01041v8.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2002.10505,\n  arXiv:2002.09478"},{"id":"http://arxiv.org/abs/2403.14092v2","updated":"2024-03-25T17:49:07Z","published":"2024-03-21T02:59:56Z","title":"Carbon Footprint Reduction for Sustainable Data Centers in Real-Time","summary":"  As machine learning workloads significantly increase energy consumption,\nsustainable data centers with low carbon emissions are becoming a top priority\nfor governments and corporations worldwide. This requires a paradigm shift in\noptimizing power consumption in cooling and IT loads, shifting flexible loads\nbased on the availability of renewable energy in the power grid, and leveraging\nbattery storage from the uninterrupted power supply in data centers, using\ncollaborative agents. The complex association between these optimization\nstrategies and their dependencies on variable external factors like weather and\nthe power grid carbon intensity makes this a hard problem. Currently, a\nreal-time controller to optimize all these goals simultaneously in a dynamic\nreal-world setting is lacking. We propose a Data Center Carbon Footprint\nReduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that\noptimizes data centers for the multiple objectives of carbon footprint\nreduction, energy consumption, and energy cost. The results show that the\nDC-CFR MARL agents effectively resolved the complex interdependencies in\noptimizing cooling, load shifting, and energy storage in real-time for various\nlocations under real-world dynamic weather and grid carbon intensity\nconditions. DC-CFR significantly outperformed the industry standard ASHRAE\ncontroller with a considerable reduction in carbon emissions (14.5%), energy\nusage (14.4%), and energy cost (13.7%) when evaluated over one year across\nmultiple geographical regions.\n","authors":["Soumyendu Sarkar","Avisek Naug","Ricardo Luna","Antonio Guillen","Vineet Gundecha","Sahand Ghorbanpour","Sajad Mousavi","Dejan Markovikj","Ashwin Ramesh Babu"],"pdf_url":"https://arxiv.org/pdf/2403.14092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16982v1","updated":"2024-03-25T17:43:50Z","published":"2024-03-25T17:43:50Z","title":"State-Augmented Linear Games with Antagonistic Error for\n  High-Dimensional, Nonlinear Hamilton-Jacobi Reachability","summary":"  Hamilton-Jacobi Reachability (HJR) is a popular method for analyzing the\nliveness and safety of a dynamical system with bounded control and disturbance.\nThe corresponding HJ value function offers a robust controller and\ncharacterizes the reachable sets, but is traditionally solved with Dynamic\nProgramming (DP) and limited to systems of dimension less than six. Recently,\nthe space-parallelizeable, generalized Hopf formula has been shown to also\nsolve the HJ value with a nearly three-log increase in dimension limit, but is\nlimited to linear systems. To extend this potential, we demonstrate how\nstate-augmented (SA) spaces, which are well-known for their improved\nlinearization accuracy, may be used to solve tighter, conservative\napproximations of the value function with any linear model in this SA space.\nNamely, we show that with a representation of the true dynamics in the SA\nspace, a series of inequalities confirms that the value of a SA linear game\nwith antagonistic error is a conservative envelope of the true value function.\nIt follows that if the optimal controller for the HJ SA linear game with error\nmay succeed, it will also succeed in the true system. Unlike previous methods,\nthis result offers the ability to safely approximate reachable sets and their\ncorresponding controllers with the Hopf formula in a non-convex manner.\nFinally, we demonstrate this in the slow manifold system for clarity, and in\nthe controlled Van der Pol system with different lifting functions.\n","authors":["Will Sharpless","Yat Tin Chow","Sylvia Herbert"],"pdf_url":"https://arxiv.org/pdf/2403.16982v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16979v1","updated":"2024-03-25T17:41:52Z","published":"2024-03-25T17:41:52Z","title":"An Optimal Solution to Infinite Horizon Nonlinear Control Problems: Part\n  II","summary":"  This paper considers the infinite horizon optimal control problem for\nnonlinear systems. Under the condition of nonlinear controllability of the\nsystem to any terminal set containing the origin and forward invariance of the\nterminal set, we establish a regularized solution approach consisting of a\n``finite free final time\" optimal transfer problem to the terminal set which\nrenders the set globally asymptotically stable. Further, we show that the\napproximations converge to the optimal infinite horizon cost as the size of the\nterminal set decreases to zero. We also perform the analysis for the discounted\nproblem and show that the terminal set is asymptotically stable only for a\nsubset of the state space and not globally. The theory is empirically evaluated\non various nonholonomic robotic systems to show that the cost of our\napproximate problem converges and the transfer time into the terminal set is\ndependent on the initial state of the system, necessitating the free final time\nformulation.\n","authors":["Mohamed Naveed Gul Mohamed","Aayushman Sharma","Raman Goyal","Suman Chakravorty"],"pdf_url":"https://arxiv.org/pdf/2403.16979v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16956v1","updated":"2024-03-25T17:17:35Z","published":"2024-03-25T17:17:35Z","title":"Bayesian Methods for Trust in Collaborative Multi-Agent Autonomy","summary":"  Multi-agent, collaborative sensor fusion is a vital component of a\nmulti-national intelligence toolkit. In safety-critical and/or contested\nenvironments, adversaries may infiltrate and compromise a number of agents. We\nanalyze state of the art multi-target tracking algorithms under this\ncompromised agent threat model. We prove that the track existence probability\ntest (\"track score\") is significantly vulnerable to even small numbers of\nadversaries. To add security awareness, we design a trust estimation framework\nusing hierarchical Bayesian updating. Our framework builds beliefs of trust on\ntracks and agents by mapping sensor measurements to trust pseudomeasurements\n(PSMs) and incorporating prior trust beliefs in a Bayesian context. In case\nstudies, our trust estimation algorithm accurately estimates the\ntrustworthiness of tracks/agents, subject to observability limitations.\n","authors":["R. Spencer Hallyburton","Miroslav Pajic"],"pdf_url":"https://arxiv.org/pdf/2403.16956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16900v1","updated":"2024-03-25T16:11:29Z","published":"2024-03-25T16:11:29Z","title":"Spline Trajectory Tracking and Obstacle Avoidance for Mobile Agents via\n  Convex Optimization","summary":"  We propose an output feedback control-based motion planning technique for\nagents to enable them to converge to a specified polynomial trajectory while\nimposing a set of safety constraints on our controller to avoid collisions\nwithin the free configuration space (polygonal environment). To achieve this,\nwe 1) decompose our polygonal environment into different overlapping cells 2)\nwrite out our polynomial trajectories as the output of a reference dynamical\nsystem with given initial conditions 3) formulate convergence and safety\nconstraints as Linear Matrix Inequalities (LMIs) on our controller using\nControl Lyapunov Functions (CLFs) and Control Barrier Functions (CBFs) and 4)\nsolve a semi-definite programming (SDP) problem with convergence and safety\nconstraints imposed to synthesize a controller for each convex cell. Extensive\nsimulations are included to test our motion planning method under different\ninitial conditions and different reference trajectories. The synthesized\ncontroller is robust to changes in initial conditions and is always safe\nrelative to the boundaries of the polygonal environment.\n","authors":["Akua Dickson","Christos G. Cassandras","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.16900v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16899v1","updated":"2024-03-25T16:10:47Z","published":"2024-03-25T16:10:47Z","title":"State Space Models as Foundation Models: A Control Theoretic Overview","summary":"  In recent years, there has been a growing interest in integrating linear\nstate-space models (SSM) in deep neural network architectures of foundation\nmodels. This is exemplified by the recent success of Mamba, showing better\nperformance than the state-of-the-art Transformer architectures in language\ntasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a\nlatent space in order to learn a compressed representation of the data. The\nsame goal has been pursued by control theorists using SSMs to efficiently model\ndynamical systems. Therefore, SSMs can be naturally connected to deep sequence\nmodeling, offering the opportunity to create synergies between the\ncorresponding research areas. This paper is intended as a gentle introduction\nto SSM-based architectures for control theorists and summarizes the latest\nresearch developments. It provides a systematic review of the most successful\nSSM proposals and highlights their main features from a control theoretic\nperspective. Additionally, we present a comparative analysis of these models,\nevaluating their performance on a standardized benchmark designed for assessing\na model's efficiency at learning long sequences.\n","authors":["Carmen Amo Alonso","Jerome Sieber","Melanie N. Zeilinger"],"pdf_url":"https://arxiv.org/pdf/2403.16899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08318v2","updated":"2024-03-25T16:06:34Z","published":"2023-06-14T07:38:01Z","title":"Identification of Energy Management Configuration Concepts from a Set of\n  Pareto-optimal Solutions","summary":"  Implementing resource efficient energy management systems in facilities and\nbuildings becomes increasingly important in the transformation to a sustainable\nsociety. However, selecting a suitable configuration based on multiple,\ntypically conflicting objectives, such as cost, robustness with respect to\nuncertainty of grid operation, or renewable energy utilization, is a difficult\nmulti-criteria decision making problem. The recently developed concept\nidentification technique can facilitate a decision maker by sorting\nconfiguration options into semantically meaningful groups (concepts). In this\nprocess, the partitioning of the objectives and design parameters into\ndifferent sets (called description spaces) is a very important step. In this\nstudy we focus on utilizing the concept identification technique for finding\nrelevant and viable energy management configurations from a very large data set\nof Pareto-optimal solutions. The data set consists of 20000 realistic\nPareto-optimal building energy management configurations generated by a\nmany-objective evolutionary optimization of a high quality Digital Twin energy\nmanagement simulator. We analyze how the choice of description spaces, i.e.,\nthe partitioning of the objectives and parameters, impacts the type of\ninformation that can be extracted. We show that the decision maker can\nintroduce constraints and biases into that process to meet expectations and\npreferences. The iterative approach presented in this work allows for the\ngeneration of valuable insights into trade-offs between specific objectives,\nand constitutes a powerful and flexible tool to support the decision making\nprocess when designing large and complex energy management systems.\n","authors":["Felix Lanfermann","Qiqi Liu","Yaochu Jin","Sebastian Schmitt"],"pdf_url":"https://arxiv.org/pdf/2306.08318v2.pdf","comment":"18 pages, 8 figures, accepted at Energy Conversion and Management: X"},{"id":"http://arxiv.org/abs/2403.16859v1","updated":"2024-03-25T15:23:14Z","published":"2024-03-25T15:23:14Z","title":"A Semi-Lagrangian Approach for Time and Energy Path Planning\n  Optimization in Static Flow Fields","summary":"  Efficient path planning for autonomous mobile robots is a critical problem\nacross numerous domains, where optimizing both time and energy consumption is\nparamount. This paper introduces a novel methodology that considers the dynamic\ninfluence of an environmental flow field and considers geometric constraints,\nincluding obstacles and forbidden zones, enriching the complexity of the\nplanning problem. We formulate it as a multi-objective optimal control problem,\npropose a novel transformation called Harmonic Transformation, and apply a\nsemi-Lagrangian scheme to solve it. The set of Pareto efficient solutions is\nobtained considering two distinct approaches: a deterministic method and an\nevolutionary-based one, both of which are designed to make use of the proposed\nHarmonic Transformation. Through an extensive analysis of these approaches, we\ndemonstrate their efficacy in finding optimized paths.\n","authors":["Víctor C. da S. Campos","Armando A. Neto","Douglas G. Macharet"],"pdf_url":"https://arxiv.org/pdf/2403.16859v1.pdf","comment":"12 pages, initial paper submission; Preprint submitted to the IEEE\n  Transactions on Intelligent Transportation Systems"},{"id":"http://arxiv.org/abs/2403.16855v1","updated":"2024-03-25T15:18:23Z","published":"2024-03-25T15:18:23Z","title":"Semantic-Aware Remote Estimation of Multiple Markov Sources Under\n  Constraints","summary":"  This paper studies semantic-aware communication for remote estimation of\nmultiple Markov sources over a lossy and rate-constrained channel. Unlike most\nexisting studies that treat all source states equally, we exploit the semantics\nof information and consider that the remote actuator has different tolerances\nfor the estimation errors of different states. We aim to find an optimal\nscheduling policy that minimizes the long-term state-dependent costs of\nestimation errors under a transmission frequency constraint. We theoretically\nshow the structure of the optimal policy by leveraging the average-cost\nConstrained Markov Decision Process (CMDP) theory and the Lagrangian dynamic\nprogramming. By exploiting the optimal structural results, we develop a novel\npolicy search algorithm, termed intersection search plus relative value\niteration (Insec-RVI), that can find the optimal policy using only a few\niterations. To avoid the ``curse of dimensionality'' of MDPs, we propose an\nonline low-complexity drift-plus-penalty (DPP) scheduling algorithm based on\nthe Lyapunov optimization theorem. We also design an efficient average-cost\nQ-learning algorithm to estimate the optimal policy without knowing a priori\nthe channel and source statistics. Numerical results show that continuous\ntransmission is inefficient, and remarkably, our semantic-aware policies can\nattain the optimum by strategically utilizing fewer transmissions by exploiting\nthe timing of the important information.\n","authors":["Jiping Luo","Nikolaos Pappas"],"pdf_url":"https://arxiv.org/pdf/2403.16855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16836v1","updated":"2024-03-25T14:59:11Z","published":"2024-03-25T14:59:11Z","title":"Energy Efficiency Optimization Method of WDM Visible Light Communication\n  System for Indoor Broadcasting Networks","summary":"  This paper introduces a novel approach to optimize energy efficiency in\nwavelength division multiplexing (WDM) Visible Light Communication (VLC)\nsystems designed for indoor broadcasting networks. A physics-based LED model is\nintegrated into system energy efficiency optimization, enabling quantitative\nanalysis of the critical issue of VLC energy efficiency: the nonlinear\ninterplay between illumination and communication performance. The optimization\njointly incorporates constraints on communication quality of each channel, and\nillumination performance, standardized by the International Commission on\nIllumination (CIE). The formulated nonlinear optimization problem is solved by\nthe Sequential Quadratic Programming (SQP) algorithm in an experiment-based\nsimulation. An integrated Red-Green-Blue-Yellow Light Emitting Diode (RGBY-LED)\nis measured for model calibration and three different scenarios are simulated\nto evaluate the generality of the proposed method. Results demonstrate a double\nenhancement in performance and a high versatility in accommodating various\nscenarios. Furthermore, it highlights the importance of balancing communication\nand illumination imperatives in VLC systems, challenging conventional\nperceptions focused solely on minimizing power consumption.\n","authors":["Dayu Shi","Xun Zhang","Ziqi Liu","Xuanbang Chen","Jianghao Li","Xiaodong Liu","William Shieh"],"pdf_url":"https://arxiv.org/pdf/2403.16836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16823v1","updated":"2024-03-25T14:48:00Z","published":"2024-03-25T14:48:00Z","title":"Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A\n  User-Centric Learning Approach","summary":"  Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets)\nare an emerging indoor wireless communication paradigm, which combines the\nadvantages of the capacious optical spectra of LiFi and ubiquitous coverage of\nWiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource\nmanagement for such hybrid networks. The existing LB methods are mostly\nnetwork-centric, relying on a central unit to make a solution for the users all\nat once. Consequently, the solution needs to be updated for all users at the\nsame pace, regardless of their moving status. This would affect the network\nperformance in two aspects: i) when the update frequency is low, it would\ncompromise the connectivity of fast-moving users; ii) when the update frequency\nis high, it would cause unnecessary handovers as well as hefty feedback costs\nfor slow-moving users. Motivated by this, we investigate user-centric LB which\nallows users to update their solutions at different paces. The research is\ndeveloped upon our previous work on adaptive target-condition neural network\n(ATCNN), which can conduct LB for individual users in quasi-static channels. In\nthis paper, a deep neural network (DNN) model is designed to enable an adaptive\nupdate interval for each individual user. This new model is termed as\nmobility-supporting neural network (MSNN). Associating MSNN with ATCNN, a\nuser-centric LB framework named mobility-supporting ATCNN (MS-ATCNN) is\nproposed to handle resource management and mobility management simultaneously.\nResults show that at the same level of average update interval, MS-ATCNN can\nachieve a network throughput up to 215\\% higher than conventional LB methods\nsuch as game theory, especially for a larger number of users. In addition,\nMS-ATCNN costs an ultra low runtime at the level of 100s $\\mu$s, which is two\nto three orders of magnitude lower than game theory.\n","authors":["Han Ji","Xiping Wu"],"pdf_url":"https://arxiv.org/pdf/2403.16823v1.pdf","comment":"12 pages, 12 figures, 3 tables, submitted to IEEE TWC"},{"id":"http://arxiv.org/abs/2403.16821v1","updated":"2024-03-25T14:47:16Z","published":"2024-03-25T14:47:16Z","title":"Scheduling Power-Intensive Operations of Battery Energy Storage Systems\n  and Application to Hybrid Hydropower Plants","summary":"  Classical schedulers for Battery Energy Storage Systems (BESSs) use static\npower constraints, assuming that the BESS can provide the rated power at any\nState-Of-Charge (SOC) level and that these are representative of the underlying\nphysical constraints of the system (BESS voltage and current). Static power\nconstraints, however, can generate unfeasible schedules, especially in\npower-intensive applications, as demonstrated in this paper. This paper derives\na set of alternative constraints for the BESS power that are cognizant of the\nphysical limits of the BESS. It is shown that these constraints, developed by\nleveraging an equivalent circuit model of the BESS, can be formulated as linear\ninequalities in scheduling problems, thus leaving the properties of the\noriginal problem (i.e., linearity and convexity) unaltered. A comparative\nanalysis against traditional schedulers from the literature shows significant\nreductions in current violations and the generation of feasible schedules.\nThese findings underscore the crucial role of implementing more advanced power\nconstraints of BESSs in power-intensive applications, thereby enhancing the\nreliability of BESS scheduling strategies.\n","authors":["Stefano Cassano","Fabrizio Sossan"],"pdf_url":"https://arxiv.org/pdf/2403.16821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00670v2","updated":"2024-03-25T14:37:40Z","published":"2024-01-01T05:20:43Z","title":"Hybrid physics-informed metabolic cybergenetics: process rates augmented\n  with machine-learning surrogates informed by flux balance analysis","summary":"  Metabolic cybergenetics is a promising concept that interfaces gene\nexpression and cellular metabolism with computers for real-time dynamic\nmetabolic control. The focus is on control at the transcriptional level,\nserving as a means to modulate intracellular metabolic fluxes. Recent\nstrategies in this field have employed constraint-based dynamic models for\nprocess optimization, control, and estimation. However, this results in bilevel\ndynamic optimization problems, which pose considerable numerical and conceptual\nchallenges. In this study, we present an alternative hybrid physics-informed\ndynamic modeling framework for metabolic cybergenetics, aimed at simplifying\noptimization, control, and estimation tasks. By utilizing machine-learning\nsurrogates, our approach effectively embeds the physics of metabolic networks\ninto the process rates of structurally simpler macro-kinetic models coupled\nwith gene expression. These surrogates, informed by flux balance analysis, link\nthe domains of manipulatable intracellular enzymes to metabolic exchange\nfluxes. This ensures that critical knowledge captured by the system's metabolic\nnetwork is preserved. The resulting models can be integrated into metabolic\ncybergenetic schemes involving single-level optimizations. Additionally, the\nhybrid modeling approach maintains the number of system states at a necessary\nminimum, easing the burden of process monitoring and estimation. Our hybrid\nphysics-informed metabolic cybergenetic framework is demonstrated using a\ncomputational case study on the optogenetically-assisted production of\nitaconate by $\\textit{Escherichia coli}$.\n","authors":["Sebastián Espinel-Ríos","José L. Avalos"],"pdf_url":"https://arxiv.org/pdf/2401.00670v2.pdf","comment":"25 pages, 10 figures, journal submission (reviewed/accepted version)"},{"id":"http://arxiv.org/abs/2403.16809v1","updated":"2024-03-25T14:32:28Z","published":"2024-03-25T14:32:28Z","title":"An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems","summary":"  The increasing prevalence of Cyber-Physical Systems and the Internet of\nThings (CPS-IoT) applications and Foundation Models are enabling new\napplications that leverage real-time control of the environment. For example,\nreal-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems\ncan reduce its usage when not needed for the comfort of human occupants, hence\nreducing energy consumption. Collecting real-time feedback on human preferences\nin such human-in-the-loop (HITL) systems, however, is difficult in practice. We\npropose the use of large language models (LLMs) to deal with the challenges of\ndynamic environments and difficult-to-obtain data in CPS optimization. In this\npaper, we present a case study that employs LLM agents to mimic the behaviors\nand thermal preferences of various population groups (e.g. young families, the\nelderly) in a shopping mall. The aggregated thermal preferences are integrated\ninto an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which\nemploys the LLM as a dynamic simulation of the physical environment to learn\nhow to balance between energy savings and occupant comfort. Our results show\nthat LLMs are capable of simulating complex population movements within large\nopen spaces. Besides, AitL-RL demonstrates superior performance compared to the\npopular existing policy of set point control, suggesting that adaptive and\npersonalized decision-making is critical for efficient optimization in CPS-IoT\napplications. Through this case study, we demonstrate the potential of\nintegrating advanced Foundation Models like LLMs into CPS-IoT to enhance system\nadaptability and efficiency. The project's code can be found on our GitHub\nrepository.\n","authors":["Hanqing Yang","Marie Siew","Carlee Joe-Wong"],"pdf_url":"https://arxiv.org/pdf/2403.16809v1.pdf","comment":"Accepted at International Workshop on Foundation Models for\n  Cyber-Physical Systems & Internet of Things (FMSys) 2024, Co-located at\n  CPS-IoT Week 2024"},{"id":"http://arxiv.org/abs/2403.16797v1","updated":"2024-03-25T14:14:39Z","published":"2024-03-25T14:14:39Z","title":"Privacy Preservation by Intermittent Transmission in Cooperative LQG\n  Control Systems","summary":"  In this paper, we study a cooperative linear quadratic Gaussian (LQG) control\nsystem with a single user and a server. In this system, the user runs a process\nand employs the server to meet the needs of computation. However, the user\nregards its state trajectories as privacy. Therefore, we propose a privacy\nscheme, in which the user sends data to the server intermittently. By this\nscheme, the server's received information of the user is reduced, and\nconsequently the user's privacy is preserved. In this paper, we consider a\nperiodic transmission scheme. We analyze the performance of privacy\npreservation and LQG control of different transmission periods. Under the given\nthreshold of the control performance loss, a trade-off optimization problem is\nproposed. Finally, we give the solution to the optimization problem.\n","authors":["Wenhao Lin","Yuqing Ni","Wen Yang","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2403.16797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16786v1","updated":"2024-03-25T14:01:58Z","published":"2024-03-25T14:01:58Z","title":"DBPF: A Framework for Efficient and Robust Dynamic Bin-Picking","summary":"  Efficiency and reliability are critical in robotic bin-picking as they\ndirectly impact the productivity of automated industrial processes. However,\ntraditional approaches, demanding static objects and fixed collisions, lead to\ndeployment limitations, operational inefficiencies, and process unreliability.\nThis paper introduces a Dynamic Bin-Picking Framework (DBPF) that challenges\ntraditional static assumptions. The DBPF endows the robot with the reactivity\nto pick multiple moving arbitrary objects while avoiding dynamic obstacles,\nsuch as the moving bin. Combined with scene-level pose generation, the proposed\npose selection metric leverages the Tendency-Aware Manipulability Network\noptimizing suction pose determination. Heuristic task-specific designs like\nvelocity-matching, dynamic obstacle avoidance, and the resight policy, enhance\nthe picking success rate and reliability. Empirical experiments demonstrate the\nimportance of these components. Our method achieves an average 84% success\nrate, surpassing the 60% of the most comparable baseline, crucially, with zero\ncollisions. Further evaluations under diverse dynamic scenarios showcase DBPF's\nrobust performance in dynamic bin-picking. Results suggest that our framework\noffers a promising solution for efficient and reliable robotic bin-picking\nunder dynamics.\n","authors":["Yichuan Li","Junkai Zhao","Yixiao Li","Zheng Wu","Rui Cao","Masayoshi Tomizuka","Yunhui Liu"],"pdf_url":"https://arxiv.org/pdf/2403.16786v1.pdf","comment":"8 pages, 5 figures. This paper has been accepted by IEEE RA-L on\n  2024-03-24. See the supplementary video at youtube:\n  https://youtu.be/n5af2VsKhkg"},{"id":"http://arxiv.org/abs/2403.16767v1","updated":"2024-03-25T13:45:55Z","published":"2024-03-25T13:45:55Z","title":"Policy Gradient-based Model Free Optimal LQG Control with a\n  Probabilistic Risk Constraint","summary":"  In this paper, we investigate a model-free optimal control design that\nminimizes an infinite horizon average expected quadratic cost of states and\ncontrol actions subject to a probabilistic risk or chance constraint using\ninput-output data. In particular, we consider linear time-invariant systems and\ndesign an optimal controller within the class of linear state feedback control.\nThree different policy gradient (PG) based algorithms, natural policy gradient\n(NPG), Gauss-Newton policy gradient (GNPG), and deep deterministic policy\ngradient (DDPG), are developed, and compared with the optimal risk-neutral\nlinear-quadratic regulator (LQR) and a scenario-based model predictive control\n(MPC) technique via numerical simulations. The convergence properties and the\naccuracy of all the algorithms are compared numerically. We also establish\nanalytical convergence properties of the NPG and GNPG algorithms under the\nknown model scenario, while the proof of convergence for the unknown model\nscenario is part of our ongoing work.\n","authors":["Arunava Naha","Subhrakanti Dey"],"pdf_url":"https://arxiv.org/pdf/2403.16767v1.pdf","comment":"Submitted to IEEE CDC2024"},{"id":"http://arxiv.org/abs/2403.16755v1","updated":"2024-03-25T13:34:31Z","published":"2024-03-25T13:34:31Z","title":"A Blotto Game Approach to Ride-hailing Markets with Electric Vehicles","summary":"  When a centrally operated ride-hailing company considers to enter a market\nalready served by another company, it has to make a strategic decision about\nhow to distribute its fleet among different regions in the area. This decision\nwill be influenced by the market share the company can secure and the costs\nassociated with charging the vehicles in each region, all while competing with\nthe company already operating in the area. In this paper, we propose a Colonel\nBlotto-like game to model this decision-making. For the class of games that we\nstudy, we first prove the existence and uniqueness of a Nash Equilibrium.\nSubsequently, we provide its general characterization and present an algorithm\nfor computing the ones in the feasible set's interior. Additionally, for a\nsimplified scenario involving two regions, which would correspond to a city\narea with a downtown and a suburban region, we also provide a method to check\nfor the equilibria on the feasible set's boundary. Finally, through a numerical\ncase study, we illustrate the impact of charging prices on the position of the\nNash equilibrium.\n","authors":["Marko Maljkovic","Gustav Nilsson","Nikolas Geroliminis"],"pdf_url":"https://arxiv.org/pdf/2403.16755v1.pdf","comment":"Extended version of the paper accepted for presentation at the 2024\n  European Control Conference (ECC2024)"},{"id":"http://arxiv.org/abs/2403.16742v1","updated":"2024-03-25T13:13:39Z","published":"2024-03-25T13:13:39Z","title":"A Branch and Bound method for the exact parameter identification of the\n  PK/PD model for anesthetic drugs","summary":"  We address the problem of parameter identification for the standard\npharmacokinetic/pharmacodynamic (PK/PD) model for anesthetic drugs. Our main\ncontribution is the development of a global optimization method that guarantees\nfinding the parameters that minimize the one-step ahead prediction error. The\nmethod is based on a branch-and-bound algorithm, that can be applied to solve a\nmore general class of nonlinear regression problems. We present some simulation\nresults, based on a dataset of twelve patients. In these simulations, we are\nalways able to identify the exact parameters, despite the non-convexity of the\noverall identification problem.\n","authors":["Giulia Di Credico","Luca Consolini","Mattia Laurini","Marco Locatelli","Marco Milanesi","Michele Schiavo","Antonio Visioli"],"pdf_url":"https://arxiv.org/pdf/2403.16742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16740v1","updated":"2024-03-25T13:12:39Z","published":"2024-03-25T13:12:39Z","title":"Looking back and forward: A retrospective and future directions on\n  Software Engineering for systems-of-systems","summary":"  Modern systems are increasingly connected and more integrated with other\nexisting systems, giving rise to systems-of-systems (SoS). An SoS consists of a\nset of independent, heterogeneous systems that interact to provide new\nfunctionalities and accomplish global missions through emergent behavior\nmanifested at runtime. The distinctive characteristics of SoS, when contrasted\nto traditional systems, pose significant research challenges within Software\nEngineering. These challenges motivate the need for a paradigm shift and the\nexploration of novel approaches for designing, developing, deploying, and\nevolving these systems. The International Workshop on Software Engineering for\nSystems-of-Systems (SESoS) series started in 2013 to fill a gap in scientific\nforums addressing SoS from the Software Engineering perspective, becoming the\nfirst venue for this purpose. This article presents a study aimed at outlining\nthe evolution and future trajectory of Software Engineering for SoS based on\nthe examination of 57 papers spanning the 11 editions of the SESoS workshop\n(2013-2023). The study combined scoping review and scientometric analysis\nmethods to categorize and analyze the research contributions concerning\ntemporal and geographic distribution, topics of interest, research\nmethodologies employed, application domains, and research impact. Based on such\na comprehensive overview, this article discusses current and future directions\nin Software Engineering for SoS.\n","authors":["Everton Cavalcante","Thais Batista","Flavio Oquendo"],"pdf_url":"https://arxiv.org/pdf/2403.16740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16738v1","updated":"2024-03-25T13:09:53Z","published":"2024-03-25T13:09:53Z","title":"A data-based comparison of methods for reducing the peak volume flow\n  rate in a district heating system","summary":"  This work concerns reduction of the peak flow rate of a district heating\ngrid, a key system property which is bounded by pipe dimensions and pumping\ncapacity. The peak flow rate constrains the number of additional consumers that\ncan be connected, and may be a limiting factor in reducing supply temperatures\nwhen transitioning to the 4th generation of district heating. We evaluate a\nfull year of operational data from a subset of customer meters in a district\nheating system in Germany. We consider the peak flow rate reduction that could\nbe achieved with full a posteriori knowledge of this data. Three strategies for\nreducing the peak flow rate are investigated: A load shifting demand response\nstrategy, an upper limitation in substation return temperatures, and an upper\nlimitation on each substation's volume flow rate. We show that imposing up to\nto 18 % load flexibility for the customers provides an equal reduction in the\npeak system flow rate under the load shifting strategy. The limited return\ntemperature strategy is less efficient at curtailing the peak flow rate, but\nprovides an overall reduction of volume flow rates. Finally, the flow rate\nlimitation method can introduce new, higher flow rate peaks, reducing\nperformance.\n","authors":["Felix Agner","Ulrich Trabert","Anders Rantzer","Janybek Orozaliev"],"pdf_url":"https://arxiv.org/pdf/2403.16738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16727v1","updated":"2024-03-25T13:01:29Z","published":"2024-03-25T13:01:29Z","title":"SIS epidemics on open networks: A replacement-based approximation","summary":"  In this paper we analyze continuous-time SIS epidemics subject to arrivals\nand departures of agents, by using an approximated process based on\nreplacements. In defining the SIS dynamics in an open network, we consider a\nstochastic setting in which arrivals and departures take place according to\nPoisson processes with similar rates, and the new value of the infection\nprobability of an arriving agent is drawn from a continuous distribution. Since\nthe system size changes with time, we define an approximated process, in which\nreplacements take place instead of arrivals and departures, and we focus on the\nevolution of an aggregate measure of the level of infection. So long as the\nreproduction number is less than one, the long-term behavior of this function\nmeasures the impact of the changes of the set of agents in the epidemic. We\nderive upper bounds for the expectation and variance of this function and we\ninclude a numerical example to show that the approximated process is close to\nthe original SIS process.\n","authors":["Renato Vizuete","Paolo Frasca","Elena Panteley"],"pdf_url":"https://arxiv.org/pdf/2403.16727v1.pdf","comment":"7 pages, 2 figures, to appear in European Control Conference (ECC\n  2024)"},{"id":"http://arxiv.org/abs/2403.16711v1","updated":"2024-03-25T12:49:09Z","published":"2024-03-25T12:49:09Z","title":"Predictable Interval MDPs through Entropy Regularization","summary":"  Regularization of control policies using entropy can be instrumental in\nadjusting predictability of real-world systems. Applications benefiting from\nsuch approaches range from, e.g., cybersecurity, which aims at maximal\nunpredictability, to human-robot interaction, where predictable behavior is\nhighly desirable. In this paper, we consider entropy regularization for\ninterval Markov decision processes (IMDPs). IMDPs are uncertain MDPs, where\ntransition probabilities are only known to belong to intervals. Lately, IMDPs\nhave gained significant popularity in the context of abstracting stochastic\nsystems for control design. In this work, we address robust minimization of the\nlinear combination of entropy and a standard cumulative cost in IMDPs, thereby\nestablishing a trade-off between optimality and predictability. We show that\noptimal deterministic policies exist, and devise a value-iteration algorithm to\ncompute them. The algorithm solves a number of convex programs at each step.\nFinally, through an illustrative example we show the benefits of penalizing\nentropy in IMDPs.\n","authors":["Menno van Zutphen","Giannis Delimpaltadakis","Maurice Heemels","Duarte Antunes"],"pdf_url":"https://arxiv.org/pdf/2403.16711v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16696v1","updated":"2024-03-25T12:27:24Z","published":"2024-03-25T12:27:24Z","title":"BatDeck: Advancing Nano-drone Navigation with Low-power Ultrasound-based\n  Obstacle Avoidance","summary":"  Nano-drones, distinguished by their agility, minimal weight, and\ncost-effectiveness, are particularly well-suited for exploration in confined,\ncluttered and narrow spaces. Recognizing transparent, highly reflective or\nabsorbing materials, such as glass and metallic surfaces is challenging, as\nclassical sensors, such as cameras or laser rangers, often do not detect them.\nInspired by bats, which can fly at high speeds in complete darkness with the\nhelp of ultrasound, this paper introduces \\textit{BatDeck}, a pioneering\nsensor-deck employing a lightweight and low-power ultrasonic sensor for\nnano-drone autonomous navigation. This paper first provides insights about\nsensor characteristics, highlighting the influence of motor noise on the\nultrasound readings, then it introduces the results of extensive experimental\ntests for obstacle avoidance (OA) in a diverse environment. Results show that\n\\textit{BatDeck} allows exploration for a flight time of 8 minutes while\ncovering 136m on average before crash in a challenging environment with\ntransparent and reflective obstacles, proving the effectiveness of ultrasonic\nsensors for OA on nano-drones.\n","authors":["Hanna Müller","Victor Kartsch","Michele Magno","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2403.16696v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2311.08321v2","updated":"2024-03-25T11:48:53Z","published":"2023-11-14T17:05:49Z","title":"Peak Estimation of Rational Systems using Convex Optimization","summary":"  This paper presents algorithms that upper-bound the peak value of a state\nfunction along trajectories of a continuous-time system with rational dynamics.\nThe finite-dimensional but nonconvex peak estimation problem is cast as a\nconvex infinite-dimensional linear program in occupation measures. This\ninfinite-dimensional program is then truncated into finite-dimensions using the\nmoment-Sum-of-Squares (SOS) hierarchy of semidefinite programs. Prior work on\ntreating rational dynamics using the moment-SOS approach involves clearing\ndynamics to common denominators or adding lifting variables to handle\nreciprocal terms under new equality constraints. Our solution method uses a\nsum-of-rational method based on absolute continuity of measures. The Moment-SOS\ntruncations of our program possess lower computational complexity and\n(empirically demonstrated) higher accuracy of upper bounds on example systems\nas compared to prior approaches.\n","authors":["Jared Miller","Roy S. Smith"],"pdf_url":"https://arxiv.org/pdf/2311.08321v2.pdf","comment":"9 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2403.16652v1","updated":"2024-03-25T11:40:32Z","published":"2024-03-25T11:40:32Z","title":"Trajectory Planning of Robotic Manipulator in Dynamic Environment\n  Exploiting DRL","summary":"  This study is about the implementation of a reinforcement learning algorithm\nin the trajectory planning of manipulators. We have a 7-DOF robotic arm to pick\nand place the randomly placed block at a random target point in an unknown\nenvironment. The obstacle is randomly moving which creates a hurdle in picking\nthe object. The objective of the robot is to avoid the obstacle and pick the\nblock with constraints to a fixed timestamp. In this literature, we have\napplied a deep deterministic policy gradient (DDPG) algorithm and compared the\nmodel's efficiency with dense and sparse rewards.\n","authors":["Osama Ahmad","Zawar Hussain","Hammad Naeem"],"pdf_url":"https://arxiv.org/pdf/2403.16652v1.pdf","comment":"Accepted in ICIESTR-2024"},{"id":"http://arxiv.org/abs/2403.16634v1","updated":"2024-03-25T11:22:38Z","published":"2024-03-25T11:22:38Z","title":"Symbolic and User-friendly Geometric Algebra Routines (SUGAR) for\n  Computations in Matlab","summary":"  Geometric algebra (GA) is a mathematical tool for geometric computing,\nproviding a framework that allows a unified and compact approach to geometric\nrelations which in other mathematical systems are typically described using\ndifferent more complicated elements. This fact has led to an increasing\nadoption of GA in applied mathematics and engineering problems. However, the\nscarcity of symbolic implementations of GA and its inherent complexity,\nrequiring a specific mathematical background, make it challenging and less\nintuitive for engineers to work with. This prevents wider adoption among more\napplied professionals. To address this challenge, this paper introduces SUGAR\n(Symbolic and User-friendly Geometric Algebra Routines), an open-source toolbox\ndesigned for Matlab and licensed under the MIT License. SUGAR facilitates the\ntranslation of GA concepts into Matlab and provides a collection of\nuser-friendly functions tailored for GA computations, including support for\nsymbolic operations. It supports both numeric and symbolic computations in\nhigh-dimensional GAs. Specifically tailored for applied mathematics and\nengineering applications, SUGAR has been meticulously engineered to represent\ngeometric elements and transformations within two and three-dimensional\nprojective and conformal geometric algebras, aligning with established\ncomputational methodologies in the literature. Furthermore, SUGAR efficiently\nhandles functions of multivectors, such as exponential, logarithmic,\nsinusoidal, and cosine functions, enhancing its applicability across various\nengineering domains, including robotics, control systems, and power\nelectronics. Finally, this work includes four distinct validation examples,\ndemonstrating SUGAR's capabilities across the above-mentioned fields and its\npractical utility in addressing real-world applied mathematics and engineering\nproblems.\n","authors":["Manel Velasco","Isiah Zaplana","Arnau Dória-Cerezo","Pau Martí"],"pdf_url":"https://arxiv.org/pdf/2403.16634v1.pdf","comment":"33 pages, 6 figures, journal paper submitted to ACM TOMS"},{"id":"http://arxiv.org/abs/2403.16619v1","updated":"2024-03-25T10:51:30Z","published":"2024-03-25T10:51:30Z","title":"Guided Bayesian Optimization: Data-Efficient Controller Tuning with\n  Digital Twin","summary":"  This article presents the guided Bayesian optimization algorithm as an\nefficient data-driven method for iteratively tuning closed-loop controller\nparameters using an event-triggered digital twin of the system based on\navailable closed-loop data. We define a controller tuning framework independent\nof the controller or the plant structure. Our proposed methodology is\nmodel-free, making it suitable for nonlinear and unmodelled plants with\nmeasurement noise. The objective function consists of performance metrics\nmodeled by Gaussian processes. We utilize the available information in the\nclosed-loop system to identify and progressively maintain a digital twin that\nguides the optimizer, improving the data efficiency of our method. Switching\nthe digital twin on and off is triggered by data-driven criteria related to the\ndigital twin's uncertainty estimations in the BO tuning framework. Effectively,\nit replaces much of the exploration of the real system with exploration\nperformed on the digital twin. We analyze the properties of our method in\nsimulation and demonstrate its performance on two real closed-loop systems with\ndifferent plant and controller structures. The experimental results show that\nour method requires fewer experiments on the physical plant than Bayesian\noptimization to find the optimal controller parameters.\n","authors":["Mahdi Nobar","Jürg Keller","Alisa Rupenyan","Mohammad Khosravi","John Lygeros"],"pdf_url":"https://arxiv.org/pdf/2403.16619v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2310.00262v2","updated":"2024-03-25T10:45:50Z","published":"2023-09-30T05:26:42Z","title":"Robust Integral Consensus Control of Multi-Agent Networks Perturbed by\n  Matched and Unmatched Disturbances: The Case of Directed Graphs","summary":"  This work presents a new method to design consensus controllers for perturbed\ndouble integrator systems whose interconnection is described by a directed\ngraph containing a rooted spanning tree. We propose new robust controllers to\nsolve the consensus and synchronization problems when the systems are under the\neffects of matched and unmatched disturbances. In both problems, we present\nsimple continuous controllers, whose integral actions allow us to handle the\ndisturbances. A rigorous stability analysis based on Lyapunov's direct method\nfor unperturbed networked systems is presented. To assess the performance of\nour result, a representative simulation study is presented.\n","authors":["Jose Guadalupe Romero","David Navarro-Alarcon"],"pdf_url":"https://arxiv.org/pdf/2310.00262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16595v1","updated":"2024-03-25T10:16:51Z","published":"2024-03-25T10:16:51Z","title":"The Adaptive Workplace: Orchestrating Architectural Services around the\n  Wellbeing of Individual Occupants","summary":"  As the academic consortia members of the EU Horizon project SONATA\n(\"Situation-aware OrchestratioN of AdapTive Architecture\"), we respond to the\nworkshop call for \"Office Wellbeing by Design: Don't Stand for Anything Less\"\nby proposing the \"Adaptive Workplace\" concept. In essence, our vision aims to\nadapt a workplace to the ever-changing needs of individual occupants, instead\nof that occupants are expected to adapt to their workplace.\n","authors":["Andrew Vande Moere","Sara Arko","Alena Safrova Drasilova","Tomáš Ondráček","Ilaria Pigliautile","Benedetta Pioppi","Anna Laura Pisello","Jakub Prochazka","Paula Acuna Roncancio","Davide Schaumann","Marcel Schweiker","Binh Vinh Duc Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.16595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16593v1","updated":"2024-03-25T10:09:42Z","published":"2024-03-25T10:09:42Z","title":"Counter-example guided Imitation Learning of Feedback Controllers from\n  Temporal Logic Specifications","summary":"  We present a novel method for imitation learning for control requirements\nexpressed using Signal Temporal Logic (STL). More concretely we focus on the\nproblem of training a neural network to imitate a complex controller. The\nlearning process is guided by efficient data aggregation based on\ncounter-examples and a coverage measure. Moreover, we introduce a method to\nevaluate the performance of the learned controller via parameterization and\nparameter estimation of the STL requirements. We demonstrate our approach with\na flying robot case study.\n","authors":["Thao Dang","Alexandre Donzé","Inzemamul Haque","Nikolaos Kekatos","Indranil Saha"],"pdf_url":"https://arxiv.org/pdf/2403.16593v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16585v1","updated":"2024-03-25T09:53:59Z","published":"2024-03-25T09:53:59Z","title":"Sparsity-Constrained Linear Quadratic Regulation Problem: Greedy\n  Approach with Performance Guarantee","summary":"  We study a linear quadratic regulation problem with a constraint where the\ncontrol input can be nonzero only at a limited number of times. Given that this\nconstraint leads to a combinational optimization problem, we adopt a greedy\nmethod to find a suboptimal solution. To quantify the performance of the greedy\nalgorithm, we employ two metrics that reflect the submodularity level of the\nobjective function: The submodularity ratio and curvature. We first present an\nexplicit form of the optimal control input that is amenable to evaluating these\nmetrics. Subsequently, we establish bounds on the submodularity ratio and\ncurvature, which enable us to offer a practical performance guarantee for the\ngreedy algorithm. The effectiveness of our guarantee is further demonstrated\nthrough numerical simulations.\n","authors":["Shumpei Nishida","Kunihisa Okano"],"pdf_url":"https://arxiv.org/pdf/2403.16585v1.pdf","comment":"8 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16565v1","updated":"2024-03-25T09:32:29Z","published":"2024-03-25T09:32:29Z","title":"Decoupling parameter variation from noise: Biquadratic Lyapunov forms in\n  data-driven LPV control","summary":"  A promising step from linear towards nonlinear data-driven control is via the\ndesign of controllers for linear parameter-varying (LPV) systems, which are\nlinear systems whose parameters are varying along a measurable scheduling\nsignal. However, the interplay between uncertainty arising from corrupted data\nand the parameter-varying nature of these systems impacts the stability\nanalysis, and limits the generalization of well-understood data-driven methods\nfor linear time-invariant systems. In this work, we decouple this interplay\nusing a recently developed variant of the Fundamental Lemma for LPV systems and\nthe viewpoint of data-informativity, in combination with biquadratic Lyapunov\nforms. Together, these allow us to develop novel linear matrix inequality\nconditions for the existence of scheduling-dependent Lyapunov functions,\nincorporating the intrinsic nonlinearity. Appealingly, these results are stated\npurely in terms of the collected data and bounds on the noise, and they are\ncomputationally favorable to check.\n","authors":["Chris Verhoek","Jaap Eising","Florian Dörfler","Roland Tóth"],"pdf_url":"https://arxiv.org/pdf/2403.16565v1.pdf","comment":"Submitted for CDC 2024"},{"id":"http://arxiv.org/abs/2403.16555v1","updated":"2024-03-25T09:10:57Z","published":"2024-03-25T09:10:57Z","title":"Hybrid low-dimensional limiting state of charge estimator for multi-cell\n  lithium-ion batteries","summary":"  The state of charge (SOC) of lithium-ion batteries needs to be accurately\nestimated for safety and reliability purposes. For battery packs made of a\nlarge number of cells, it is not always feasible to design one SOC estimator\nper cell due to limited computational resources. Instead, only the minimum and\nthe maximum SOC need to be estimated. The challenge is that the cells having\nminimum and maximum SOC typically change over time. In this context, we present\na low-dimensional hybrid estimator of the minimum (maximum) SOC, whose\nconvergence is analytically guaranteed. We consider for this purpose a battery\nconsisting of cells interconnected in series, which we model by electric\nequivalent circuit models. We then present the hybrid estimator, which runs an\nobserver designed for a single cell at any time instant, selected by a\nswitching-like logic mechanism. We establish a practical exponential stability\nproperty for the estimation error on the minimum (maximum) SOC thereby\nguaranteeing the ability of the hybrid scheme to generate accurate estimates of\nthe minimum (maximum) SOC. The analysis relies on non-smooth hybrid Lyapunov\ntechniques. A numerical illustration is provided to showcase the relevance of\nthe proposed approach.\n","authors":["Mira Khalil","Romain Postoyan","Stéphane Raël","Dragan Nešić"],"pdf_url":"https://arxiv.org/pdf/2403.16555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16489v1","updated":"2024-03-25T07:17:44Z","published":"2024-03-25T07:17:44Z","title":"Spatially temporally distributed informative path planning for\n  multi-robot systems","summary":"  This paper investigates the problem of informative path planning for a mobile\nrobotic sensor network in spatially temporally distributed mapping. The robots\nare able to gather noisy measurements from an area of interest during their\nmovements to build a Gaussian Process (GP) model of a spatio-temporal field.\nThe model is then utilized to predict the spatio-temporal phenomenon at\ndifferent points of interest. To spatially and temporally navigate the group of\nrobots so that they can optimally acquire maximal information gains while their\nconnectivity is preserved, we propose a novel multistep prediction informative\npath planning optimization strategy employing our newly defined local cost\nfunctions. By using the dual decomposition method, it is feasible and practical\nto effectively solve the optimization problem in a distributed manner. The\nproposed method was validated through synthetic experiments utilizing\nreal-world data sets.\n","authors":["Binh Nguyen","Linh Nguyen","Truong X. Nghiem","Hung La","Jose Baca","Pablo Rangel","Miguel Cid Montoya","Thang Nguyen"],"pdf_url":"https://arxiv.org/pdf/2403.16489v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16470v1","updated":"2024-03-25T06:52:26Z","published":"2024-03-25T06:52:26Z","title":"Data-Driven Extrusion Force Control Tuning for 3D Printing","summary":"  The quality of 3D prints often varies due to different conditions inherent to\neach print, such as filament type, print speed, and nozzle size. Closed-loop\nprocess control methods improve the accuracy and repeatability of 3D prints.\nHowever, optimal tuning of controllers for given process parameters and design\ngeometry is often a challenge with manually tuned controllers resulting in\ninconsistent and suboptimal results. This work employs Bayesian optimization to\nidentify the optimal controller parameters. Additionally, we explore transfer\nlearning in the context of 3D printing by leveraging prior information from\npast trials. By integrating optimized extrusion force control and transfer\nlearning, we provide a novel framework for closed-loop 3D printing and propose\nan automated calibration routine that produces high-quality prints for a\ndesired combination of print settings, material, and shape.\n","authors":["Xavier Guidetti","Ankita Mukne","Marvin Rueppel","Yannick Nagel","Efe C. Balta","John Lygeros"],"pdf_url":"https://arxiv.org/pdf/2403.16470v1.pdf","comment":"Submitted to IEEE CASE 2024"},{"id":"http://arxiv.org/abs/2207.05370v2","updated":"2024-03-25T06:33:43Z","published":"2022-07-12T08:06:17Z","title":"Joint Ranging and Phase Offset Estimation for Multiple Drones using\n  ADS-B Signatures","summary":"  A new method for joint ranging and Phase Offset (PO) estimation of multiple\ndrones/aircrafts is proposed in this paper. The proposed method employs the\nsuperimposed uncoordinated Automatic Dependent Surveillance Broadcast (ADS-B)\npackets broadcasted by drones/aircrafts for joint range and PO estimation. It\njointly estimates range and PO prior to ADS-B packet decoding; thus, it can\nimprove air safety when packet decoding is infeasible due to packet collision.\nMoreover, it enables coherent detection of ADS-B packets, which can result in\nmore reliable multiple target tracking in aviation systems using cooperative\nsensors for detect and avoid (DAA). By minimizing the Kullback Leibler\nDivergence (KLD) statistical distance measure, we show that the received\ncomplex baseband signal coming from K uncoordinated drones corrupted by\nAdditive White Gaussian Noise (AWGN) at a single antenna receiver can be\napproximated by an independent and identically distributed Gaussian Mixture\n(GM) with 2 power K mixture components in the two dimensional (2D) plane. While\ndirect joint Maximum Likelihood Estimation (MLE) of range and PO from the\nderived GM Probability Density Function (PDF) leads to an intractable\nmaximization, our proposed method employs the Expectation Maximization (EM)\nalgorithm to estimate the modes of the 2D Gaussian mixture followed by a\nreordering estimation technique through combinatorial optimization to estimate\nrange and PO. An extension to a multiple antenna receiver is also investigated\nin this paper. While the proposed estimator can estimate the range of multiple\ndrones with a single receive antenna, a larger number of drones can be\nsupported with higher accuracy by the use of multiple antennas at the receiver.\nThe effectiveness of the proposed estimator is supported by simulation results.\nWe show that the proposed estimator can jointly estimate the range of three\ndrones accurately.\n","authors":["Mostafa Mohammadkarimi","Geert Leus","Raj Thilak Rajan"],"pdf_url":"https://arxiv.org/pdf/2207.05370v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02921v3","updated":"2024-03-25T05:54:29Z","published":"2023-08-05T16:54:59Z","title":"PowerSimulationsDynamics.jl -- An Open Source Modeling Package for\n  Modern Power Systems with Inverter-Based Resources","summary":"  In this paper we present the development of an open-source simulation\ntoolbox, PowerSimulationsDynamics.jl, to study the dynamic response of power\nsystems, focusing on the requirements to model systems with high penetrations\nof Inverter-Based Resources (IBRs). PowerSimulationsDynamics.jl is implemented\nin Julia and features a rich library of synchronous generator, inverter, and\nload models. In addition, it allows the study of quasi-static phasors and\nelectromagnetic dq models that use a dynamic network representation. Case\nstudies and validation exercises show that PowerSimulationsDynamics.jl results\nclosely match other commercial and open-source simulation tools.\n","authors":["Jose Daniel Lara","Rodrigo Henriquez-Auba","Matthew Bossart","Duncan S. Callaway","Clayton Barrows"],"pdf_url":"https://arxiv.org/pdf/2308.02921v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02306v3","updated":"2024-03-25T05:35:08Z","published":"2024-01-04T14:55:03Z","title":"Secure Control of Connected and Automated Vehicles Using Trust-Aware\n  Robust Event-Triggered Control Barrier Functions","summary":"  We address the security of a network of Connected and Automated Vehicles\n(CAVs) cooperating to safely navigate through a conflict area (e.g., traffic\nintersections, merging roadways, roundabouts). Previous studies have shown that\nsuch a network can be targeted by adversarial attacks causing traffic jams or\nsafety violations ending in collisions. We focus on attacks targeting the V2X\ncommunication network used to share vehicle data and consider as well\nuncertainties due to noise in sensor measurements and communication channels.\nTo combat these, motivated by recent work on the safe control of CAVs, we\npropose a trust-aware robust event-triggered decentralized control and\ncoordination framework that can provably guarantee safety. We maintain a trust\nmetric for each vehicle in the network computed based on their behavior and\nused to balance the tradeoff between conservativeness (when deeming every\nvehicle as untrustworthy) and guaranteed safety and security. It is important\nto highlight that our framework is invariant to the specific choice of the\ntrust framework. Based on this framework, we propose an attack detection and\nmitigation scheme which has twofold benefits: (i) the trust framework is immune\nto false positives, and (ii) it provably guarantees safety against false\npositive cases. We use extensive simulations (in SUMO and CARLA) to validate\nthe theoretical guarantees and demonstrate the efficacy of our proposed scheme\nto detect and mitigate adversarial attacks.\n","authors":["H M Sabbir Ahmad","Ehsan Sabouni","Akua Dickson","Wei Xiao","Christos G. Cassandras","Wenchao Li"],"pdf_url":"https://arxiv.org/pdf/2401.02306v3.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2305.16818"},{"id":"http://arxiv.org/abs/2311.08942v2","updated":"2024-03-25T04:41:13Z","published":"2023-11-15T13:27:17Z","title":"Thermally-Resilient Soft Gripper for On-Orbit Operations","summary":"  Research in soft manipulators has significantly enhanced object grasping\ncapabilities, thanks to their adaptability to various shapes and sizes.\nApplying this technology to on-orbit servicing, especially during the capture\nand containment stages of active space debris removal missions, might offer a\nsecure, adaptable, and cost-effective solution compared to the trend of\nincreasing the degrees of freedom and complexity of the manipulator (e.g.\nClearSpace, Astroscale). This work aims to conduct an experimental proof of\nconcept, for which challenges such as radiation, vacuum, and microgravity are\nsignificant, but the predominant issue is ensuring effective operation in the\nextreme temperature swings, where flexible materials may exhibit cryogenic\ncrystallization or drastic shifts in their elasticity. This work addresses this\nchallenge through an initial stage of analytical modeling of the thermal\ndynamics inside the manipulator in orbit; which is then used for the\ndevelopment of a first experimental prototype tested with liquid nitrogen and\nheat guns. The multi-layered design for Low Earth Orbit (LEO) leverages the\nproperties of TPU at low infill rates for lightweight inherent flexibility,\nsilicone rubber ensuring structural integrity, PTFE (Teflon) for unparalleled\nthermal stability, and aerogel for insulation. The tendon-actuated servo-driven\ngripper is tested in the laboratory by varying the shape and size of objects\nduring the grasping. The results, based on servomotor force metrics to assess\nthe flexible manipulator's adaptability and object capture efficiency across\ntemperature changes, affirm the concept's viability. Forces increase up to\n220$\\%$ in cryogenic conditions and decrease by no more than 50$\\%$ at high\ntemperatures.\n","authors":["Fernando Ruiz","Begona Arrue","Anibal Ollero"],"pdf_url":"https://arxiv.org/pdf/2311.08942v2.pdf","comment":"Submitted to IROS 2024"},{"id":"http://arxiv.org/abs/2403.16411v1","updated":"2024-03-25T04:11:52Z","published":"2024-03-25T04:11:52Z","title":"A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups","summary":"  Stochastic inference on Lie groups plays a key role in state estimation\nproblems, such as inertial navigation, visual inertial odometry, pose\nestimation in virtual reality, etc. A key problem is fusing independent\nconcentrated Gaussian distributions defined at different reference points on\nthe group. In this paper we approximate distributions at different points in\nthe group in a single set of exponential coordinates and then use classical\nGaussian fusion to obtain the fused posteriori in those coordinates. We\nconsider several approximations including the exact Jacobian of the change of\ncoordinate map, first and second order Taylor's expansions of the Jacobian, and\nparallel transport with and without curvature correction associated with the\nunderlying geometry of the Lie group. Preliminary results on SO(3) demonstrate\nthat a novel approximation using parallel transport with curvature correction\nachieves similar accuracy to the state-of-the-art optimisation based algorithms\nat a fraction of the computational cost.\n","authors":["Yixiao Ge","Pieter van Goor","Robert Mahony"],"pdf_url":"https://arxiv.org/pdf/2403.16411v1.pdf","comment":"Preprint for L-CSS"},{"id":"http://arxiv.org/abs/2403.16402v1","updated":"2024-03-25T03:38:23Z","published":"2024-03-25T03:38:23Z","title":"A Distributionally Robust Model Predictive Control for Static and\n  Dynamic Uncertainties in Smart Grids","summary":"  The integration of various power sources, including renewables and electric\nvehicles, into smart grids is expanding, introducing uncertainties that can\nresult in issues like voltage imbalances, load fluctuations, and power losses.\nThese challenges negatively impact the reliability and stability of online\nscheduling in smart grids. Existing research often addresses uncertainties\naffecting current states but overlooks those that impact future states, such as\nthe unpredictable charging patterns of electric vehicles. To distinguish\nbetween these, we term them static uncertainties and dynamic uncertainties,\nrespectively. This paper introduces WDR-MPC, a novel approach that stands for\ntwo-stage Wasserstein-based Distributionally Robust (WDR) optimization within a\nModel Predictive Control (MPC) framework, aimed at effectively managing both\ntypes of uncertainties in smart grids. The dynamic uncertainties are first\nreformulated into ambiguity tubes and then the distributionally robust bounds\nof both dynamic and static uncertainties can be established using WDR\noptimization. By employing ambiguity tubes and WDR optimization, the stochastic\nMPC system is converted into a nominal one. Moreover, we develop a convex\nreformulation method to speed up WDR computation during the two-stage\noptimization. The distinctive contribution of this paper lies in its holistic\napproach to both static and dynamic uncertainties in smart grids. Comprehensive\nexperiment results on IEEE 38-bus and 94-bus systems reveal the method's\nsuperior performance and the potential to enhance grid stability and\nreliability.\n","authors":["Qi Li","Ye Shi","Yuning Jiang","Yuanming Shi","Haoyu Wang","H. Vincent Poor"],"pdf_url":"https://arxiv.org/pdf/2403.16402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16391v1","updated":"2024-03-25T03:13:56Z","published":"2024-03-25T03:13:56Z","title":"Physics-informed RL for Maximal Safety Probability Estimation","summary":"  Accurate risk quantification and reachability analysis are crucial for safe\ncontrol and learning, but sampling from rare events, risky states, or long-term\ntrajectories can be prohibitively costly. Motivated by this, we study how to\nestimate the long-term safety probability of maximally safe actions without\nsufficient coverage of samples from risky states and long-term trajectories.\nThe use of maximal safety probability in control and learning is expected to\navoid conservative behaviors due to over-approximation of risk. Here, we first\nshow that long-term safety probability, which is multiplicative in time, can be\nconverted into additive costs and be solved using standard reinforcement\nlearning methods. We then derive this probability as solutions of partial\ndifferential equations (PDEs) and propose Physics-Informed Reinforcement\nLearning (PIRL) algorithm. The proposed method can learn using sparse rewards\nbecause the physics constraints help propagate risk information through\nneighbors. This suggests that, for the purpose of extracting more information\nfor efficient learning, physics constraints can serve as an alternative to\nreward shaping. The proposed method can also estimate long-term risk using\nshort-term samples and deduce the risk of unsampled states. This feature is in\nstark contrast with the unconstrained deep RL that demands sufficient data\ncoverage. These merits of the proposed method are demonstrated in numerical\nsimulation.\n","authors":["Hikaru Hoshino","Yorie Nakahira"],"pdf_url":"https://arxiv.org/pdf/2403.16391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16377v1","updated":"2024-03-25T02:47:29Z","published":"2024-03-25T02:47:29Z","title":"Real-time Adaptation for Condition Monitoring Signal Prediction using\n  Label-aware Neural Processes","summary":"  Building a predictive model that rapidly adapts to real-time condition\nmonitoring (CM) signals is critical for engineering systems/units.\nUnfortunately, many current methods suffer from a trade-off between\nrepresentation power and agility in online settings. For instance, parametric\nmethods that assume an underlying functional form for CM signals facilitate\nefficient online prediction updates. However, this simplification leads to\nvulnerability to model specifications and an inability to capture complex\nsignals. On the other hand, approaches based on over-parameterized or\nnon-parametric models can excel at explaining complex nonlinear signals, but\nreal-time updates for such models pose a challenging task. In this paper, we\npropose a neural process-based approach that addresses this trade-off. It\nencodes available observations within a CM signal into a representation space\nand then reconstructs the signal's history and evolution for prediction. Once\ntrained, the model can encode an arbitrary number of observations without\nrequiring retraining, enabling on-the-spot real-time predictions along with\nquantified uncertainty and can be readily updated as more online data is\ngathered. Furthermore, our model is designed to incorporate qualitative\ninformation (i.e., labels) from individual units. This integration not only\nenhances individualized predictions for each unit but also enables joint\ninference for both signals and their associated labels. Numerical studies on\nboth synthetic and real-world data in reliability engineering highlight the\nadvantageous features of our model in real-time adaptation, enhanced signal\nprediction with uncertainty quantification, and joint prediction for labels and\nsignals.\n","authors":["Seokhyun Chung","Raed Al Kontar"],"pdf_url":"https://arxiv.org/pdf/2403.16377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02609v3","updated":"2024-03-25T01:50:40Z","published":"2023-09-05T22:53:37Z","title":"Directionality-Aware Mixture Model Parallel Sampling for Efficient\n  Linear Parameter Varying Dynamical System Learning","summary":"  The Linear Parameter Varying Dynamical System (LPV-DS) is an effective\napproach that learns stable, time-invariant motion policies using statistical\nmodeling and semi-definite optimization to encode complex motions for reactive\nrobot control. Despite its strengths, the LPV-DS learning approach faces\nchallenges in achieving a high model accuracy without compromising the\ncomputational efficiency. To address this, we introduce the\nDirectionality-Aware Mixture Model (DAMM), a novel statistical model that\napplies the Riemannian metric on the n-sphere $\\mathbb{S}^n$ to efficiently\nblend non-Euclidean directional data with $\\mathbb{R}^m$ Euclidean states.\nAdditionally, we develop a hybrid Markov chain Monte Carlo technique that\ncombines Gibbs Sampling with Split/Merge Proposal, allowing for parallel\ncomputation to drastically speed up inference. Our extensive empirical tests\ndemonstrate that LPV-DS integrated with DAMM achieves higher reproduction\naccuracy, better model efficiency, and near real-time/online learning compared\nto standard estimation methods on various datasets. Lastly, we demonstrate its\nsuitability for incrementally learning multi-behavior policies in real-world\nrobot experiments.\n","authors":["Sunan Sun","Haihui Gao","Tianyu Li","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2309.02609v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00237v2","updated":"2024-03-25T01:14:56Z","published":"2024-03-01T02:37:11Z","title":"Stable Reduced-Rank VAR Identification","summary":"  The vector autoregression (VAR) has been widely used in system\nidentification, econometrics, natural science, and many other areas. However,\nwhen the state dimension becomes large the parameter dimension explodes. So\nrank reduced modelling is attractive and is well developed. But a fundamental\nrequirement in almost all applications is stability of the fitted model. And\nthis has not been addressed in the rank reduced case. Here, we develop, for the\nfirst time, a closed-form formula for an estimator of a rank reduced transition\nmatrix which is guaranteed to be stable. We show that our estimator is\nconsistent and asymptotically statistically efficient and illustrate it in\ncomparative simulations.\n","authors":["Xinhui Rong","Victor Solo"],"pdf_url":"https://arxiv.org/pdf/2403.00237v2.pdf","comment":"16 pages, 6 figures"}],"Computational Engineering":[{"id":"http://arxiv.org/abs/2403.17131v1","updated":"2024-03-25T19:16:11Z","published":"2024-03-25T19:16:11Z","title":"Deep learning-based predictive modelling of transonic flow over an\n  aerofoil","summary":"  Effectively predicting transonic unsteady flow over an aerofoil poses\ninherent challenges. In this study, we harness the power of deep neural network\n(DNN) models using the attention U-Net architecture. Through efficient training\nof these models, we achieve the capability to capture the complexities of\ntransonic and unsteady flow dynamics at high resolution, even when faced with\npreviously unseen conditions. We demonstrate that by leveraging the\ndifferentiability inherent in neural network representations, our approach\nprovides a framework for assessing fundamental physical properties via global\ninstability analysis. This integration bridges deep neural network models and\ntraditional modal analysis, offering valuable insights into transonic flow\ndynamics and enhancing the interpretability of neural network models in\nflowfield diagnostics.\n","authors":["Li-Wei Chen","Nils Thuerey"],"pdf_url":"https://arxiv.org/pdf/2403.17131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16507v1","updated":"2024-03-25T07:44:17Z","published":"2024-03-25T07:44:17Z","title":"An experimental evaluation of choices of SSA forecasting parameters","summary":"  Six time series related to atmospheric phenomena are used as inputs for\nexperiments offorecasting with singular spectrum analysis (SSA). Existing\nmethods for SSA parametersselection are compared throughout their forecasting\naccuracy relatively to an optimal aposteriori selection and to a naive\nforecasting methods. The comparison shows that awidespread practice of\nselecting longer windows leads often to poorer predictions. It alsoconfirms\nthat the choices of the window length and of the grouping are essential.\nWiththe mean error of rainfall forecasting below 1.5%, SSA appears as a viable\nalternative forhorizons beyond two weeks.\n","authors":["Teodor Knapik","Adolphe Ratiarison","Hasina Razafindralambo"],"pdf_url":"https://arxiv.org/pdf/2403.16507v1.pdf","comment":"Revue Africaine de Recherche en Informatique et Math{\\'e}matiques\n  Appliqu{\\'e}es, In press, 40"}]},"2024-03-24T00:00:00Z":{"Systems and Control":[{"id":"http://arxiv.org/abs/2403.16313v1","updated":"2024-03-24T22:35:54Z","published":"2024-03-24T22:35:54Z","title":"Datasets of Great Britain Primary Substations Integrated with Household\n  Heating Information","summary":"  The growing demand for electrified heating, electrified transportation, and\npower-intensive data centres challenge distribution networks. If\nelectrification projects are carried out without considering electrical\ndistribution infrastructure, there could be unexpected blackouts and financial\nlosses. Datasets containing real-world distribution network information are\nrequired to address this. On the other hand, social data, such as household\nheating composition, are closely coupled with people's lives. Studying the\ncoupling between the energy system and society is important in promoting social\nwelfare. To fill these gaps, this paper introduces two datasets. The first is\nthe main dataset for the distribution networks in Great Britain (GB),\ncollecting information on firm capacity, peak demands, locations, and parent\ntransmission nodes (the Grid Supply Point, namely GSP) for all primary\nsubstations (PSs). PSs are a crucial part of the UK distribution network and\nare at the lowest voltage level (11 kV) with publicly available data for most\nUK Distribution Network Operators (DNOs). Substation firm capacity and peak\ndemand facilitate an understanding of the remaining room of the existing\nnetwork. The parent GSP information helps link the dataset of distribution\nnetworks to datasets of transmission networks. The second dataset extends the\nmain network dataset, linking each PS to information about the number of\nhouseholds that use different types of central heating recorded in census data.\nThe derivation of the second dataset is based on locations of PSs collected in\nthe main dataset with appropriate assumptions. The derivation process may also\nbe replicated to integrate other social datasets.\n","authors":["Yihong Zhou","Chaimaa Essayeh","Thomas Morstyn"],"pdf_url":"https://arxiv.org/pdf/2403.16313v1.pdf","comment":"Submitted to the journal \"Data in Brief\""},{"id":"http://arxiv.org/abs/2403.16307v1","updated":"2024-03-24T22:12:40Z","published":"2024-03-24T22:12:40Z","title":"ANN-Based Adaptive NMPC for Uranium Extraction-Scrubbing Operation in\n  Spent Nuclear Fuel Treatment Process","summary":"  This paper addresses the particularities in optimal control of the uranium\nextraction-scrubbing operation in the PUREX process. The control problem\nrequires optimally stabilizing the system at a desired solvent saturation\nlevel, guaranteeing constraints, disturbance rejection, and adapting to set\npoint variations. A qualified simulator named PAREX was developed by the French\nAlternative Energies and Atomic Energy Commission (CEA) to simulate\nliquid-liquid extraction operations in the PUREX process. However, since the\nmathematical model is complex and is described by a system of nonlinear, stiff,\nhigh-dimensional differential-algebraic equations (DAE), applying optimal\ncontrol methods will lead to a large-scale nonlinear programming problem with a\nhuge computational burden. The solution we propose in this work is to train a\nneural network to predict the process outputs using the measurement history.\nThis neural network architecture, which employs the long short-term memory\n(LSTM), linear regression and logistic regression networks, allows reducing the\nnumber of state variables, thus reducing the complexity of the optimization\nproblems in the control scheme. Furthermore, nonlinear model predictive control\n(NMPC) and moving horizon estimation (MHE) problems are developed and solved\nusing the PSO (Particle Swarm Optimization) algorithm. Simulation results show\nthat the proposed adaptive optimal control scheme satisfies the requirements of\nthe control problem and provides promise for experimental testing.\n","authors":["Duc-Tri Vo","Ionela Prodan","Laurent Lefèvre","Vincent Vanel","Sylvain Costenoble","Binh Dinh"],"pdf_url":"https://arxiv.org/pdf/2403.16307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16306v1","updated":"2024-03-24T21:55:46Z","published":"2024-03-24T21:55:46Z","title":"Control-Coherent Koopman Modeling: A Physical Modeling Approach","summary":"  The modeling of nonlinear dynamics based on Koopman operator theory, which is\noriginally applicable only to autonomous systems with no control, is extended\nto non-autonomous control system without approximation to input matrix B.\nPrevailing methods using a least square estimate of the B matrix may result in\nan erroneous input matrix, misinforming the controller about the structure of\nthe input matrix in a lifted space. Here, a new method for constructing a\nKoopman model that comprises the exact input matrix B is presented. A set of\nstate variables are introduced so that the control inputs are linearly involved\nin the dynamics of actuators. With these variables, a lifted linear model with\nthe exact control matrix, called a Control-Coherent Koopman Model, is\nconstructed by superposing control input terms, which are linear in local\nactuator dynamics, to the Koopman operator of the associated autonomous\nnonlinear system. The proposed method is applied to multi degree-of-freedom\nrobotic arms and multi-cable manipulation systems. Model Predictive Control is\napplied to the former. It is demonstrated that the prevailing Dynamic Mode\nDecomposition with Control (DMDc) using an approximate control matrix B does\nnot provide a satisfactory result, while the Control-Coherent Koopman Model\nperforms well with the correct B matrix.\n","authors":["H. Harry Asada","Jose A. Solano-Castellanos"],"pdf_url":"https://arxiv.org/pdf/2403.16306v1.pdf","comment":null}],"Computational Engineering":[{"id":"http://arxiv.org/abs/2403.16055v1","updated":"2024-03-24T07:47:00Z","published":"2024-03-24T07:47:00Z","title":"Modal-adaptive Knowledge-enhanced Graph-based Financial Prediction from\n  Monetary Policy Conference Calls with LLM","summary":"  Financial prediction from Monetary Policy Conference (MPC) calls is a new yet\nchallenging task, which targets at predicting the price movement and volatility\nfor specific financial assets by analyzing multimodal information including\ntext, video, and audio. Although the existing work has achieved great success\nusing cross-modal transformer blocks, it overlooks the potential external\nfinancial knowledge, the varying contributions of different modalities to\nfinancial prediction, as well as the innate relations among different financial\nassets. To tackle these limitations, we propose a novel Modal-Adaptive\nkNowledge-enhAnced Graph-basEd financial pRediction scheme, named MANAGER.\nSpecifically, MANAGER resorts to FinDKG to obtain the external related\nknowledge for the input text. Meanwhile, MANAGER adopts BEiT-3 and Hidden-unit\nBERT (HuBERT) to extract the video and audio features, respectively.\nThereafter, MANAGER introduces a novel knowledge-enhanced cross-modal graph\nthat fully characterizes the semantic relations among text, external knowledge,\nvideo and audio, to adaptively utilize the information in different modalities,\nwith ChatGLM2 as the backbone. Extensive experiments on a publicly available\ndataset Monopoly verify the superiority of our model over cutting-edge methods.\n","authors":["Kun Ouyang","Yi Liu","Shicheng Li","Ruihan Bao","Keiko Harimoto","Xu Sun"],"pdf_url":"https://arxiv.org/pdf/2403.16055v1.pdf","comment":"Accepted by LREC Coling 2024 -FinNLP"},{"id":"http://arxiv.org/abs/2403.15989v1","updated":"2024-03-24T02:54:46Z","published":"2024-03-24T02:54:46Z","title":"Knowledge-guided Machine Learning: Current Trends and Future Prospects","summary":"  This paper presents an overview of scientific modeling and discusses the\ncomplementary strengths and weaknesses of ML methods for scientific modeling in\ncomparison to process-based models. It also provides an introduction to the\ncurrent state of research in the emerging field of scientific knowledge-guided\nmachine learning (KGML) that aims to use both scientific knowledge and data in\nML frameworks to achieve better generalizability, scientific consistency, and\nexplainability of results. We discuss different facets of KGML research in\nterms of the type of scientific knowledge used, the form of knowledge-ML\nintegration explored, and the method for incorporating scientific knowledge in\nML. We also discuss some of the common categories of use cases in environmental\nsciences where KGML methods are being developed, using illustrative examples in\neach category.\n","authors":["Anuj Karpatne","Xiaowei Jia","Vipin Kumar"],"pdf_url":"https://arxiv.org/pdf/2403.15989v1.pdf","comment":null}]},"2024-03-23T00:00:00Z":{"Computational Engineering":[{"id":"http://arxiv.org/abs/2310.01968v2","updated":"2024-03-23T06:00:31Z","published":"2023-10-03T11:21:34Z","title":"PyHexTop: a compact Python code for topology optimization using\n  hexagonal elements","summary":"  Python serves as an open-source and cost-effective alternative to the MATLAB\nprogramming language. This paper introduces a concise topology optimization\nPython code, named ``\\texttt{PyHexTop},\" primarily intended for educational\npurposes. Code employs hexagonal elements to parameterize design domains as\nsuch elements provide checkerboard-free optimized design naturally.\n\\texttt{PyHexTop} is developed based on the ``\\texttt{HoneyTop90}\" MATLAB\ncode~\\cite{kumar2023honeytop90} and uses the \\texttt{NumPy} and \\texttt{SciPy}\nlibraries. Code is straightforward and easily comprehensible, proving a helpful\ntool that can help people new in the topology optimization field to learn and\nexplore. \\texttt{PyHexTop} is specifically tailored to address compliance\nminimization with specified volume constraints. The paper provides a detailed\nexplanation of the code for solving the Messerschmitt-Bolkow-Blohm beam and\nextensions to solve problems different problems. The code is publicly shared\nat: \\url{https://github.com/PrabhatIn/PyHexTop.}\n","authors":["Aditi Agarwal","Anupam Saxena","Prabhat Kumar"],"pdf_url":"https://arxiv.org/pdf/2310.01968v2.pdf","comment":"Accepted in NCMDAO 2023 conference"}]},"2024-03-22T00:00:00Z":{"Computational Engineering":[{"id":"http://arxiv.org/abs/2309.06617v2","updated":"2024-03-22T21:57:18Z","published":"2023-09-12T22:20:15Z","title":"Accelerating model evaluations in uncertainty propagation on tensor\n  grids using computational graph transformations","summary":"  Methods such as non-intrusive polynomial chaos (NIPC), and stochastic\ncollocation are frequently used for uncertainty propagation problems.\nParticularly for low-dimensional problems, these methods often use a\ntensor-product grid for sampling the space of uncertain inputs. A limitation of\nthis approach is that it encounters a significant challenge: the number of\nsample points grows exponentially with the increase of uncertain inputs.\nCurrent strategies to mitigate computational costs abandon the tensor structure\nof sampling points, with the aim of reducing their overall count.\nContrastingly, our investigation reveals that preserving the tensor structure\nof sample points can offer distinct advantages in specific scenarios. Notably,\nby manipulating the computational graph of the targeted model, it is feasible\nto avoid redundant evaluations at the operation level to significantly reduce\nthe model evaluation cost on tensor-grid inputs. This paper presents a\npioneering method: Accelerated Model Evaluations on Tensor grids using\nComputational graph transformations (AMTC). The core premise of AMTC lies in\nthe strategic modification of the computational graph of the target model to\nalgorithmically remove the repeated evaluations on the operation level. We\nimplemented the AMTC method within the compiler of a new modeling language\ncalled the Computational System Design Language (CSDL). We demonstrate the\neffectiveness of AMTC by using it with the full-grid NIPC method to solve four\nlow-dimensional UQ problems involving an analytical piston model, a\nmultidisciplinary unmanned aerial vehicle design model, a multi-point air taxi\nmission analysis model, and a single-disciplinary rotor model, respectively.\nFor three of the four test problems, AMTC reduces the model evaluation cost by\nbetween 50% and 90%, making the full-grid NIPC the most efficacious method to\nuse among the UQ methods implemented.\n","authors":["Bingran Wang","Mark Sperry","Victor E. Gandarillas","John T. Hwang"],"pdf_url":"https://arxiv.org/pdf/2309.06617v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15622v1","updated":"2024-03-22T21:17:57Z","published":"2024-03-22T21:17:57Z","title":"A gradient-enhanced univariate dimension reduction method for\n  uncertainty propagation","summary":"  The univariate dimension reduction (UDR) method stands as a way to estimate\nthe statistical moments of the output that is effective in a large class of\nuncertainty quantification (UQ) problems. UDR's fundamental strategy is to\napproximate the original function using univariate functions so that the UQ\ncost only scales linearly with the dimension of the problem. Nonetheless, UDR's\neffectiveness can diminish when uncertain inputs have high variance,\nparticularly when assessing the output's second and higher-order statistical\nmoments. This paper proposes a new method, gradient-enhanced univariate\ndimension reduction (GUDR), that enhances the accuracy of UDR by incorporating\nunivariate gradient function terms into the UDR approximation function.\nTheoretical results indicate that the GUDR approximation is expected to be one\norder more accurate than UDR in approximating the original function, and it is\nexpected to generate more accurate results in computing the output's second and\nhigher-order statistical moments. Our proposed method uses a computational\ngraph transformation strategy to efficiently evaluate the GUDR approximation\nfunction on tensor-grid quadrature inputs, and use the tensor-grid input-output\ndata to compute the statistical moments of the output. With an efficient\nautomatic differentiation method to compute the gradients, our method preserves\nUDR's linear scaling of computation time with problem dimension. Numerical\nresults show that the GUDR is more accurate than UDR in estimating the standard\ndeviation of the output and has a performance comparable to the method of\nmoments using a third-order Taylor series expansion.\n","authors":["Bingran Wang","Nicholas C. Orndorff","Mark Sperry","John T. Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.15622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15614v1","updated":"2024-03-22T20:57:06Z","published":"2024-03-22T20:57:06Z","title":"Graph-accelerated non-intrusive polynomial chaos expansion using\n  partially tensor-structured quadrature rules","summary":"  Recently, the graph-accelerated non-intrusive polynomial chaos (NIPC) method\nhas been proposed for solving uncertainty quantification (UQ) problems. This\nmethod leverages the full-grid integration-based NIPC method to address UQ\nproblems while employing the computational graph transformation approach, AMTC,\nto accelerate the tensor-grid evaluations. This method exhibits remarkable\nefficacy on a broad range of low-dimensional (three dimensions or less) UQ\nproblems featuring multidisciplinary models. However, it often does not scale\nwell with problem dimensions due to the exponential increase in the number of\nquadrature points when using the full-grid quadrature rule. To expand the\napplicability of this method to a broader range of UQ problems, this paper\nintroduces a new framework for generating a tailored, partially\ntensor-structured quadrature rule to use with the graph-accelerated NIPC\nmethod. This quadrature rule, generated through the designed quadrature\napproach, possesses a tensor structure that is tailored for the computational\nmodel. The selection of the tensor structure is guided by an analysis of the\ncomputational graph, ensuring that the quadrature rule effectively capitalizes\non the sparsity within the computational graph when paired with the AMTC\nmethod. This method has been tested on one 4D and one 6D UQ problem, both\noriginating from aircraft design scenarios and featuring multidisciplinary\nmodels. Numerical results show that, when using with graph-accelerated NIPC\nmethod, our approach generates a partially tensor-structured quadrature rule\nthat outperforms the full-grid Gauss quadrature and the designed quadrature\nmethods (more than 40% reduction in computational costs) in both of the test\nproblems.\n","authors":["Bingran Wang","Nicholas C. Orndorff","John T. Hwang"],"pdf_url":"https://arxiv.org/pdf/2403.15614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15596v1","updated":"2024-03-22T19:56:48Z","published":"2024-03-22T19:56:48Z","title":"A Linear Time-Delay Scheme to Propagate Reduced Electron Density\n  Matrices","summary":"  For any linear system where the unreduced dynamics are governed by unitary\npropagators, we derive a closed, time-delayed, linear system for a\nreduced-dimensional quantity of interest. We apply this method to understand\nthe memory-dependence of reduced $1$-electron density matrices in\ntime-dependent configuration interaction (TDCI), a scheme to solve for the\ncorrelated dynamics of electrons in molecules. Though time-dependent density\nfunctional theory has established that the reduced $1$-electron density\npossesses memory-dependence, the precise nature of this memory-dependence has\nnot been understood. We derive a self-contained, symmetry/constraint-preserving\nmethod to propagate reduced TDCI electron density matrices. In numerical tests\non two model systems (H$_2$ and HeH$^+$), we show that with sufficiently large\ntime-delay (or memory-dependence), our method propagates reduced TDCI density\nmatrices with high quantitative accuracy. We study the dependence of our\nresults on time step and basis set. To derive our method, we calculate the\n$4$-index tensor that relates reduced and full TDCI density matrices. Our\ncalculation applies to any TDCI system, regardless of basis set, number of\nelectrons, or choice of Slater determinants in the wave function. This\ncalculation enables a proof that the trace of the reduced TDCI density matrix\nis constant and equals the number of electrons.\n","authors":["Harish S. Bhat","Hardeep Bassi","Karnamohit Ranka","Christine M. Isborn"],"pdf_url":"https://arxiv.org/pdf/2403.15596v1.pdf","comment":"29 pages, 7 figures"},{"id":"http://arxiv.org/abs/2309.10652v3","updated":"2024-03-22T18:45:20Z","published":"2023-09-19T14:33:14Z","title":"Nonlinear dynamic analysis of shear- and torsion-free rods using\n  isogeometric discretization and outlier removal","summary":"  In this paper, we present a discrete formulation of nonlinear shear- and\ntorsion-free rods introduced by Gebhardt and Romero in [20] that uses\nisogeometric discretization and robust time integration. Omitting the director\nas an independent variable field, we reduce the number of degrees of freedom\nand obtain discrete solutions in multiple copies of the Euclidean space (R^3),\nwhich is larger than the corresponding multiple copies of the manifold (R^3 x\nS^2) obtained with standard Hermite finite elements. For implicit time\nintegration, we choose the same integration scheme as Gebhardt and Romero in\n[20] that is a hybrid form of the midpoint and the trapezoidal rules. In\naddition, we apply a recently introduced approach for outlier removal by\nHiemstra et al. [26] that reduces high-frequency content in the response\nwithout affecting the accuracy, ensuring robustness of our nonlinear discrete\nformulation. We illustrate the efficiency of our nonlinear discrete formulation\nfor static and transient rods under different loading conditions, demonstrating\ngood accuracy in space, time and the frequency domain. Our numerical example\ncoincides with a relevant application case, the simulation of mooring lines.\n","authors":["Thi-Hoa Nguyen","Bruno A. Roccia","René R. Hiemstra","Cristian G. Gebhardt","Dominik Schillinger"],"pdf_url":"https://arxiv.org/pdf/2309.10652v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15129v1","updated":"2024-03-22T11:30:57Z","published":"2024-03-22T11:30:57Z","title":"Digital twin model of colon electromechanics for manometry prediction of\n  laser tissue soldering","summary":"  The present study introduces an advanced multi-physics and multi-scale\nmodeling approach to investigate in silico colon motility. We introduce a\ngeneralized electromechanical framework, integrating cellular electrophysiology\nand smooth muscle contractility, thus advancing a first-of-its-kind\ncomputational model of laser tissue soldering after incision resection. The\nproposed theoretical framework comprises three main elements: a microstructural\nmaterial model describing intestine wall geometry and composition of\nreinforcing fibers, with four fiber families, two active-conductive and two\npassive; an electrophysiological model describing the propagation of slow\nwaves, based on a fully-coupled nonlinear phenomenological approach; and a\nthermodynamical consistent mechanical model describing the hyperelastic\nenergetic contributions ruling tissue equilibrium under diverse loading\nconditions. The active strain approach was adopted to describe tissue\nelectromechanics by exploiting the multiplicative decomposition of the\ndeformation gradient for each active fiber family and solving the governing\nequations via a staggered finite element scheme. The computational framework\nwas fine-tuned according to state-of-the-art experimental evidence, and\nextensive numerical analyses allowed us to compare manometric traces computed\nvia numerical simulations with those obtained clinically in human patients. The\nmodel proved capable of reproducing both qualitatively and quantitatively high\nor low-amplitude propagation contractions. Colon motility after laser tissue\nsoldering demonstrates that material properties and couplings of the deposited\ntissue are critical to reproducing a physiological muscular contraction, thus\nrestoring a proper peristaltic activity.\n","authors":["René Thierry Djoumessi","Pietro Lenarda","Alessio Gizzi","Simone Giusti","Pietro Alduini","Marco Paggi"],"pdf_url":"https://arxiv.org/pdf/2403.15129v1.pdf","comment":null}]},"2024-03-21T00:00:00Z":{"Computational Engineering":[{"id":"http://arxiv.org/abs/2403.14795v1","updated":"2024-03-21T19:16:30Z","published":"2024-03-21T19:16:30Z","title":"Advanced Deep Operator Networks to Predict Multiphysics Solution Fields\n  in Materials Processing and Additive Manufacturing","summary":"  Unlike classical artificial neural networks, which require retraining for\neach new set of parametric inputs, the Deep Operator Network (DeepONet), a\nlately introduced deep learning framework, approximates linear and nonlinear\nsolution operators by taking parametric functions (infinite-dimensional\nobjects) as inputs and mapping them to complete solution fields. In this paper,\ntwo newly devised DeepONet formulations with sequential learning and Residual\nU-Net (ResUNet) architectures are trained for the first time to simultaneously\npredict complete thermal and mechanical solution fields under variable loading,\nloading histories, process parameters, and even variable geometries. Two\nreal-world applications are demonstrated: 1- coupled thermo-mechanical analysis\nof steel continuous casting with multiple visco-plastic constitutive laws and\n2- sequentially coupled direct energy deposition for additive manufacturing.\nDespite highly challenging spatially variable target stress distributions,\nDeepONets can infer reasonably accurate full-field temperature and stress\nsolutions several orders of magnitude faster than traditional and highly\noptimized finite-element analysis (FEA), even when FEA simulations are run on\nthe latest high-performance computing platforms. The proposed DeepONet model's\nability to provide field predictions almost instantly for unseen input\nparameters opens the door for future preliminary evaluation and design\noptimization of these vital industrial processes.\n","authors":["Shashank Kushwaha","Jaewan Park","Seid Koric","Junyan He","Iwona Jasiuk","Diab Abueidda"],"pdf_url":"https://arxiv.org/pdf/2403.14795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14788v1","updated":"2024-03-21T19:05:22Z","published":"2024-03-21T19:05:22Z","title":"Geom-DeepONet: A Point-cloud-based Deep Operator Network for Field\n  Predictions on 3D Parameterized Geometries","summary":"  Modern digital engineering design process commonly involves expensive\nrepeated simulations on varying three-dimensional (3D) geometries. The\nefficient prediction capability of neural networks (NNs) makes them a suitable\nsurrogate to provide design insights. Nevertheless, few available NNs can\nhandle solution prediction on varying 3D shapes. We present a novel deep\noperator network (DeepONet) variant called Geom-DeepONet, which encodes\nparameterized 3D geometries and predicts full-field solutions on an arbitrary\nnumber of nodes. To the best of the authors' knowledge, this is the first\nattempt in the literature and is our primary novelty. In addition to expressing\nshapes using mesh coordinates, the signed distance function for each node is\nevaluated and used to augment the inputs to the trunk network of the\nGeom-DeepONet, thereby capturing both explicit and implicit representations of\nthe 3D shapes. The powerful geometric encoding capability of a sinusoidal\nrepresentation network (SIREN) is also exploited by replacing the classical\nfeedforward neural networks in the trunk with SIREN. Additional data fusion\nbetween the branch and trunk networks is introduced by an element-wise product.\nA numerical benchmark was conducted to compare Geom-DeepONet to PointNet and\nvanilla DeepONet, where results show that our architecture trains fast with a\nsmall memory footprint and yields the most accurate results among the three\nwith less than 2 MPa stress error. Results show a much lower generalization\nerror of our architecture on unseen dissimilar designs than vanilla DeepONet.\nOnce trained, the model can predict vector solutions, and speed can be over\n$10^5$ times faster than implicit finite element simulations for large meshes.\n","authors":["Junyan He","Seid Koric","Diab Abueidda","Ali Najafi","Iwona Jasiuk"],"pdf_url":"https://arxiv.org/pdf/2403.14788v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14510v1","updated":"2024-03-21T16:07:30Z","published":"2024-03-21T16:07:30Z","title":"Universal Differential Equations as a Common Modeling Language for\n  Neuroscience","summary":"  The unprecedented availability of large-scale datasets in neuroscience has\nspurred the exploration of artificial deep neural networks (DNNs) both as\nempirical tools and as models of natural neural systems. Their appeal lies in\ntheir ability to approximate arbitrary functions directly from observations,\ncircumventing the need for cumbersome mechanistic modeling. However, without\nappropriate constraints, DNNs risk producing implausible models, diminishing\ntheir scientific value. Moreover, the interpretability of DNNs poses a\nsignificant challenge, particularly with the adoption of more complex\nexpressive architectures. In this perspective, we argue for universal\ndifferential equations (UDEs) as a unifying approach for model development and\nvalidation in neuroscience. UDEs view differential equations as\nparameterizable, differentiable mathematical objects that can be augmented and\ntrained with scalable deep learning techniques. This synergy facilitates the\nintegration of decades of extensive literature in calculus, numerical analysis,\nand neural modeling with emerging advancements in AI into a potent framework.\nWe provide a primer on this burgeoning topic in scientific machine learning and\ndemonstrate how UDEs fill in a critical gap between mechanistic,\nphenomenological, and data-driven models in neuroscience. We outline a flexible\nrecipe for modeling neural systems with UDEs and discuss how they can offer\nprincipled solutions to inherent challenges across diverse neuroscience\napplications such as understanding neural computation, controlling neural\nsystems, neural decoding, and normative modeling.\n","authors":["Ahmed ElGazzar","Marcel van Gerven"],"pdf_url":"https://arxiv.org/pdf/2403.14510v1.pdf","comment":"23 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.14404v1","updated":"2024-03-21T13:52:55Z","published":"2024-03-21T13:52:55Z","title":"Physics-Informed Diffusion Models","summary":"  Generative models such as denoising diffusion models are quickly advancing\ntheir ability to approximate highly complex data distributions. They are also\nincreasingly leveraged in scientific machine learning, where samples from the\nimplied data distribution are expected to adhere to specific governing\nequations. We present a framework to inform denoising diffusion models on\nunderlying constraints on such generated samples during model training. Our\napproach improves the alignment of the generated samples with the imposed\nconstraints and significantly outperforms existing methods without affecting\ninference speed. Additionally, our findings suggest that incorporating such\nconstraints during training provides a natural regularization against\noverfitting. Our framework is easy to implement and versatile in its\napplicability for imposing equality and inequality constraints as well as\nauxiliary optimization objectives.\n","authors":["Jan-Hendrik Bastek","WaiChing Sun","Dennis M. Kochmann"],"pdf_url":"https://arxiv.org/pdf/2403.14404v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.17799v3","updated":"2024-03-21T11:40:29Z","published":"2023-05-28T19:02:51Z","title":"I-FENN for thermoelasticity based on physics-informed temporal\n  convolutional network (PI-TCN)","summary":"  Most currently available methods for modeling multiphysics, including\nthermoelasticity, using machine learning approaches, are focused on solving\ncomplete multiphysics problems using data-driven or physics-informed\nmulti-layer perceptron (MLP) networks. Such models rely on incremental\nstep-wise training of the MLPs, and lead to elevated computational expense;\nthey also lack the rigor of existing numerical methods like the finite element\nmethod. We propose an integrated finite element neural network (I-FENN)\nframework to expedite the solution of coupled transient thermoelasticity. A\nnovel physics-informed temporal convolutional network (PI-TCN) is developed and\nembedded within the finite element framework to leverage the fast inference of\nneural networks (NNs). The PI-TCN model captures some of the fields in the\nmultiphysics problem; then, the network output is used to compute the other\nfields of interest using the finite element method. We establish a framework\nthat computationally decouples the energy equation from the linear momentum\nequation. We first develop a PI-TCN model to predict the spatiotemporal\nevolution of the temperature field across the simulation time based on the\nenergy equation and strain data. The PI-TCN model is integrated into the finite\nelement framework, where the PI-TCN output (temperature) is used to introduce\nthe temperature effect to the linear momentum equation. The finite element\nproblem is solved using the implicit Euler time discretization scheme,\nresulting in a computational cost comparable to that of a weakly-coupled\nthermoelasticity problem but with the ability to solve fully-coupled problems.\nFinally, we demonstrate I-FENN's computational efficiency and generalization\ncapability in thermoelasticity through several numerical examples.\n","authors":["Diab W. Abueidda","Mostafa E. Mobasher"],"pdf_url":"https://arxiv.org/pdf/2305.17799v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14063v1","updated":"2024-03-21T01:20:32Z","published":"2024-03-21T01:20:32Z","title":"DiffSTOCK: Probabilistic relational Stock Market Predictions using\n  Diffusion Models","summary":"  In this work, we propose an approach to generalize denoising diffusion\nprobabilistic models for stock market predictions and portfolio management.\nPresent works have demonstrated the efficacy of modeling interstock relations\nfor market time-series forecasting and utilized Graph-based learning models for\nvalue prediction and portfolio management. Though convincing, these\ndeterministic approaches still fall short of handling uncertainties i.e., due\nto the low signal-to-noise ratio of the financial data, it is quite challenging\nto learn effective deterministic models. Since the probabilistic methods have\nshown to effectively emulate higher uncertainties for time-series predictions.\nTo this end, we showcase effective utilisation of Denoising Diffusion\nProbabilistic Models (DDPM), to develop an architecture for providing better\nmarket predictions conditioned on the historical financial indicators and\ninter-stock relations. Additionally, we also provide a novel deterministic\narchitecture MaTCHS which uses Masked Relational Transformer(MRT) to exploit\ninter-stock relations along with historical stock features. We demonstrate that\nour model achieves SOTA performance for movement predication and Portfolio\nmanagement.\n","authors":["Divyanshu Daiya","Monika Yadav","Harshit Singh Rao"],"pdf_url":"https://arxiv.org/pdf/2403.14063v1.pdf","comment":"Accepted for presentation to the 2024 IEEE International Conference\n  on Acoustics, Speech, and Signal Processing (ICASSP 2024), Seoul, Korea"},{"id":"http://arxiv.org/abs/2403.10625v2","updated":"2024-03-21T00:31:52Z","published":"2024-03-15T18:40:22Z","title":"FloodGenome: Interpretable Machine Learning for Decoding Features\n  Shaping Property Flood Risk Predisposition in Cities","summary":"  Understanding the fundamental characteristics that shape the inherent flood\nrisk disposition of urban areas is critical for integrated urban design\nstrategies for flood risk reduction. Flood risk disposition specifies an\ninherent and event-independent magnitude of property flood risk and measures\nthe extent to which urban areas are susceptible to property damage if exposed\nto a weather hazard. This study presents FloodGenome as an interpretable\nmachine learning model for evaluation of the extent to which various\nhydrological, topographic, and built-environment features and their\ninteractions shape flood risk disposition in urban areas. Using flood damage\nclaims data from the U.S. National Flood Insurance Program covering the period\n2003 through 2023 across four metropolitan statistical areas (MSAs), the\nanalysis computes building damage ratios and flood claim counts by employing\nk-means clustering for classifying census block groups (CBGs) into distinct\nproperty flood risk disposition levels. Then a random forest model is created\nto specify property flood risk levels of CBGs based on various intertwined\nhydrological, topographic, and built-environment features. The model\ntransferability analysis results show consistent performance across MSAs,\nrevealing the universality of underlying features that shape city property\nflood risks. The FloodGenome model is then used to:(1) evaluate the extent to\nwhich future urban development would exacerbate flood risk disposition of urban\nareas; and (2) specify property flood risk levels at finer spatial resolution\nproviding critical insights for flood risk management processes. The\nFloodGenome model and the findings provide novel tools and insights for\nimproving the characterization and understanding of intertwined features that\nshape flood risk profiles of cities.\n","authors":["Chenyue Liu","Ali Mostafavi"],"pdf_url":"https://arxiv.org/pdf/2403.10625v2.pdf","comment":null}]},"2024-03-20T00:00:00Z":{"Computational Engineering":[{"id":"http://arxiv.org/abs/2310.06155v3","updated":"2024-03-20T20:43:03Z","published":"2023-10-09T21:05:27Z","title":"CoQuest: Exploring Research Question Co-Creation with an LLM-based Agent","summary":"  Developing novel research questions (RQs) often requires extensive literature\nreviews, especially in interdisciplinary fields. To support RQ development\nthrough human-AI co-creation, we leveraged Large Language Models (LLMs) to\nbuild an LLM-based agent system named CoQuest. We conducted an experiment with\n20 HCI researchers to examine the impact of two interaction designs:\nbreadth-first and depth-first RQ generation. The findings revealed that\nparticipants perceived the breadth-first approach as more creative and\ntrustworthy upon task completion. Conversely, during the task, participants\nconsidered the depth-first generated RQs as more creative. Additionally, we\ndiscovered that AI processing delays allowed users to reflect on multiple RQs\nsimultaneously, leading to a higher quantity of generated RQs and an enhanced\nsense of control. Our work makes both theoretical and practical contributions\nby proposing and evaluating a mental model for human-AI co-creation of RQs. We\nalso address potential ethical issues, such as biases and over-reliance on AI,\nadvocating for using the system to improve human research creativity rather\nthan automating scientific inquiry.\n","authors":["Yiren Liu","Si Chen","Haocong Cheng","Mengxia Yu","Xiao Ran","Andrew Mo","Yiliu Tang","Yun Huang"],"pdf_url":"https://arxiv.org/pdf/2310.06155v3.pdf","comment":"Accepted to SIGCHI 2024"},{"id":"http://arxiv.org/abs/2403.13704v1","updated":"2024-03-20T16:08:27Z","published":"2024-03-20T16:08:27Z","title":"Improving the Adaptive Moment Estimation (ADAM) stochastic optimizer\n  through an Implicit-Explicit (IMEX) time-stepping approach","summary":"  The Adam optimizer, often used in Machine Learning for neural network\ntraining, corresponds to an underlying ordinary differential equation (ODE) in\nthe limit of very small learning rates. This work shows that the classical Adam\nalgorithm is a first order implicit-explicit (IMEX) Euler discretization of the\nunderlying ODE. Employing the time discretization point of view, we propose new\nextensions of the Adam scheme obtained by using higher order IMEX methods to\nsolve the ODE. Based on this approach, we derive a new optimization algorithm\nfor neural network training that performs better than classical Adam on several\nregression and classification problems.\n","authors":["Abhinab Bhattacharjee","Andrey A. Popov","Arash Sarshar","Adrian Sandu"],"pdf_url":"https://arxiv.org/pdf/2403.13704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13654v1","updated":"2024-03-20T14:59:10Z","published":"2024-03-20T14:59:10Z","title":"A globalized and preconditioned Newton-CG solver for metric-aware curved\n  high-order mesh optimization","summary":"  We present a specific-purpose globalized and preconditioned Newton-CG solver\nto minimize a metric-aware curved high-order mesh distortion. The solver is\nspecially devised to optimize curved high-order meshes for high polynomial\ndegrees with a target metric featuring non-uniform sizing, high stretching\nratios, and curved alignment -- exactly the features that stiffen the\noptimization problem. To this end, we consider two ingredients: a\nspecific-purpose globalization and a specific-purpose\nJacobi-$\\text{iLDL}^{\\text{T}}(0)$ preconditioning with varying accuracy and\ncurvature tolerances (dynamic forcing terms) for the CG method. These\nimprovements are critical in stiff problems because, without them, the large\nnumber of non-linear and linear iterations makes curved optimization\nimpractical. Finally, to analyze the performance of our method, the results\ncompare the specific-purpose solver with standard optimization methods. For\nthis, we measure the matrix-vector products indicating the solver computational\ncost and the line-search iterations indicating the total amount of objective\nfunction evaluations. When we combine the globalization and the linear solver\ningredients, we conclude that the specific-purpose Newton-CG solver reduces the\ntotal number of matrix-vector products by one order of magnitude. Moreover, the\nnumber of non-linear and line-search iterations is mainly smaller but of\nsimilar magnitude.\n","authors":["Guillermo Aparicio-Estrems","Abel Gargallo-Peiró","Xevi Roca"],"pdf_url":"https://arxiv.org/pdf/2403.13654v1.pdf","comment":"64 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.13528v1","updated":"2024-03-20T12:01:02Z","published":"2024-03-20T12:01:02Z","title":"Defining metric-aware size-shape measures to validate and optimize\n  curved high-order meshes","summary":"  We define a regularized size-shape distortion (quality) measure for curved\nhigh-order elements on a Riemannian space. To this end, we measure the\ndeviation of a given element, straight-sided or curved, from the stretching,\nalignment, and sizing determined by a target metric. The defined distortion\n(quality) is suitable to check the validity and the quality of straight-sided\nand curved elements on Riemannian spaces determined by constant and point-wise\nvarying metrics. The examples illustrate that the distortion can be minimized\nto curve (deform) the elements of a given high-order (linear) mesh and try to\nmatch with curved (linear) elements the point-wise stretching, alignment, and\nsizing of a discrete target metric tensor. In addition, the resulting meshes\nsimultaneously match the curved features of the target metric and boundary.\nFinally, to verify if the minimization of the metric-aware size-shape\ndistortion leads to meshes approximating the target metric, we compute the\nRiemannian measures for the element edges, faces, and cells. The results show\nthat, when compared to anisotropic straight-sided meshes, the Riemannian\nmeasures of the curved high-order mesh entities are closer to unit.\nFurthermore, the optimized meshes illustrate the potential of curved\n$r$-adaptation to improve the accuracy of a function representation.\n","authors":["Guillermo Aparicio-Estrems","Abel Gargallo-Peiró","Xevi Roca"],"pdf_url":"https://arxiv.org/pdf/2403.13528v1.pdf","comment":"49 pages, 23 figures"},{"id":"http://arxiv.org/abs/2403.13515v1","updated":"2024-03-20T11:31:39Z","published":"2024-03-20T11:31:39Z","title":"Efficient numerical methods for the Maxey-Riley equations with Basset\n  history term","summary":"  The Maxey-Riley equations (MRE) describe the motion of a finite-sized,\nspherical particle in a fluid. Because of wake effects, the force acting on a\nparticle depends on its past trajectory. This is modelled by an integral term\nin the MRE, also called Basset force, that makes its numerical solution\nchallenging and memory intensive. A recent approach proposed by Prasath, Vasan\nand Govindarajan exploits connections between the integral term and fractional\nderivatives to reformulate the MRE as a time-dependent partial differential\nequation on a semi-infinite pseudo-space. They also propose a numerical\nalgorithm based on polynomial expansions. This paper develops a numerical\napproach based on finite difference instead, by adopting techniques by Koleva\nand Fazio and Janelli to cope with the issues of having an unbounded spatial\ndomain. We compare convergence order and computational efficiency for particles\nof varying size and density of the polynomial expansion by Prasath et al., our\nfinite difference schemes and a direct integrator for the MRE based on\nmulti-step methods proposed by Daitche.\n","authors":["Julio Urizarna-Carasa","Leon Schlegel","Daniel Ruprecht"],"pdf_url":"https://arxiv.org/pdf/2403.13515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13439v1","updated":"2024-03-20T09:27:49Z","published":"2024-03-20T09:27:49Z","title":"Stochastic Geometry Models for Texture Synthesis of Machined Metallic\n  Surfaces: Sandblasting and Milling","summary":"  Training defect detection algorithms for visual surface inspection systems\nrequires a large and representative set of training data. Often there is not\nenough real data available which additionally cannot cover the variety of\npossible defects. Synthetic data generated by a synthetic visual surface\ninspection environment can overcome this problem. Therefore, a digital twin of\nthe object is needed, whose micro-scale surface topography is modeled by\ntexture synthesis models. We develop stochastic texture models for sandblasted\nand milled surfaces based on topography measurements of such surfaces. As the\nsurface patterns differ significantly, we use separate modeling approaches for\nthe two cases. Sandblasted surfaces are modeled by a combination of data-based\ntexture synthesis methods that rely entirely on the measurements. In contrast,\nthe model for milled surfaces is procedural and includes all process-related\nparameters known from the machine settings.\n","authors":["Natascha Jeziorski","Claudia Redenbach"],"pdf_url":"https://arxiv.org/pdf/2403.13439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13429v1","updated":"2024-03-20T09:17:12Z","published":"2024-03-20T09:17:12Z","title":"Detecting and Triaging Spoofing using Temporal Convolutional Networks","summary":"  As algorithmic trading and electronic markets continue to transform the\nlandscape of financial markets, detecting and deterring rogue agents to\nmaintain a fair and efficient marketplace is crucial. The explosion of large\ndatasets and the continually changing tricks of the trade make it difficult to\nadapt to new market conditions and detect bad actors. To that end, we propose a\nframework that can be adapted easily to various problems in the space of\ndetecting market manipulation. Our approach entails initially employing a\nlabelling algorithm which we use to create a training set to learn a weakly\nsupervised model to identify potentially suspicious sequences of order book\nstates. The main goal here is to learn a representation of the order book that\ncan be used to easily compare future events. Subsequently, we posit the\nincorporation of expert assessment to scrutinize specific flagged order book\nstates. In the event of an expert's unavailability, recourse is taken to the\napplication of a more complex algorithm on the identified suspicious order book\nstates. We then conduct a similarity search between any new representation of\nthe order book against the expert labelled representations to rank the results\nof the weak learner. We show some preliminary results that are promising to\nexplore further in this direction\n","authors":["Kaushalya Kularatnam","Tania Stathaki"],"pdf_url":"https://arxiv.org/pdf/2403.13429v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03823v2","updated":"2024-03-20T09:08:02Z","published":"2023-11-07T09:12:53Z","title":"Data-informed uncertainty quantification for laser-based powder bed\n  fusion additive manufacturing","summary":"  We present an efficient approach to quantify the uncertainties associated\nwith the numerical simulations of the laser-based powder bed fusion of metals\nprocesses. Our study focuses on a thermomechanical model of an Inconel 625\ncantilever beam, based on the AMBench2018-01 benchmark proposed by the National\nInstitute of Standards and Technology (NIST). The proposed approach consists of\na forward uncertainty quantification analysis of the residual strains of the\ncantilever beam given the uncertainty in some of the parameters of the\nnumerical simulation, namely the powder convection coefficient and the\nactivation temperature. The uncertainty on such parameters is modelled by a\ndata-informed probability density function obtained by a Bayesian inversion\nprocedure, based on the displacement experimental data provided by NIST. To\novercome the computational challenges of both the Bayesian inversion and the\nforward uncertainty quantification analysis we employ a multi-fidelity\nsurrogate modelling technique, specifically the multi-index stochastic\ncollocation method. The proposed approach allows us to achieve a 33\\% reduction\nin the uncertainties on the prediction of residual strains compared with what\nwe would get basing the forward UQ analysis on a-priori ranges for the\nuncertain parameters, and in particular the mode of the probability density\nfunction of such quantities (i.e., its ``most likely value'', roughly speaking)\nresults to be in good agreement with the experimental data provided by NIST,\neven though only displacement data were used for the Bayesian inversion\nprocedure.\n","authors":["Mihaela Chiappetta","Chiara Piazzola","Lorenzo Tamellini","Alessandro Reali","Ferdinando Auricchio","Massimo Carraturo"],"pdf_url":"https://arxiv.org/pdf/2311.03823v2.pdf","comment":"28 pages, 14 figures"},{"id":"http://arxiv.org/abs/2403.13413v1","updated":"2024-03-20T08:55:16Z","published":"2024-03-20T08:55:16Z","title":"On the moments of Cox rate-and-state models","summary":"  Rate-and-state models are widely used physical models for the relation\nbetween changes in pore pressure due to fluid injection or gas extraction and\nthe induced seismic hazard in a field. We consider the modification where the\npore pressure measurements are affected by noise and provide explicit\nexpressions for the first and second moments of the state variable. We show\nthat when the pressure increases, there is positive correlation. In the case of\ndecreasing pressure, both positive and negative correlation is possible. Using\nthe delta method, approximate first and second moments of the rate variable are\nderived and compared to empirical moments.\n","authors":["Z. Baki","M. N. M. van Lieshout"],"pdf_url":"https://arxiv.org/pdf/2403.13413v1.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2403.13409v1","updated":"2024-03-20T08:50:41Z","published":"2024-03-20T08:50:41Z","title":"Influence of concentration-dependent material properties on the fracture\n  and debonding of electrode particles with core-shell structure","summary":"  Core-shell electrode particle designs offer a route to improved lithium-ion\nbattery performance. However, they are susceptible to mechanical damage such as\nfracture and debonding, which can significantly reduce their lifetime. Using a\ncoupled finite element model, we explore the impacts of diffusion-induced\nstresses on the failure mechanisms of an exemplar system with an NMC811 core\nand an NMC111 shell. In particular, we systematically compare the implications\nof assuming constant material properties against using Li\nconcentration-dependent diffusion coefficient and partial molar volume. With\nconstant material properties, our results show that smaller cores with thinner\nshells avoid debonding and fracture regimes. When factoring in a\nconcentration-dependent partial molar volume, the maximum values of tensile\nhoop stress in the shell are found to be significantly lower than those\npredicted with constant properties, reducing the likelihood of fracture.\nFurthermore, with a concentration-dependent diffusion coefficient, significant\nbarriers to full electrode utilisation are observed due to reduced lithium\nmobility at high states of lithiation. This provides a possible explanation for\nthe reduced accessible capacity observed in experiments. Shell thickness is\nfound to be the dominant factor in precluding structural integrity once the\nconcentration dependency is accounted for. These findings shed new light on the\nperformance and effective design of core-shell electrode particles.\n","authors":["Y. Tu","B. Wu","W. Ai","E. Martínez-Pañeda"],"pdf_url":"https://arxiv.org/pdf/2403.13409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13380v1","updated":"2024-03-20T08:22:24Z","published":"2024-03-20T08:22:24Z","title":"A characteristics-based method for shock-ramp data analysis","summary":"  For the data analysis problem of shock-ramp compression, i.e., ramp\ncompression after a relatively strong initial shock, a characteristics-based\nmethod that strictly deals with the initial hydrodynamic shock is described in\ndetail. Validation of this analysis method using simulated shock-ramp data\ngenerated by molecular dynamics and one-dimensional radiation hydrodynamic code\nis also presented.\n","authors":["Jingxiang Shen","Wei Kang"],"pdf_url":"https://arxiv.org/pdf/2403.13380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13379v1","updated":"2024-03-20T08:21:35Z","published":"2024-03-20T08:21:35Z","title":"Application of advanced ultrasonic testing methods to Dissimilar Metal\n  Welds -- Comparison of simulated and experimental results","summary":"  Widely present in the primary circuit of Nuclear Power Plants (NPP),\nDissimilar Metal Welds (DMW) are inspected using Ultrasonic nondestructive\nTesting (UT) techniques to ensure the integrity of the structure and detect\ndefects such as Stress Corrosion Cracking (SCC).In a previous collaborative\nresearch, CRIEPI and CEA have worked on the understanding of the propagation of\nultrasonic waves in complex materials. Indeed, the ultrasonic propagation can\nbe disturbed due to the anisotropic and inhomogeneous properties of the medium\nand the interpretation of inspection results can then be difficult. An\nanalytical model, based on a dynamic ray theory, developed by CEA-LIST and\nimplemented in the CIVA software had been used to predict the ultrasonic\npropagation in a DMW. The model evaluates the ray trajectories, the travel-time\nand the computation of the amplitude along the ray tube in a medium described\nthanks to a continuously varying description of its physical properties. In\nthis study, the weld had been described by an analytical law of the\ncrystallographic orientation. The simulated results of the detection of\ncalibrated notches located in the buttering and the weld had been compared with\nexperimental data and had shown a good agreement.The new collaborative program\npresented in this paper aims at detecting a real SCC defect located close to\nthe root of the DMW. Thus, simulations have been performed for a DMW described\nwith an analytical law and a smooth cartography of the crystallographic\norientation. Furthermore, advanced ultrasonic testing methods have been used to\ninspect the specimen and detect the real SCC defect. Experimental and simulated\nresults of the mock-up inspection have been compared.\n","authors":["Audrey Gardahaut","Hugues Lourme","Steve Mahaut","Masaki Nagai","Shan Lin"],"pdf_url":"https://arxiv.org/pdf/2403.13379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13357v1","updated":"2024-03-20T07:36:14Z","published":"2024-03-20T07:36:14Z","title":"A Machine Learning Approach for Multiscale Modeling of the Facet\n  Capsular Ligament","summary":"  We develop a new neural network architecture that strictly enforces\nconstitutive constraints such as polyconvexity, frame-indifference, zero strain\nenergy with zero deformations, and the symmetry of the stress and material\nstiffness. Additionally, we show that for this neural network, the accuracy is\nsignificantly improved by using a Sobolev minimization strategy that includes\nderivative terms. Using our network and Sobolev minimization, we obtain a NMSE\nof 0.15% for the energy, 0.815% averaged across the components of the stress,\nand 5.4% averaged across the components of the stiffness. This machine learned\nconstitutive model was deployed in a finite element simulation of a facet\ncapsular ligament. The displacement fields and stress-strain curves where\ncompared to a multiscale simulation that required running on a GPU based\nsupercomputer. At 70% strain, the model using the neural network had less than\n10% relative error in the mean stress value.\n","authors":["Jacob S. Merson","Nishan Parvez"],"pdf_url":"https://arxiv.org/pdf/2403.13357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12696v2","updated":"2024-03-20T06:36:29Z","published":"2024-03-19T12:52:46Z","title":"Bayesian estimation and uncertainty quantification of a\n  temperature-dependent thermal conductivity","summary":"  We consider the problem of estimating a temperature-dependent thermal\nconductivity model (curve) from temperature measurements. We apply a Bayesian\nestimation approach that takes into account measurement errors and limited\nprior information of system properties. The approach intertwines system\nsimulation and Markov chain Monte Carlo (MCMC) sampling. We investigate the\nimpact of assuming different model classes - cubic polynomials and piecewise\nlinear functions - their parametrization, and different types of prior\ninformation - ranging from uninformative to informative. Piecewise linear\nfunctions require more parameters (conductivity values) to be estimated than\nthe four parameters (coefficients or conductivity values) needed for cubic\npolynomials. The former model class is more flexible, but the latter requires\nless MCMC samples. While parametrizing polynomials with coefficients may feel\nmore natural, it turns out that parametrizing them using conductivity values is\nfar more natural for the specification of prior information. Robust estimation\nis possible for all model classes and parametrizations, as long as the prior\ninformation is accurate or not too informative. Gaussian Markov random field\npriors are especially well-suited for piecewise linear functions.\n","authors":["Rodrigo L. S. Silva","Clemens Verhoosel","Erik Quaeghebeur"],"pdf_url":"https://arxiv.org/pdf/2403.12696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13270v1","updated":"2024-03-20T03:09:51Z","published":"2024-03-20T03:09:51Z","title":"Canonical Descriptors for Periodic Lattice Truss Materials","summary":"  For decades, aspects of the topological architecture, and of the mechanical\nas well as other physical behaviors of periodic lattice truss materials (PLTMs)\nhave been massively studied. Their approximate infinite design space presents a\ndouble-edged sword, implying on one hand dramatic designability in fulfilling\nthe requirement of various performance, but on the other hand unexpected\nintractability in determining the best candidate with tailoring properties. In\nrecent years, the development of additive manufacturing and artificial\nintelligence spurs an explosion in the methods exploring the design space and\nsearching its boundaries. However, regrettably, a normative description with\nsufficient information of PLTMs applying to machine learning has not yet been\nconstructed, which confines the inverse design to some discrete and small\nscrutinized space. In the current paper, we develop a system of canonical\ndescriptors for PLTMs, encoding not only the geometrical configurations but\nalso mechanical properties into matrix forms to establish good quantitative\ncorrelations between structures and mechanical behaviors. The system mainly\nconsists of the geometry matrix for the lattice node configuration, density,\nstretching and bending stiffness matrices for the lattice strut properties, as\nwell as packing matrix for the principal periodic orientation. All these\nmatrices are theoretically derived based on the intrinsic nature of PLTMs,\nleading to concise descriptions and sufficient information. The\ncharacteristics, including the completeness and uniqueness, of the descriptors\nare analyzed. In addition, we discuss how the current system of descriptors can\nbe applied to the database construction and material discovery, and indicate\nthe possible open problems.\n","authors":["Ge Qi","Huai-Liang Zheng","Chen-xi Liu","Li MA","Kai-Uwe Schröder"],"pdf_url":"https://arxiv.org/pdf/2403.13270v1.pdf","comment":"57 pages, 7 figures, 3 tables"}]}}